[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "assignment-1.html",
    "href": "assignment-1.html",
    "title": "Assignment 1: Reliability and tools for reproducible data science",
    "section": "",
    "text": "The purpose of this assignment is to present estimates of reliability of measures collected in the physiology lab. A second purpose is to use tools for reproducible data science. The report that you are expected to hand in therefore has some strict requirements in its format (see below). The assignment is a group assignment and at least three students are expected to contribute to each report.\n\n\nThe report should describe one test that you have performed in the physiology-lab. Select the test that is most interesting to you. The test should be described with a detailed protocol, including preparations of the participant (that is being tested), standardization of the test, and post-test data preparation. Post-test data preparation refers to steps needed to get data from e.g. equipment used during the test. This section should take into account and reference (Halperin, Pyne, and Martin 2015; Tanner and Gore 2012)\nThe next section should contain descriptive data from the test. This could be measures of central tendency and variability in the measures you have collected. If possible, try to find similar estimates in the scientific literature.\nFinally, we are interested in reliability. Here you need to calculate an estimate of reliability of the test. Use (and reference) (Hopkins 2000). Try to be clear with what measure of reliability you are using and what it is telling you.\n\n\n\nThe report should be uploaded to github with both a source file (.Rmd or .qmd) and report file as output (html, pdf or docx-file). The github folder should also contain the dataset being used in the calculations. Work on your assignment as a R project. Contributions from members of the group can be made directly to the github repository. Follow this checklist on how to get started:\n(A nice introduction to github with R can be found here)\n\nCreate a github account. All members of the group should have their own account.\nOnce signed in to github.com, create a new repository with an informative name. Make sure that the repository is public. One member of the group can create this repository in their own account.\nBe sure to download and install git on your computer.\nStart up RStudio, start a new project, select the Version Control option and copy the address to the repository.\nAdd files to your project and upload to github. You need to add files, commit them and push using git. Use the command line in your terminal:\n\n\ngit add -A\n\ngit commit -m \"A short message to describe your changes\"\n\ngit push\n\n\nWhen you want to download the latest version of your project, type in the terminal:\n\n\ngit pull\n\n\nYou may encounter conflicts if pushing changes that overwrites changes made by other group members. These may be tricky but can be resolved… Have patience!\n\nIf you are contributing to the project owned by another person in your group it is reccomended that you create a pull request with your changes.\n\n\n\nIf you want more data, you may “borrow” data from previous students:\nVO2max tests (Group 1) VO2max tests (Group 2) Lactate threshold tests\n\n\n\nCopy the link to your github folder into canvas."
  },
  {
    "objectID": "assignment-1.html#elements-of-the-report",
    "href": "assignment-1.html#elements-of-the-report",
    "title": "Assignment 1: Reliability and tools for reproducible data science",
    "section": "",
    "text": "The report should describe one test that you have performed in the physiology-lab. Select the test that is most interesting to you. The test should be described with a detailed protocol, including preparations of the participant (that is being tested), standardization of the test, and post-test data preparation. Post-test data preparation refers to steps needed to get data from e.g. equipment used during the test. This section should take into account and reference (Halperin, Pyne, and Martin 2015; Tanner and Gore 2012)\nThe next section should contain descriptive data from the test. This could be measures of central tendency and variability in the measures you have collected. If possible, try to find similar estimates in the scientific literature.\nFinally, we are interested in reliability. Here you need to calculate an estimate of reliability of the test. Use (and reference) (Hopkins 2000). Try to be clear with what measure of reliability you are using and what it is telling you."
  },
  {
    "objectID": "assignment-1.html#the-format-of-the-report",
    "href": "assignment-1.html#the-format-of-the-report",
    "title": "Assignment 1: Reliability and tools for reproducible data science",
    "section": "",
    "text": "The report should be uploaded to github with both a source file (.Rmd or .qmd) and report file as output (html, pdf or docx-file). The github folder should also contain the dataset being used in the calculations. Work on your assignment as a R project. Contributions from members of the group can be made directly to the github repository. Follow this checklist on how to get started:\n(A nice introduction to github with R can be found here)\n\nCreate a github account. All members of the group should have their own account.\nOnce signed in to github.com, create a new repository with an informative name. Make sure that the repository is public. One member of the group can create this repository in their own account.\nBe sure to download and install git on your computer.\nStart up RStudio, start a new project, select the Version Control option and copy the address to the repository.\nAdd files to your project and upload to github. You need to add files, commit them and push using git. Use the command line in your terminal:\n\n\ngit add -A\n\ngit commit -m \"A short message to describe your changes\"\n\ngit push\n\n\nWhen you want to download the latest version of your project, type in the terminal:\n\n\ngit pull\n\n\nYou may encounter conflicts if pushing changes that overwrites changes made by other group members. These may be tricky but can be resolved… Have patience!\n\nIf you are contributing to the project owned by another person in your group it is reccomended that you create a pull request with your changes."
  },
  {
    "objectID": "assignment-1.html#data",
    "href": "assignment-1.html#data",
    "title": "Assignment 1: Reliability and tools for reproducible data science",
    "section": "",
    "text": "If you want more data, you may “borrow” data from previous students:\nVO2max tests (Group 1) VO2max tests (Group 2) Lactate threshold tests"
  },
  {
    "objectID": "assignment-1.html#how-to-hand-in-the-report.",
    "href": "assignment-1.html#how-to-hand-in-the-report.",
    "title": "Assignment 1: Reliability and tools for reproducible data science",
    "section": "",
    "text": "Copy the link to your github folder into canvas."
  },
  {
    "objectID": "assignment-2.html",
    "href": "assignment-2.html",
    "title": "Assignment 2: Regression models, predicting from data",
    "section": "",
    "text": "There are several suggestions on how to best capture the physiological “essence” of the lactate threshold test (See Tanner and Gore 2012, chap. 6). A simple, and very common way to analyze the relationship between exercise intensity and blood lactate is to determine exercise intensity at fixed blood lactate values. This can be done by fitting a regression model that captures the relationship and then “inverse predict” the exercise intensity value. An example of such inverse prediction can be found in the lecture notes.\nYour report should use data from the reliability project in the lab. Calculate at least two lactate thresholds (e.g. exercise intensity at 2 and 4 mmol L-1) and compare the reliability (typical error as a percentage of the mean) between the two thresholds. If you want to complicate things further you may want to implement other lactate threshold concepts (described in Tanner and Gore 2012; Newell et al. 2007).\n\n\n\nIn the molecular laboratory you have been tasked to extract and analyze DNA. In this process we have to determine the size of resulting PCR (polymerase chain reaction) amplified DNA fragments. A tutorial using Image J and R can be found here. In your report you should show how you arrived to your predicted sizes by including the code chunk in your report.\n\n\n\nUsing the hypertrophy data set, state a question that concerns a linear relationship between two variables in the data set. These variables might be related to muscle size and strength, or two molecular markers or any other variables you are interested in. Include a regression table from your analysis in the report and interpret its components in plain language (e.g. for an unit difference in the independent variable the dependent variable differs by y units). The interpretation should also include a description and explanation of the standard error, the t-value and the p-value. Valuable guidance on how to interpret the table may be found in for example and in (Frigessi and Aalen 2018), (Campbell, Walters, and Machin 2020) and (Spiegelhalter 2019, chap. 5).\nSpecial attention should be made concerning the p-value. How do you define and interpret the p-value in your regression table. What does it mean?.\n\n\n\nThe report is a group assignment, it is not to be included in the portfolio (mappeeksamen). However, it is required in order to pass the course (arbeidskrav).\nCreate a new project on github and collaborate with your group there. The repository with all data and coded needed to create the report, and the report itself (in html, docx or pdf format) should be reported on canvas as a link to the repository. Each member of the group hand in the link in canvas. The repository should be the same for all group members."
  },
  {
    "objectID": "assignment-2.html#part-1-lactate-thresholds",
    "href": "assignment-2.html#part-1-lactate-thresholds",
    "title": "Assignment 2: Regression models, predicting from data",
    "section": "",
    "text": "There are several suggestions on how to best capture the physiological “essence” of the lactate threshold test (See Tanner and Gore 2012, chap. 6). A simple, and very common way to analyze the relationship between exercise intensity and blood lactate is to determine exercise intensity at fixed blood lactate values. This can be done by fitting a regression model that captures the relationship and then “inverse predict” the exercise intensity value. An example of such inverse prediction can be found in the lecture notes.\nYour report should use data from the reliability project in the lab. Calculate at least two lactate thresholds (e.g. exercise intensity at 2 and 4 mmol L-1) and compare the reliability (typical error as a percentage of the mean) between the two thresholds. If you want to complicate things further you may want to implement other lactate threshold concepts (described in Tanner and Gore 2012; Newell et al. 2007)."
  },
  {
    "objectID": "assignment-2.html#part-2-predicting-sizes-of-dna-fragments",
    "href": "assignment-2.html#part-2-predicting-sizes-of-dna-fragments",
    "title": "Assignment 2: Regression models, predicting from data",
    "section": "",
    "text": "In the molecular laboratory you have been tasked to extract and analyze DNA. In this process we have to determine the size of resulting PCR (polymerase chain reaction) amplified DNA fragments. A tutorial using Image J and R can be found here. In your report you should show how you arrived to your predicted sizes by including the code chunk in your report."
  },
  {
    "objectID": "assignment-2.html#part-3-intepreting-a-regression-table",
    "href": "assignment-2.html#part-3-intepreting-a-regression-table",
    "title": "Assignment 2: Regression models, predicting from data",
    "section": "",
    "text": "Using the hypertrophy data set, state a question that concerns a linear relationship between two variables in the data set. These variables might be related to muscle size and strength, or two molecular markers or any other variables you are interested in. Include a regression table from your analysis in the report and interpret its components in plain language (e.g. for an unit difference in the independent variable the dependent variable differs by y units). The interpretation should also include a description and explanation of the standard error, the t-value and the p-value. Valuable guidance on how to interpret the table may be found in for example and in (Frigessi and Aalen 2018), (Campbell, Walters, and Machin 2020) and (Spiegelhalter 2019, chap. 5).\nSpecial attention should be made concerning the p-value. How do you define and interpret the p-value in your regression table. What does it mean?."
  },
  {
    "objectID": "assignment-2.html#how-to-hand-in-the-report",
    "href": "assignment-2.html#how-to-hand-in-the-report",
    "title": "Assignment 2: Regression models, predicting from data",
    "section": "",
    "text": "The report is a group assignment, it is not to be included in the portfolio (mappeeksamen). However, it is required in order to pass the course (arbeidskrav).\nCreate a new project on github and collaborate with your group there. The repository with all data and coded needed to create the report, and the report itself (in html, docx or pdf format) should be reported on canvas as a link to the repository. Each member of the group hand in the link in canvas. The repository should be the same for all group members."
  },
  {
    "objectID": "assignment-3.html",
    "href": "assignment-3.html",
    "title": "Assignment 3: Drawing inference from statistical models, and statistical power",
    "section": "",
    "text": "This assignment is set up as a statistical laboratory, we will perform simulations and your assignment is to interpret and explain the results. Create a report based on the code used in the lab and make sure you answer the specified questions (1-8). You can be as creative as you want and explore the results further.\nThe report should be handed in on canvas as a link to github repository containing a reproducible .Rmd (or qmd) file.\n\n\nIn this assignment we will simulate a population of possible values, from this population we will draw random samples, calculate statistics and interpret them. The population of values can be regarded as the possible differences between two treatments in a cross-over study where participants have performed both treatments. The values in the population are calculate as \\(Treatment - Control\\).\nWe will simulate a population of one million numbers with a mean of 1.5 and a standard deviation of 3. We will make two different set of studies, one set with a sample size of 8 and one set with a sample size of 40. In order to be sure you replicate your results, include and run set.seed() before simulations in your final script.\nWe will use the lm function to estimate the average value of the population. We do this in an “intercept-only” model. This model can be written as\n\\[Y_i = \\beta_0 + \\epsilon_i\\]\nwhere \\(\\beta_0\\) is the intercept and can be interpreted as the average value of \\(Y\\), our dependent variable. \\(\\epsilon\\) is the error term, each observation (\\(i\\)) deviates from the intercept to some degree. If the intercept term is positive or negative we can interpret it as a difference between the two treatments (described above). This model is equivalent to a one-sample t-test. Let’s get started!\nIn the code chunk below, we will simulate the population of differences between treatments. We will then draw two random samples corresponding sample sizes of 8 and 40 and save these data in data frames with the dependent variable named y. We fit the very simple model y ~ 1 as a linear model and save the model object as m1 and m2.\n\nlibrary(tidyverse)\n\nset.seed(1)\npopulation &lt;- rnorm(1000000, mean = 1.5, sd = 3)\n\n\nsamp1 &lt;- data.frame(y = sample(population, 8, replace = FALSE))\n\nsamp2 &lt;- data.frame(y = sample(population, 40, replace = FALSE))\n\n\nm1 &lt;- lm(y ~ 1, data = samp1)\nm2 &lt;- lm(y ~ 1, data = samp2)\n\nsummary(m1)\n\n\nCall:\nlm(formula = y ~ 1, data = samp1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.5322 -1.2523 -0.0883  1.3540  4.8692 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)    1.840      1.251    1.47    0.185\n\nResidual standard error: 3.539 on 7 degrees of freedom\n\n\nThe results from a simple model can be calculated by hand. The Estimate corresponds to the average of all values in the sample, from the smaller sample, samp1 we can do mean(samp1$y). This average should correspond to coef(m1) which should be 1.84. The variation of the data is most often described with the standard deviation (SD). The SD of y in the smaller sample is sd(samp1$y) (corresponding to 3.539). However, the regression table (summary(m1)) show you the standard error (SE). This statistic is an attempt to estimate the variation in a hypothetical distribution of means. The standard error is (in this simple case) \\(SE_y = \\frac{SD_y}{\\sqrt{n}}\\). Calculating by hand using the data in samp1 we would do sd(samp1$y)/sqrt(8). Amazingly this corresponds to 1.251!\nBy using the estimate 1.84 and the corresponding SE (1.251) we can calculate the t-value as the ratio \\(\\frac{Estimate}{SE}\\). The t-value may in turn be used to determine the are under the curve of a t-distribution. The t-value from the above calculation is 1.4702611. Using our single \\(n=8\\) study, we estimate that values of t, as extreme or even more extreme as our observed value both above and below 0, would occur in 18.5% of studies if the null-hypothesis was true. This corresponds to a p-value of 0.185. The figure below shows a graphical representation of a t-value distribution under the assumption that the null-hypothesis is true.\n\n\n\n\n\nA t-distribution estimated from model m1 with the shaded area corresponding to the observed p-value.\n\n\n\n\n\nIn light of what you know now about the process of conducting a study with a random sample, use your own words and…\n\nExplain the estimate, SE, t-value, and p-value from the regression models that we created previously (m1 and m2).\nDiscuss what contributes to the different results in the two studies (m1 and m2).\nWhy do we use the shaded area in the lower and upper tail of the t-distribution (See Figure @ref(fig:t-dist-fig)).\n\n\n\n\n\nBelow we will perform 1000 studies and save the results from each study. This will make it possible for us to get an actual sampling distribution. Copy the code to your own document to run the experiment.\n\n# Create data frames to store the model estimates\nresults_8 &lt;- data.frame(estimate = rep(NA, 1000), \n                      se = rep(NA, 1000), \n                      pval = rep(NA, 1000), \n                      n = 8)  \n\nresults_40 &lt;- data.frame(estimate = rep(NA, 1000), \n                      se = rep(NA, 1000), \n                      pval = rep(NA, 1000), \n                      n = 40)\n\n# A for loop used to sample 1000 studies, each iteration (i) will draw a new sample\n# from the population. \n\nfor(i in 1:1000) {\n  \n  # Draw a sample \n  samp1 &lt;- data.frame(y = sample(population, 8, replace = FALSE))\n  samp2 &lt;- data.frame(y = sample(population, 40, replace = FALSE))\n\n  # Model the data\n  m1 &lt;- lm(y ~ 1, data = samp1)\n  m2 &lt;- lm(y ~ 1, data = samp2)\n  \n  # Extract values from the models\n  results_8[i, 1] &lt;- coef(summary(m1))[1, 1]\n  results_8[i, 2] &lt;- coef(summary(m1))[1, 2]\n  results_8[i, 3] &lt;- coef(summary(m1))[1, 4]\n\n  results_40[i, 1] &lt;- coef(summary(m2))[1, 1]\n  results_40[i, 2] &lt;- coef(summary(m2))[1, 2]\n  results_40[i, 3] &lt;- coef(summary(m2))[1, 4]\n  \n  \n}\n\n\n# Save the results in a combined data frame\n\nresults &lt;- bind_rows(results_8, results_40)\n\n\nUsing the results data frame…\n\nCalculate the standard deviation of the estimate variable, and the average of the se variable for each of the study sample sizes (8 and 40). Explain why these numbers are very similar. How can you define the Standard Error (SE) in light of these calculations?\nCreate a histogram (see example code below) of the p-values from each study sample-size. How do you interpret these histograms, what do they tell you about the effect of sample size on statistical power?\nCalculate the number of studies from each sample size that declare a statistical significant effect (specify a threshold for \\(\\alpha\\), your significance level).\nUsing the pwr package, calculate the power of a one-sample t-test, with a effect size of 1.5/3, your specified significance level and sample sizes 8 and 40. Explain the results in the light of your simulations.\n\n\n\n# Example code for copy and paste\n\n# A two facets histogram can be created with ggplot2\nresults %&gt;%\n  ggplot(aes(pval)) + \n  geom_histogram() +\n  facet_wrap(~ n)\n\n\n# Count the proportion of tests below a certain p-value for each \nresults %&gt;%\n  filter(pval &lt; 0.05) %&gt;%\n  group_by(n) %&gt;%\n  summarise(sig_results = n()/1000)\n\n# Using the pwr package\nlibrary(pwr)\n\npwr.t.test(n = 40, sig.level = 0.05, d = 1.5/3, type = \"one.sample\")\n\n\n\n\nWe will now simulate a population without differences between treatment and control. The code below is very similar to the one we use above, except that we use an average effect of 0 in the population.\n\npopulation &lt;- rnorm(1000000, mean = 0, sd = 3)\n\n\n# Create data frames to store the model estimates\nresults_8 &lt;- data.frame(estimate = rep(NA, 1000), \n                      se = rep(NA, 1000), \n                      pval = rep(NA, 1000), \n                      n = 8)  \n\nresults_40 &lt;- data.frame(estimate = rep(NA, 1000), \n                      se = rep(NA, 1000), \n                      pval = rep(NA, 1000), \n                      n = 40)\n\n# A for loop used to sample 1000 studies, each iteration (i) will draw a new sample\n# from the population. \n\nfor(i in 1:1000) {\n  \n  # Draw a sample \n  samp1 &lt;- data.frame(y = sample(population, 8, replace = FALSE))\n  samp2 &lt;- data.frame(y = sample(population, 40, replace = FALSE))\n\n  # Model the data\n  m1 &lt;- lm(y ~ 1, data = samp1)\n  m2 &lt;- lm(y ~ 1, data = samp2)\n  \n  # Extract values from the models\n  results_8[i, 1] &lt;- coef(summary(m1))[1, 1]\n  results_8[i, 2] &lt;- coef(summary(m1))[1, 2]\n  results_8[i, 3] &lt;- coef(summary(m1))[1, 4]\n\n  results_40[i, 1] &lt;- coef(summary(m2))[1, 1]\n  results_40[i, 2] &lt;- coef(summary(m2))[1, 2]\n  results_40[i, 3] &lt;- coef(summary(m2))[1, 4]\n  \n  \n}\n\n\n# Save the results in a combined data frame\n\nresults_null &lt;- bind_rows(results_8, results_40)\n\n\nUsing the new data frame with results from studies of a population with an average effect of zero, create new histograms.\n\nWith a significance level of 5%, how many studies would give you a “false positive” result if you did many repeated studies?"
  },
  {
    "objectID": "assignment-3.html#setting-up-a-simulation",
    "href": "assignment-3.html#setting-up-a-simulation",
    "title": "Assignment 3: Drawing inference from statistical models, and statistical power",
    "section": "",
    "text": "In this assignment we will simulate a population of possible values, from this population we will draw random samples, calculate statistics and interpret them. The population of values can be regarded as the possible differences between two treatments in a cross-over study where participants have performed both treatments. The values in the population are calculate as \\(Treatment - Control\\).\nWe will simulate a population of one million numbers with a mean of 1.5 and a standard deviation of 3. We will make two different set of studies, one set with a sample size of 8 and one set with a sample size of 40. In order to be sure you replicate your results, include and run set.seed() before simulations in your final script.\nWe will use the lm function to estimate the average value of the population. We do this in an “intercept-only” model. This model can be written as\n\\[Y_i = \\beta_0 + \\epsilon_i\\]\nwhere \\(\\beta_0\\) is the intercept and can be interpreted as the average value of \\(Y\\), our dependent variable. \\(\\epsilon\\) is the error term, each observation (\\(i\\)) deviates from the intercept to some degree. If the intercept term is positive or negative we can interpret it as a difference between the two treatments (described above). This model is equivalent to a one-sample t-test. Let’s get started!\nIn the code chunk below, we will simulate the population of differences between treatments. We will then draw two random samples corresponding sample sizes of 8 and 40 and save these data in data frames with the dependent variable named y. We fit the very simple model y ~ 1 as a linear model and save the model object as m1 and m2.\n\nlibrary(tidyverse)\n\nset.seed(1)\npopulation &lt;- rnorm(1000000, mean = 1.5, sd = 3)\n\n\nsamp1 &lt;- data.frame(y = sample(population, 8, replace = FALSE))\n\nsamp2 &lt;- data.frame(y = sample(population, 40, replace = FALSE))\n\n\nm1 &lt;- lm(y ~ 1, data = samp1)\nm2 &lt;- lm(y ~ 1, data = samp2)\n\nsummary(m1)\n\n\nCall:\nlm(formula = y ~ 1, data = samp1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.5322 -1.2523 -0.0883  1.3540  4.8692 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)    1.840      1.251    1.47    0.185\n\nResidual standard error: 3.539 on 7 degrees of freedom\n\n\nThe results from a simple model can be calculated by hand. The Estimate corresponds to the average of all values in the sample, from the smaller sample, samp1 we can do mean(samp1$y). This average should correspond to coef(m1) which should be 1.84. The variation of the data is most often described with the standard deviation (SD). The SD of y in the smaller sample is sd(samp1$y) (corresponding to 3.539). However, the regression table (summary(m1)) show you the standard error (SE). This statistic is an attempt to estimate the variation in a hypothetical distribution of means. The standard error is (in this simple case) \\(SE_y = \\frac{SD_y}{\\sqrt{n}}\\). Calculating by hand using the data in samp1 we would do sd(samp1$y)/sqrt(8). Amazingly this corresponds to 1.251!\nBy using the estimate 1.84 and the corresponding SE (1.251) we can calculate the t-value as the ratio \\(\\frac{Estimate}{SE}\\). The t-value may in turn be used to determine the are under the curve of a t-distribution. The t-value from the above calculation is 1.4702611. Using our single \\(n=8\\) study, we estimate that values of t, as extreme or even more extreme as our observed value both above and below 0, would occur in 18.5% of studies if the null-hypothesis was true. This corresponds to a p-value of 0.185. The figure below shows a graphical representation of a t-value distribution under the assumption that the null-hypothesis is true.\n\n\n\n\n\nA t-distribution estimated from model m1 with the shaded area corresponding to the observed p-value.\n\n\n\n\n\nIn light of what you know now about the process of conducting a study with a random sample, use your own words and…\n\nExplain the estimate, SE, t-value, and p-value from the regression models that we created previously (m1 and m2).\nDiscuss what contributes to the different results in the two studies (m1 and m2).\nWhy do we use the shaded area in the lower and upper tail of the t-distribution (See Figure @ref(fig:t-dist-fig))."
  },
  {
    "objectID": "assignment-3.html#many-studies",
    "href": "assignment-3.html#many-studies",
    "title": "Assignment 3: Drawing inference from statistical models, and statistical power",
    "section": "",
    "text": "Below we will perform 1000 studies and save the results from each study. This will make it possible for us to get an actual sampling distribution. Copy the code to your own document to run the experiment.\n\n# Create data frames to store the model estimates\nresults_8 &lt;- data.frame(estimate = rep(NA, 1000), \n                      se = rep(NA, 1000), \n                      pval = rep(NA, 1000), \n                      n = 8)  \n\nresults_40 &lt;- data.frame(estimate = rep(NA, 1000), \n                      se = rep(NA, 1000), \n                      pval = rep(NA, 1000), \n                      n = 40)\n\n# A for loop used to sample 1000 studies, each iteration (i) will draw a new sample\n# from the population. \n\nfor(i in 1:1000) {\n  \n  # Draw a sample \n  samp1 &lt;- data.frame(y = sample(population, 8, replace = FALSE))\n  samp2 &lt;- data.frame(y = sample(population, 40, replace = FALSE))\n\n  # Model the data\n  m1 &lt;- lm(y ~ 1, data = samp1)\n  m2 &lt;- lm(y ~ 1, data = samp2)\n  \n  # Extract values from the models\n  results_8[i, 1] &lt;- coef(summary(m1))[1, 1]\n  results_8[i, 2] &lt;- coef(summary(m1))[1, 2]\n  results_8[i, 3] &lt;- coef(summary(m1))[1, 4]\n\n  results_40[i, 1] &lt;- coef(summary(m2))[1, 1]\n  results_40[i, 2] &lt;- coef(summary(m2))[1, 2]\n  results_40[i, 3] &lt;- coef(summary(m2))[1, 4]\n  \n  \n}\n\n\n# Save the results in a combined data frame\n\nresults &lt;- bind_rows(results_8, results_40)\n\n\nUsing the results data frame…\n\nCalculate the standard deviation of the estimate variable, and the average of the se variable for each of the study sample sizes (8 and 40). Explain why these numbers are very similar. How can you define the Standard Error (SE) in light of these calculations?\nCreate a histogram (see example code below) of the p-values from each study sample-size. How do you interpret these histograms, what do they tell you about the effect of sample size on statistical power?\nCalculate the number of studies from each sample size that declare a statistical significant effect (specify a threshold for \\(\\alpha\\), your significance level).\nUsing the pwr package, calculate the power of a one-sample t-test, with a effect size of 1.5/3, your specified significance level and sample sizes 8 and 40. Explain the results in the light of your simulations.\n\n\n\n# Example code for copy and paste\n\n# A two facets histogram can be created with ggplot2\nresults %&gt;%\n  ggplot(aes(pval)) + \n  geom_histogram() +\n  facet_wrap(~ n)\n\n\n# Count the proportion of tests below a certain p-value for each \nresults %&gt;%\n  filter(pval &lt; 0.05) %&gt;%\n  group_by(n) %&gt;%\n  summarise(sig_results = n()/1000)\n\n# Using the pwr package\nlibrary(pwr)\n\npwr.t.test(n = 40, sig.level = 0.05, d = 1.5/3, type = \"one.sample\")"
  },
  {
    "objectID": "assignment-3.html#many-studies-without-population-effect",
    "href": "assignment-3.html#many-studies-without-population-effect",
    "title": "Assignment 3: Drawing inference from statistical models, and statistical power",
    "section": "",
    "text": "We will now simulate a population without differences between treatment and control. The code below is very similar to the one we use above, except that we use an average effect of 0 in the population.\n\npopulation &lt;- rnorm(1000000, mean = 0, sd = 3)\n\n\n# Create data frames to store the model estimates\nresults_8 &lt;- data.frame(estimate = rep(NA, 1000), \n                      se = rep(NA, 1000), \n                      pval = rep(NA, 1000), \n                      n = 8)  \n\nresults_40 &lt;- data.frame(estimate = rep(NA, 1000), \n                      se = rep(NA, 1000), \n                      pval = rep(NA, 1000), \n                      n = 40)\n\n# A for loop used to sample 1000 studies, each iteration (i) will draw a new sample\n# from the population. \n\nfor(i in 1:1000) {\n  \n  # Draw a sample \n  samp1 &lt;- data.frame(y = sample(population, 8, replace = FALSE))\n  samp2 &lt;- data.frame(y = sample(population, 40, replace = FALSE))\n\n  # Model the data\n  m1 &lt;- lm(y ~ 1, data = samp1)\n  m2 &lt;- lm(y ~ 1, data = samp2)\n  \n  # Extract values from the models\n  results_8[i, 1] &lt;- coef(summary(m1))[1, 1]\n  results_8[i, 2] &lt;- coef(summary(m1))[1, 2]\n  results_8[i, 3] &lt;- coef(summary(m1))[1, 4]\n\n  results_40[i, 1] &lt;- coef(summary(m2))[1, 1]\n  results_40[i, 2] &lt;- coef(summary(m2))[1, 2]\n  results_40[i, 3] &lt;- coef(summary(m2))[1, 4]\n  \n  \n}\n\n\n# Save the results in a combined data frame\n\nresults_null &lt;- bind_rows(results_8, results_40)\n\n\nUsing the new data frame with results from studies of a population with an average effect of zero, create new histograms.\n\nWith a significance level of 5%, how many studies would give you a “false positive” result if you did many repeated studies?"
  },
  {
    "objectID": "assignment-4.html",
    "href": "assignment-4.html",
    "title": "Assignment 4: Study designs",
    "section": "",
    "text": "Choose an area of interest (e.g. protein supplementation for muscle hypertrophy or the effect of block periodization on VO2max). Find at least five original research studies1 in your selected area and describe strength and weakness of these studies (see below). The report should focus on the design of the studies and selection of statistical tests to answer study aims. Conclude your report with a recommendation, how should future studies in your area be designed to best answer similar questions?\nThe report should be handed in on canvas as a link to a github folder containing a reproducible report. This is an individual assignment.\n\n\n\nWhen analyzing your studies you can use the QALMRI method2.\nNote that the report should not contain your QALMRI table but should instead be focused on describing differences and similarities in all studies together (see also below)!\n\n\n\nWhat was the broader problem the authors are trying to resolve in the study?\nWhat are the specific questions the authors are trying to answer?\n\n\nThe first point should be similar in all your studies, this could be e.g. the effect of age on physical functioning, effect of certain training protocols on VO2max, etc. The second point is potentially different between your studies.\n\n\n\n\n\nIs the specific question framed as an hypothesis or a question?\nIf the authors have formulated a hypothesis, what alternative explanations can you think of that could potentially explain the data that the authors hypothesize?\n\n\n\n\n\nWhat is the logic of the hypothesis or the question. Try to create a “line of logic” between the introduction and the question/hypothesis.\n\n\n\n\n\nDescribe the study design. Use Hulley, (2013), Chapters 7-133 in your analysis.\nDescribe the sample and if the study defines the population.\nDescribe the method of recruiting participants to the study. Did the authors justify their sample size (i.e. did they do a power calculation)?\nDescribe how the study was conducted (what tests was performed when etc.)\nDescribe the variables in the study, what variables relate to the question/hypothesis?\nWhat methods did the authors use to make claims (what statistical tests were used)\n\n\n\n\n\nWhat were the main results of the study, did the authors answer their question/address their hypothesis?\n\n\n\n\n\nWhat could the authors conclude from the study?\nWhat did the authors conclude about the study population?\n\n\n\n\n\nYour report should not contain a detailed summary of all studies for all these questions, instead you should summarize your results. Highlight differences and similarities between studies. As the main point of this review is study designs and statistical analyses, this should be your main focus. When doing your literature review, it is however a good idea to structure it in a table with the above mentioned headings:\n\nQuestion\nAlternative\nLogic\nMethods\nResults\nInference"
  },
  {
    "objectID": "assignment-4.html#overview",
    "href": "assignment-4.html#overview",
    "title": "Assignment 4: Study designs",
    "section": "",
    "text": "Choose an area of interest (e.g. protein supplementation for muscle hypertrophy or the effect of block periodization on VO2max). Find at least five original research studies1 in your selected area and describe strength and weakness of these studies (see below). The report should focus on the design of the studies and selection of statistical tests to answer study aims. Conclude your report with a recommendation, how should future studies in your area be designed to best answer similar questions?\nThe report should be handed in on canvas as a link to a github folder containing a reproducible report. This is an individual assignment."
  },
  {
    "objectID": "assignment-4.html#details",
    "href": "assignment-4.html#details",
    "title": "Assignment 4: Study designs",
    "section": "",
    "text": "When analyzing your studies you can use the QALMRI method2.\nNote that the report should not contain your QALMRI table but should instead be focused on describing differences and similarities in all studies together (see also below)!\n\n\n\nWhat was the broader problem the authors are trying to resolve in the study?\nWhat are the specific questions the authors are trying to answer?\n\n\nThe first point should be similar in all your studies, this could be e.g. the effect of age on physical functioning, effect of certain training protocols on VO2max, etc. The second point is potentially different between your studies.\n\n\n\n\n\nIs the specific question framed as an hypothesis or a question?\nIf the authors have formulated a hypothesis, what alternative explanations can you think of that could potentially explain the data that the authors hypothesize?\n\n\n\n\n\nWhat is the logic of the hypothesis or the question. Try to create a “line of logic” between the introduction and the question/hypothesis.\n\n\n\n\n\nDescribe the study design. Use Hulley, (2013), Chapters 7-133 in your analysis.\nDescribe the sample and if the study defines the population.\nDescribe the method of recruiting participants to the study. Did the authors justify their sample size (i.e. did they do a power calculation)?\nDescribe how the study was conducted (what tests was performed when etc.)\nDescribe the variables in the study, what variables relate to the question/hypothesis?\nWhat methods did the authors use to make claims (what statistical tests were used)\n\n\n\n\n\nWhat were the main results of the study, did the authors answer their question/address their hypothesis?\n\n\n\n\n\nWhat could the authors conclude from the study?\nWhat did the authors conclude about the study population?"
  },
  {
    "objectID": "assignment-4.html#performing-your-literature-review-and-writing-the-report",
    "href": "assignment-4.html#performing-your-literature-review-and-writing-the-report",
    "title": "Assignment 4: Study designs",
    "section": "",
    "text": "Your report should not contain a detailed summary of all studies for all these questions, instead you should summarize your results. Highlight differences and similarities between studies. As the main point of this review is study designs and statistical analyses, this should be your main focus. When doing your literature review, it is however a good idea to structure it in a table with the above mentioned headings:\n\nQuestion\nAlternative\nLogic\nMethods\nResults\nInference"
  },
  {
    "objectID": "assignment-4.html#footnotes",
    "href": "assignment-4.html#footnotes",
    "title": "Assignment 4: Study designs",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAvoid using review articles or meta-analyses↩︎\nSee Teaching undergraduate students to read empirical articles: An evaluation and revision of the QALMRI method, this advice was also heavily influenced by this website↩︎\nHulley, S. B. (2013). Designing clinical research. Philadelphia, Wolters Kluwer/Lippincott Williams & Wilkins.↩︎"
  },
  {
    "objectID": "assignment-5.html",
    "href": "assignment-5.html",
    "title": "Assignment 5: Analyzing repeated measures experiments",
    "section": "",
    "text": "In this assignment you will analyze and report on trial investigating the effect of resistance training volume on lean mass and muscle strength. The data are part of the exscidata package and can be accessed as data(\"strengthvolume\") and data(\"dxadata\"). Read the instructions carefully!\n\n\nYour report should consist of the sections Introduction, Methods, Results and Discussion. Each part of the report should be written as a reproducible document and a link or reference to the repository containing the source document(s) should be included in the report. Below follows detailed descriptions and requirements for each section.\n\n\nThis section should consist of a description of the field, resistance-training volume and muscle strength and mass. Use at least five to ten references to introduce your audience and explain why you are doing the analysis/study. A tip is to use the QALMRI method, introduced in Assignment 4 to structure the reading of background information. It is up to you how you motivate the study and how you phrase the purpose of the study. It could be a hypothesis based on previous studies, it could also be question to fill a knowledge gap that you have identified in your literature review.\nStructure the introduction in paragraphs. A first paragraph could contain a general introduction to the field, why is it of interest to investigate resistance-training? A second paragraph could specifically describe the specific field of resistance-training volume, why is important to know more about how we are likely to respond to different training volumes. The second paragraph should incorporate definitions important for your report, e.g., training volume, muscle mass and strength. Try to incorporate these definition as a fluid part of the text.\nA third (or last) paragraph of the introduction should contain a statement regarding the purpose of the study. The purpose could be descriptive, hypothesis-driven or guided by a question. Although it could be considered a bit backward, you should explore the data sets before you select your question/hypothesis/purpose for it to be possible to answer.\n\n\n\nThe method should give a thorough overview of the study and specific details regarding data collection. You can read about the details of this specific study in (Hammarström et al. 2020). Use your own words to describe the study based on this description. A nice way to structure the methods section is to include subheadings:\n\nParticipants and study overview: Describe the participants and give an overview of all tests/measurements. Participants should be described in the first table of the report (Table 1). The overview of the tests/measurements should be done without double presentation as details should be presented in subsequent sections.\nSpecific descriptions (e.g. strength tests): Describe in detail how tests/measurements that you mentioned in the overview where conducted.\nData analysis and statistics: Describe how you treated the data prior to statistical tests or procedures and what tests/procedures were used to draw inference (or more generally, to answer your purpose). Describe how you present data (e.g. descriptive data with mean (SD), inference with confidence intervals etc.).\n\n\n\n\nDescribe the results of your analysis. This description should make use of table and figures as well as a text that guides and structures the content to the reader. Think about it this way, the text should describe when and how to read the figures and tables. This means that all aspects of the results should be covered in the text. The figures and tables should also be “self explanatory”, this means that you have to include descriptive figure captions and descriptions of tables (see below for tips).\nAs the main purpose of the analysis should concern the effect of training volume on muscle mass and strength, it is natural that the comparison of training outcomes between volume conditions is the main analysis in the results. You may also have questions regarding the relationship between muscle strength and mass gains, if there are differences between men and women etc. Selection of statistical/analysis techniques should reflect the study question/purpose.\n\n\n\nStructure the discussion with a first paragraph describing the main results of the analysis, this could be the answer to your question or a statement regarding the study hypothesis. In the following paragraphs discuss all results that you have presented in the light of previous studies. It is your job to give the reader plausible interpretations and explanations of your results. This is how single scientific results are incorporated in our collective understanding. These interpretations can later be challenged, however if you give the reader good arguments and clear descriptions, your insights will be valuable to collective reasoning even if they turn out to be wrong in light of new data.\nEnd the discussion with a summary or conclusion. Some journals request that you state your conclusions under a specific heading in the end of the report/article.\n\n\n\n\n\n\nThe data is already structured in the exscidata package. To access the data, use the following code:\n\nlibrary(exscidata)\ndata(\"dxadata\"); data(\"strengthvolume\")\n\nTo get and overview of the variables in each data set use ?strengthvolume and ?dxadata. In the dxadata the variables of interest are organized in a more convenient way using the code below:\n\nlibrary(tidyverse)\n\ndxadata %&gt;%\n  select(participant:include, lean.left_leg, lean.right_leg) %&gt;%\n  pivot_longer(names_to = \"leg\", \n               values_to = \"lean.mass\", \n               cols = lean.left_leg:lean.right_leg) %&gt;%\n  mutate(leg = if_else(leg == \"lean.left_leg\", \"L\", \"R\"), \n         sets = if_else(multiple == leg, \"multiple\", \"single\")) %&gt;%\n  select(participant, time, sex, include, sets, leg, lean.mass) %&gt;%\n  print()\n\n# A tibble: 160 × 7\n   participant time  sex    include sets     leg   lean.mass\n   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;     &lt;dbl&gt;\n 1 FP28        pre   female incl    multiple L          7059\n 2 FP28        pre   female incl    single   R          7104\n 3 FP40        pre   female incl    single   L          7190\n 4 FP40        pre   female incl    multiple R          7506\n 5 FP21        pre   male   incl    single   L         10281\n 6 FP21        pre   male   incl    multiple R         10200\n 7 FP34        pre   female incl    single   L          6014\n 8 FP34        pre   female incl    multiple R          6009\n 9 FP23        pre   male   incl    single   L          8242\n10 FP23        pre   male   incl    multiple R          8685\n# ℹ 150 more rows"
  },
  {
    "objectID": "assignment-5.html#organizing-the-report",
    "href": "assignment-5.html#organizing-the-report",
    "title": "Assignment 5: Analyzing repeated measures experiments",
    "section": "",
    "text": "Your report should consist of the sections Introduction, Methods, Results and Discussion. Each part of the report should be written as a reproducible document and a link or reference to the repository containing the source document(s) should be included in the report. Below follows detailed descriptions and requirements for each section.\n\n\nThis section should consist of a description of the field, resistance-training volume and muscle strength and mass. Use at least five to ten references to introduce your audience and explain why you are doing the analysis/study. A tip is to use the QALMRI method, introduced in Assignment 4 to structure the reading of background information. It is up to you how you motivate the study and how you phrase the purpose of the study. It could be a hypothesis based on previous studies, it could also be question to fill a knowledge gap that you have identified in your literature review.\nStructure the introduction in paragraphs. A first paragraph could contain a general introduction to the field, why is it of interest to investigate resistance-training? A second paragraph could specifically describe the specific field of resistance-training volume, why is important to know more about how we are likely to respond to different training volumes. The second paragraph should incorporate definitions important for your report, e.g., training volume, muscle mass and strength. Try to incorporate these definition as a fluid part of the text.\nA third (or last) paragraph of the introduction should contain a statement regarding the purpose of the study. The purpose could be descriptive, hypothesis-driven or guided by a question. Although it could be considered a bit backward, you should explore the data sets before you select your question/hypothesis/purpose for it to be possible to answer.\n\n\n\nThe method should give a thorough overview of the study and specific details regarding data collection. You can read about the details of this specific study in (Hammarström et al. 2020). Use your own words to describe the study based on this description. A nice way to structure the methods section is to include subheadings:\n\nParticipants and study overview: Describe the participants and give an overview of all tests/measurements. Participants should be described in the first table of the report (Table 1). The overview of the tests/measurements should be done without double presentation as details should be presented in subsequent sections.\nSpecific descriptions (e.g. strength tests): Describe in detail how tests/measurements that you mentioned in the overview where conducted.\nData analysis and statistics: Describe how you treated the data prior to statistical tests or procedures and what tests/procedures were used to draw inference (or more generally, to answer your purpose). Describe how you present data (e.g. descriptive data with mean (SD), inference with confidence intervals etc.).\n\n\n\n\nDescribe the results of your analysis. This description should make use of table and figures as well as a text that guides and structures the content to the reader. Think about it this way, the text should describe when and how to read the figures and tables. This means that all aspects of the results should be covered in the text. The figures and tables should also be “self explanatory”, this means that you have to include descriptive figure captions and descriptions of tables (see below for tips).\nAs the main purpose of the analysis should concern the effect of training volume on muscle mass and strength, it is natural that the comparison of training outcomes between volume conditions is the main analysis in the results. You may also have questions regarding the relationship between muscle strength and mass gains, if there are differences between men and women etc. Selection of statistical/analysis techniques should reflect the study question/purpose.\n\n\n\nStructure the discussion with a first paragraph describing the main results of the analysis, this could be the answer to your question or a statement regarding the study hypothesis. In the following paragraphs discuss all results that you have presented in the light of previous studies. It is your job to give the reader plausible interpretations and explanations of your results. This is how single scientific results are incorporated in our collective understanding. These interpretations can later be challenged, however if you give the reader good arguments and clear descriptions, your insights will be valuable to collective reasoning even if they turn out to be wrong in light of new data.\nEnd the discussion with a summary or conclusion. Some journals request that you state your conclusions under a specific heading in the end of the report/article."
  },
  {
    "objectID": "assignment-5.html#organizing-the-data-analysis",
    "href": "assignment-5.html#organizing-the-data-analysis",
    "title": "Assignment 5: Analyzing repeated measures experiments",
    "section": "",
    "text": "The data is already structured in the exscidata package. To access the data, use the following code:\n\nlibrary(exscidata)\ndata(\"dxadata\"); data(\"strengthvolume\")\n\nTo get and overview of the variables in each data set use ?strengthvolume and ?dxadata. In the dxadata the variables of interest are organized in a more convenient way using the code below:\n\nlibrary(tidyverse)\n\ndxadata %&gt;%\n  select(participant:include, lean.left_leg, lean.right_leg) %&gt;%\n  pivot_longer(names_to = \"leg\", \n               values_to = \"lean.mass\", \n               cols = lean.left_leg:lean.right_leg) %&gt;%\n  mutate(leg = if_else(leg == \"lean.left_leg\", \"L\", \"R\"), \n         sets = if_else(multiple == leg, \"multiple\", \"single\")) %&gt;%\n  select(participant, time, sex, include, sets, leg, lean.mass) %&gt;%\n  print()\n\n# A tibble: 160 × 7\n   participant time  sex    include sets     leg   lean.mass\n   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;     &lt;dbl&gt;\n 1 FP28        pre   female incl    multiple L          7059\n 2 FP28        pre   female incl    single   R          7104\n 3 FP40        pre   female incl    single   L          7190\n 4 FP40        pre   female incl    multiple R          7506\n 5 FP21        pre   male   incl    single   L         10281\n 6 FP21        pre   male   incl    multiple R         10200\n 7 FP34        pre   female incl    single   L          6014\n 8 FP34        pre   female incl    multiple R          6009\n 9 FP23        pre   male   incl    single   L          8242\n10 FP23        pre   male   incl    multiple R          8685\n# ℹ 150 more rows"
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "Her finner dere feedback på arbeidskrav i emnet.\n\n\n\nAssignment\nDue date\nIncluded in portfolio\nGroup assignment\n\n\n\n\nDescriptive statistics, reliability and validity, tools for reproducible data science\n2022-09-26\nYes\nYes\n\n\nRegression models and prediction from data\n2022-11-04\nNo\nYes\n\n\nExtraction and analysis of DNA\n2022-11-23\nOptionala\nYes\n\n\nExtraction of RNA and analysis of qPCR experiments\n2022-11-23\nOptionala\nYes\n\n\nExtraction and analysis of Protein\n2022-11-23\nOptionala\nYes\n\n\nPhilosophy of scienceb (See canvas)\n2022-10-28\nYes\nNo\n\n\nDrawing inference from statistical models and statistical power\n2022-11-11\nNo\nYes\n\n\nStudy designs\n2022-11-25\nYes\nNo\n\n\nAnalyzing repeated measures experiments\n2022-11-25\nYes\nNo\n\n\n\na Select one laboratory assignments for your portfolio exam. All groups presents one selected method on 2022-11-23. b This assignment is presented in connection with lectures."
  },
  {
    "objectID": "feedback-assignments.html",
    "href": "feedback-assignments.html",
    "title": "Sjekkliste for arbeidskrav og eksamen",
    "section": "",
    "text": "Følgende punkter er basert på de arbeidskrav som til nå er levert i emnet. Jeg oppdaterer teksten etter neste innlevering."
  },
  {
    "objectID": "feedback-assignments.html#å-skrive-rapporter",
    "href": "feedback-assignments.html#å-skrive-rapporter",
    "title": "Sjekkliste for arbeidskrav og eksamen",
    "section": "Å skrive rapporter",
    "text": "Å skrive rapporter\n\nUnngå å forfatte rapporter i punkter/lister. Denne formen av tekst kan hjelpe leseren men kan også bryte opp og skape unødvendig forvirring. En punktliste kan også få spørsmålet «hvorfor» å forsvinne. Leseren kan være interessert i å vite hvorfor man velger en tilnærming eller metode.\nSkriv for en lesere som er kjent med området men savner informasjon om hvorfor man skal lese rapporten. Du må fylle i «blanks» hos leseren!\nRapporter kan med fordel struktureres som introduksjon, metode, resultat og diskusjon. Man kan velge å ikke ha med disse overskriftene men til tross organisere paragrafene på denne måten. Introduksjonen forteller leseren hva teksten skal belyse, metoden gir et innblikk i hva og hvordan du gjennomført arbeidet, resultatene beskriver og diskusjon tolker.\nVær sikker på at begreper og metoder er definerte! «Typical error», «Coefficient of variation», «laktatterskel» osv. kan være mange ulike ting. Man kan bruke en referanse eller en formel for å definere beregninger, referanser for konsepter osv.\nHusk å definer feilstapler (error bars), farger, punkter enheter osv. I tabeller, figurer og tekst."
  },
  {
    "objectID": "feedback-assignments.html#spesifikt-innhold",
    "href": "feedback-assignments.html#spesifikt-innhold",
    "title": "Sjekkliste for arbeidskrav og eksamen",
    "section": "Spesifikt innhold",
    "text": "Spesifikt innhold\n\nArbeidskrav 1\n\nSe til å organiser github-mapper på en overskuelig måte. En metode for å underlette for leseren er å bruke en README.md. Denne vil bli konvertert til HTML på github og kan dermed gi et overblikk over mappen. Hvis du trenger å spare gamle filer så kan disse flyttes til en mappe («archive» eller lignende).\nDet er viktig å vise at du bruker kilder som er angitt i oppgaven. Bruk disse for å vise til definisjoner og gi bakgrunn til hvorfor du ønsker å vise dine resultater til leseren.\nHva betyr CV, typical error osv. som du beregnet? Her holder det ikke bare med definisjoner, prøv deg også på en tolkning. En måte kan være å bruke annen litteratur som beregnet CV, eller sammenligne med et annet test som du gjort.\nNoen detaljer i en metode kan med fordel holdes kort og kanskje til og med utelates da det tilhører eks. «good laboratory practice». Her kan man spørre seg hvilken informasjon som kreves for å repetere forsøkene gitt at man praktiserer gode rutiner i laboratoriet?\n\n\n\nArbeidskrav 2\n\nAlle gruppene/rapportene er meget sparsomt skrevne. Bruk rapportens deler til å skrive noe om de grafer og analysene som dere presenter. Hvordan kan man tolke laktatterskel? hvorfor predikerer man størrelse på DNA fragmenter? Etc!\nStandard error er spredning i en estimert fordeling av «teoretiske» utvalg. Den forteller ikke hvor bra modellen er, men snarere med hvilken usikkerhet vi bør tolke estimatet. Hvordan skiller denne tolkningen fra deres tolkning?\nT-verdien i en regresjonstabell er en ratio mellom estimat og SE, denne brukes for å beregne p-verdien. P-verdien i sin tur forteller oss om hvor usannsynlig vår data, eller enda mer ekstrem data er hvis nullhypotesen er sann. Hvordan kan dere justere beskrivelsen av regresjonsoutput basert på denne beskrivelsen?\n\n\n\nArbeidskrav 3\nSpørsmål 1-3:\n\nForskjell i SE mellom modellene henger sammen med utvalgsstørrelsen. P-verdien forteller ikke sannsynligheten for at null-hypotesen er sann. Hvordan henger utvalgsstørrelse sammen p-verdien?\nDen skyggete delen av t-fordeling er resultater like eller mer ekstreme enn et observert resultat. T-fordelingen representerer vårt beste estimat av mulige resultater under null-hypotesen.\n\nSpørsmål 4:\n\nHer er del lurt å igjen definere SE! Den gjennomsnittlige standardfeilen er et estimat på spredningen i utvalgsfordelingen! Utvalgsfordelingen er alle beregnede gjennomsnittene!\n\nSpørsmål 5-7:\n\nHvordan kan vi bruke dette for å definere statistisk styrke? Statistisk styrke kan defineres ved hjelp av tankeeksperimentet at man gjennomfører 1000 studier på samme populasjon. Hvor mange vill finne en definert effekt når man setter grensen for p-verdien og utvalgsstørrelsen til et spesifikt nivå?\n\nSpørsmål 8:\n\nNår vi ikke har noen effekt så vil vi finne en effekt i et antall studier basert på den grense vi setter på \\(\\alpha\\). Dette betyr at \\(\\alpha\\) definerer våre antall falske positive fynd og at fordeling av p-verdier ved ingen effekt er uniform, hva betyr dette?\n\n\n\nArbeidskrav 4:\n\nHusk primær målsetting med denne oppgaven: «fokus på design av studiene og valg av statistiske metoder/test for å besvare studienes problemstilling». For å løfte flere av deres tekster kan man tenke at man fokuserer på å sammenligne studiene på disse punktene.\nJeg savner noe informasjon i mange arbeidskrav om studiedesigner i stort, hvordan forholder seg de studier dere har analysert til andre studiedesigner. Her kan tenkes at man gir en generell innføring i studiedesigner i en paragraf for å vise for leseren hvordan man kan forstå de studiene som du har analysert. Det er da også mulig å bruke forslag på pensum!:)\nBruk gjerne også noe plass på å beskrive de statistiske metodene som blir brukt, dette kan med fordel gjøres med henvisning til pensum (eller annen passende litteratur). Har du kjennskap til testene som blir brukt på den nivå at du vet hvilke kommandoen som skal brukes i R for å gjenskape analysene?\nTil tross for at jeg foreslå at man kan bruke QALMRI så sa jeg også at rapporten ikke skulle inneholde QALMRI-tabellen. (Jeg vil ikke underkjenne oppgaver som har QALMRI som bærende struktur).\nTenk på om din tekst kan bli mer strukturert ved å bruke tydeligere temaer per paragraf. Når en paragraf er veldig lang, og inneholder flere ulike temaer blir det vanskelig for leseren å følge. En lettlest paragraf har kanskje 100-200 ord!\nI oppgaven er det lov på kommer med noen meninger om studiedesignene, eller analysene er «gode», hva bør gjøres annerledes for å lage bedre vitenskap. Et eksempel som flere er inne på er statistiske tester innad eksperimentelle grupper, hva sier disse oss?\nEn tabell eller figur er et veldig godt innslag i en rapport. Men den må følges opp i løpende tekst.\n\nArbeidskrav 5\n\nBruk tabeller for det mest sentrale. En kortere liste på eksempelvis øvelser, eksklusjonskriterier osv. kan skrives i løpende tekst. Når man har en mer kompleks fremstilling kan tabellen hjelpe.\nHusk at %-vis endring passer mindre bra for statistiske analyser. Det finnes noen unnatak, når vi log-transformerer så blir tilbake-transformerte data % endring (pga loven om logaritmer, log(a) – log(b) = log(a/b)). Istedenfor % endring i statistiske analyser kan man bruke absolutte tall og eventuelt konvertere til % endring for fremvisning av resultater.\nVed å bruke ordet signifikant så gjør man av tradisjon et utsagn om et statistikk test, vær nøyaktig med hvilken sammenligning som er «signifikant».\nI metodeavsnitt så bør rapporten bare beskrive de deler som rapporten omhandler. Dersom man f.eks. ikke presenterer muskelbiopsidata bør disse ikke presenteres. Tanken her er å lage en rapport som etterligner men ikke er en kopi av originalrapporten!\nStudiedesignen kan stille till problemer da vi måler styrke og muskelvekst innad deltakere. Vi kan bruke mixed-effects models, men også forenkle. Under følger eksempler på modeller for muskelmassedataene (den samme tilnærmingen kan brukes for styrke).\n\n\nlibrary(tidyverse); library(exscidata); library(lme4)\n\n# Load data\ndat &lt;- dxadata %&gt;%\n  select(participant:include, lean.left_leg, lean.right_leg) %&gt;%\n        \n        # Extract leg specific data\n  pivot_longer(names_to = \"leg\", \n               values_to = \"lean.mass\", \n               cols = lean.left_leg:lean.right_leg) %&gt;%\n  mutate(leg = if_else(leg == \"lean.left_leg\", \"L\", \"R\"), \n         sets = if_else(multiple == leg, \"multiple\", \"single\")) %&gt;%\n  select(participant, time, sex, include, sets, leg, lean.mass) %&gt;%\n        # Filter only included participants\n        filter(include == \"incl\") %&gt;%\n        # Make data set wider by time \n        \n        pivot_wider(names_from = \"time\", \n                    values_from = \"lean.mass\") %&gt;%\n        # Calculate change score\n        \n        mutate(change = post - pre) %&gt;%\n        \n        # Keep change score and make it wider by sets\n        select(participant:sets, change) %&gt;%\n        pivot_wider(names_from = sets, values_from = change) %&gt;%\n        \n  print()\n\n# A tibble: 34 × 5\n   participant sex    include multiple single\n   &lt;chr&gt;       &lt;chr&gt;  &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n 1 FP28        female incl         214    123\n 2 FP40        female incl         -69      2\n 3 FP21        male   incl         619    189\n 4 FP34        female incl         396    312\n 5 FP23        male   incl        -205    445\n 6 FP36        female incl         587    386\n 7 FP38        female incl         -85    225\n 8 FP25        male   incl         373    -47\n 9 FP19        male   incl         302    127\n10 FP13        male   incl         734    915\n# ℹ 24 more rows\n\n### Use simple t-test on change score\n\nt.test(dat$multiple, dat$single, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  dat$multiple and dat$single\nt = 2.1875, df = 33, p-value = 0.0359\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n   8.586109 237.002126\nsample estimates:\nmean difference \n       122.7941 \n\n\n\nDenne enkle måten å analysere dataene på er helt OK. Et annet alternativ er en “mixed-effects” modell.\n\n\n# Load data\ndat &lt;- dxadata %&gt;%\n  select(participant:include, lean.left_leg, lean.right_leg) %&gt;%\n        \n        # Extract leg specific data\n  pivot_longer(names_to = \"leg\", \n               values_to = \"lean.mass\", \n               cols = lean.left_leg:lean.right_leg) %&gt;%\n  mutate(leg = if_else(leg == \"lean.left_leg\", \"L\", \"R\"), \n         sets = if_else(multiple == leg, \"multiple\", \"single\")) %&gt;%\n  select(participant, time, sex, include, sets, leg, lean.mass) %&gt;%\n        # Filter only included participants\n        filter(include == \"incl\") %&gt;%\n        # Fix time factor \n        mutate(time = factor(time, levels = c(\"pre\", \"post\"))) %&gt;%\n        \n  print()\n\n# A tibble: 136 × 7\n   participant time  sex    include sets     leg   lean.mass\n   &lt;chr&gt;       &lt;fct&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;     &lt;dbl&gt;\n 1 FP28        pre   female incl    multiple L          7059\n 2 FP28        pre   female incl    single   R          7104\n 3 FP40        pre   female incl    single   L          7190\n 4 FP40        pre   female incl    multiple R          7506\n 5 FP21        pre   male   incl    single   L         10281\n 6 FP21        pre   male   incl    multiple R         10200\n 7 FP34        pre   female incl    single   L          6014\n 8 FP34        pre   female incl    multiple R          6009\n 9 FP23        pre   male   incl    single   L          8242\n10 FP23        pre   male   incl    multiple R          8685\n# ℹ 126 more rows\n\n### Use a mixed model to determine effects of time and condition\n\nm &lt;- lmer(lean.mass ~ time + time:sets + (1|participant), \n          data = dat)\n\n\nsummary(m)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: lean.mass ~ time + time:sets + (1 | participant)\n   Data: dat\n\nREML criterion at convergence: 2022.2\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.72697 -0.60997 -0.03598  0.46355  2.78820 \n\nRandom effects:\n Groups      Name        Variance Std.Dev.\n participant (Intercept) 4181094  2044.8  \n Residual                  57202   239.2  \nNumber of obs: 136, groups:  participant, 34\n\nFixed effects:\n                    Estimate Std. Error t value\n(Intercept)          8603.53     353.07  24.368\ntimepost              289.06      58.01   4.983\ntimepre:setssingle    -14.53      58.01  -0.250\ntimepost:setssingle  -137.32      58.01  -2.367\n\nCorrelation of Fixed Effects:\n            (Intr) timpst tmpr:s\ntimepost    -0.082              \ntmpr:stssng -0.082  0.500       \ntmpst:stssn  0.000 -0.500  0.000\n\nconfint(m)\n\n                        2.5 %     97.5 %\n.sig01              1614.7219 2609.47559\n.sigma               206.6486  272.06561\n(Intercept)         7902.4449 9304.61392\ntimepost             175.9888  402.12889\ntimepre:setssingle  -127.5995   98.54066\ntimepost:setssingle -250.3936  -24.25346\n\n\n\nI modellen over gis ingen korreksjon for “baseline” i sammenligningen post. Dette er en legitim måte å analysere dataene på når vi kan anta at forskjeller ved baseline bare er målefeil.\nTil sist kan vi prøve en ANCOVA, denne må og ta hensyn til at dataene kommer fra samme individ to ganger\n\n\n# Load data\ndat &lt;- dxadata %&gt;%\n  select(participant:include, lean.left_leg, lean.right_leg) %&gt;%\n        \n        # Extract leg specific data\n  pivot_longer(names_to = \"leg\", \n               values_to = \"lean.mass\", \n               cols = lean.left_leg:lean.right_leg) %&gt;%\n  mutate(leg = if_else(leg == \"lean.left_leg\", \"L\", \"R\"), \n         sets = if_else(multiple == leg, \"multiple\", \"single\")) %&gt;%\n  select(participant, time, sex, include, sets, leg, lean.mass) %&gt;%\n        # Filter only included participants\n        filter(include == \"incl\") %&gt;%\n        # Fix time factor \n        mutate(time = factor(time, levels = c(\"pre\", \"post\"))) %&gt;%\n        # Pivot wider by time\n        pivot_wider(names_from = time, \n                    values_from = lean.mass) %&gt;%\n        \n  print()\n\n# A tibble: 68 × 7\n   participant sex    include sets     leg     pre  post\n   &lt;chr&gt;       &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 FP28        female incl    multiple L      7059  7273\n 2 FP28        female incl    single   R      7104  7227\n 3 FP40        female incl    single   L      7190  7192\n 4 FP40        female incl    multiple R      7506  7437\n 5 FP21        male   incl    single   L     10281 10470\n 6 FP21        male   incl    multiple R     10200 10819\n 7 FP34        female incl    single   L      6014  6326\n 8 FP34        female incl    multiple R      6009  6405\n 9 FP23        male   incl    single   L      8242  8687\n10 FP23        male   incl    multiple R      8685  8480\n# ℹ 58 more rows\n\n### Use a mixed model to determine effects of time and condition\n\nm &lt;- lmer(post ~ pre + sets + (1|participant), \n          data = dat)\n\n\nsummary(m)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: post ~ pre + sets + (1 | participant)\n   Data: dat\n\nREML criterion at convergence: 966.1\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.2720 -0.5705  0.0531  0.4515  1.5658 \n\nRandom effects:\n Groups      Name        Variance Std.Dev.\n participant (Intercept) 90183    300.3   \n Residual                53840    232.0   \nNumber of obs: 68, groups:  participant, 34\n\nFixed effects:\n              Estimate Std. Error t value\n(Intercept)  230.12571  260.08567   0.885\npre            1.00685    0.02927  34.401\nsetssingle  -122.69459   56.27822  -2.180\n\nCorrelation of Fixed Effects:\n           (Intr) pre   \npre        -0.968       \nsetssingle -0.116  0.008\nfit warnings:\nSome predictor variables are on very different scales: consider rescaling\n\nconfint(m)\n\n                   2.5 %     97.5 %\n.sig01       203.3519123 399.741610\n.sigma       183.2295834 296.089498\n(Intercept) -274.8628148 749.191873\npre            0.9483379   1.063678\nsetssingle  -234.5001167 -10.850906\n\n\n\nlme4 gir oss ikke p-verdier. Men vi kan bruke confint() for å beregne konfidensintervaller. Disse kan brukes får inferens.\nMan kan være intressert i å få ut estimate fra modellene, her kan pakken emmeans hjelpe. La si at du ønsker å estimere post-skår fra modellen m.\n\n\nlibrary(emmeans)\n\n# store estimated marginal means in new object\nem &lt;- emmeans(m, specs = ~ sets)\n\n# For plotting you can make the estimates to a data frame\ndata.frame(em)\n\n      sets   emmean       SE       df lower.CL upper.CL\n1 multiple 8885.274 65.08461 46.04908 8754.269 9016.279\n2   single 8762.579 65.08461 46.04908 8631.574 8893.584\n\n\n\n\nReproduserbare rapporter på Github\n\nMed en reproduserbar rapport mener jeg at tekst, kode og data blir brukt til å skape et «output» som en pdf-, html- eller word-fil. Fordelen med github (og lignende løsninger) er at man har mulighet å samle alle delene i et versjonskontrollsystem. Dette har fordeler for deg som forfatter og for vitenskapelig arbeid i stort (transparens, muligheter å reprodusere osv.). Hensikten med å bruke Github i emnet er å øve på denne måten å lage rapporter.\n\nMed det sagt vil jeg ikke underkjenne rapporter som ikke er reproduserbar i denne betydningen. - Vær konsekvent med filstruktur, filnavn osv på github. Filer som ikke blir brukt bør f.eks. flyttes til en annen mappe.\n\nKontroller at din mappe går å laste ned fra github, at den seneste versjonen av prosjektet ligger oppe osv!"
  },
  {
    "objectID": "feedback-assignments.html#å-bruke-r",
    "href": "feedback-assignments.html#å-bruke-r",
    "title": "Sjekkliste for arbeidskrav og eksamen",
    "section": "Å bruke R",
    "text": "Å bruke R\n\nR bruker punktum som kommaseparerare. For en helhetlig rapport kan det være lurt å velge en måte å rapportere resultater på.\nHensikten/styrken med reproduserbar rapportering er at tall, tabeller, figurer osv. er direkte koblet til dataene. Prøv å unngå å skrive inn tall direkte. Bruk isteden variabler (eks. var1) som du skaper i code chunks och settes inn i teksten. R vill i dette fallet lete etter en variable i environment som heter var1. Se her https://rmarkdown.rstudio.com/lesson-4.html for mer info.\nÅ definere innstillinger for code chunks vil gjøre rapportene lettere å lese. F.eks. ved å inkludere koden som beskrives under vil ta vekk meddelende, «warnings» og koden fra din rapport. Hus at innstillinger må settes først i code chunken.\n\n#| echo: false\n#| message: false\n#| warning: false\n\n\n\nUnngå å bruke print() i en rapport da dette vil resultere i uformatert tekst. Bruk istedenfor tabellverktøy (som gt eller knitr::kable()) for tabeller.\nSe her Quarto - Citations & Footnotes for hvordan du kan sette en egen overskrift for referanser og inkludere referanser hvor du ønsker."
  },
  {
    "objectID": "feedback-assignments.html#innlevering-av-eksamen",
    "href": "feedback-assignments.html#innlevering-av-eksamen",
    "title": "Sjekkliste for arbeidskrav og eksamen",
    "section": "Innlevering av eksamen",
    "text": "Innlevering av eksamen\n\nEksamen skal leveres som en pdf på inspera.\nI pdf:en skal det stå beskrevet hvor man kan finne data og kod (beskriv dette under preface i mallen).\nJeg har skapt en mal for innlevering av eksamen, for å bruke denne:\n- Lag en `fork` av `https://github.com/dhammarstrom/innlevering-idr4000-qmd` til din egen github bruker.\n- Last ned din `fork` til R Studio og oppdatere qmd-filene med dine tekster og repository med dine data.\n- For å lage pdf:en, tryck på render. Det kan kreves flere render for å løse problemer som oppstår. Se til å levere den endlige versjonen!\n- Den pdf som skapes finner du i mappen `_book`\n- Legg til endringer (gi add -A) og commit (git commit -m ‘a message’) og push (git push). Din versjon av innleveringsmappen er nå oppdatert på din github profil\nFor å bruke denne løsningen kreves at du har quarto installert (https://quarto.org/docs/get-started/).\nDet kreves også en installasjon av TeX, quarto sier at TinyTeX er et godt alternativ (se https://quarto.org/docs/output-formats/pdf-engine.html)\nDu kan være interessert av å forandre automatisk generert tekst i rapporten, eks. figurtekster, overskrifter. Se i .yml-filen for å endre disse innstillingene."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quantitative methods and statistics (In Sport and Exercise Science)",
    "section": "",
    "text": "Welcome to the course Quantitative methods and Statistics (IDR4000). The course aims to give students an overview of methodological aspects within the field of sport and exercise-physiology. Specifically, planning, conducting and analyzing research projects with human participants will be covered. The lecture notes for the course can be found here. This website will contain tutorials and workshops that we will work on in class."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Quantitative methods and statistics (In Sport and Exercise Science)",
    "section": "",
    "text": "Welcome to the course Quantitative methods and Statistics (IDR4000). The course aims to give students an overview of methodological aspects within the field of sport and exercise-physiology. Specifically, planning, conducting and analyzing research projects with human participants will be covered. The lecture notes for the course can be found here. This website will contain tutorials and workshops that we will work on in class."
  },
  {
    "objectID": "index.html#practical-information",
    "href": "index.html#practical-information",
    "title": "Quantitative methods and statistics (In Sport and Exercise Science)",
    "section": "Practical information",
    "text": "Practical information\nThese notes were updated on r Sys.Date() and cover the course held during 2023 autumn semester. Contact Daniel Hammarström if you have any questions regarding this content.\n\nLearning objectives\nLearning objectives can be read in Norwegian here.\n\n\nLearning strategies\nThe course will include lectures, laboratory exercises, computer exercises and workshops, seminars and student presentations. Lectures/Workshops will be held in-person.\nComputer exercises will eventually require that you have special computer software installed on your computer. The software is free (see specific chapters in lecture notes)."
  },
  {
    "objectID": "index.html#assignments-and-portfolio-exam",
    "href": "index.html#assignments-and-portfolio-exam",
    "title": "Quantitative methods and statistics (In Sport and Exercise Science)",
    "section": "Assignments and Portfolio exam",
    "text": "Assignments and Portfolio exam\nThe course is based on several assignments. Some of these assignments are to be handed in as part of a portfolio exam upon which your grade is based.\nAssignments that are due during the course (arbeidskrav) are expected to be further improved after feedback from fellow students and teachers before inclusion in your portfolio.\nSee this GitHub repository for an overview"
  },
  {
    "objectID": "notes1-linear-models.html",
    "href": "notes1-linear-models.html",
    "title": "Notes: Univariate regression models by hand",
    "section": "",
    "text": "The univariate regression model has one dependent and one independent variable. The goal is to determine the relationship between the two variables.\nHere we will explore the mathematics of this model and replicate results from R “by hand”.\nFirst, let’s simulate some data.\nlibrary(tidyverse)\n\nset.seed(100)\n\nx &lt;- runif(10, 5, 10)\ny &lt;- 10 + 2 * x + rnorm(10, 0, 1)\n\n\nggplot(data = data.frame(x, y), \n       aes(x, y)) + geom_point()"
  },
  {
    "objectID": "notes1-linear-models.html#a-categorical-predictor-variable",
    "href": "notes1-linear-models.html#a-categorical-predictor-variable",
    "title": "Notes: Univariate regression models by hand",
    "section": "A categorical predictor variable",
    "text": "A categorical predictor variable\n\nxcat &lt;- ifelse(x &lt; 7, 0, 1)\n\n\n## Calculate the correlation \nzx &lt;- (xcat - mean(xcat)) / sd(xcat)\nzy &lt;- (y - mean(y)) / sd(y)\n\n\nrxy &lt;- sum(zx * zy) / (length(xcat) - 1)\n\n\n## Calculate the slope \n\nb1 &lt;- rxy * (sd(y)/sd(xcat))\n\nb0 &lt;- mean(y) - b1 * mean(xcat)\n\n\nb1\n\n[1] 3.891868\n\nb0\n\n[1] 22.09147\n\n## Calculate the errors\n\ne &lt;- y - (b0 + b1*xcat)\n\n## \n\nvarx &lt;- (sum((xcat-mean(xcat))^2))/length(xcat)\n\nseb0 &lt;- se/sqrt(length(xcat)) * sqrt(1 + ((mean(xcat)^2) / varx))\n\nsx &lt;- sqrt(sum((xcat - mean(xcat))^2) / length(xcat))\n\nseb1 &lt;- se/sqrt(length(y)) * 1 /  sx\n\n\n## Calculate"
  },
  {
    "objectID": "workshops.html",
    "href": "workshops.html",
    "title": "Workshops",
    "section": "",
    "text": "Workshop\nAdditional material\n\n\n\n\nIntroduction to data science (Norwegian)\n\n\n\nInstalling and starting R\n\n\n\nCreating your first graph"
  },
  {
    "objectID": "ws1-data-science-intro.html",
    "href": "ws1-data-science-intro.html",
    "title": "Introduksjon til datavitenskap og data i praksis",
    "section": "",
    "text": "Diskuter i grupper (3-4 pers, 15 min):\n\nHvilke verktøy for reproduserbar dataanalyse er du kjent med i dag? På hvilken måte kan disse verktøyene brukes for reproduserbar dataanalyse?\nDiskuter noen scenarioer hvor du, i forskjellige yrkesroller (f.eks. helse-omsorg, idrett, undervisning, forskning) ville hatt bruk av PPDAC (eller lignende datadrevet grunnlag for beslutninger). Vær realistisk! Tenk på noen spesifikke arbeidsoppgaver.\nHvilke er de største hindringene for lære «datavitenskap» som beskrevet i pensum/forelesning?\n\n\n\n\nBruk Broman og Woo (Broman and Woo 2018) for å analysere et datasett. Oppgaven er å finne problemer og foreslå løsninger. Gjennomfør øvelsen i gruppe (3-4 pers, 20 min):\n\nAlle leser punkt 1 (Introduction)\nGruppe 1 analysere datasett ved hjelp av punktene 2-5\nGruppe 2 analysere ved hjelp av 6-8\nGruppe 3 bruker punktene 10-13\n\nNår det er mulig, forandre filen til det bedre. Noter når det er ikke er mulig eller gir uønskede konsekvenser."
  },
  {
    "objectID": "ws1-data-science-intro.html#hvordan-forstå-å-bruke-datavitenskap",
    "href": "ws1-data-science-intro.html#hvordan-forstå-å-bruke-datavitenskap",
    "title": "Introduksjon til datavitenskap og data i praksis",
    "section": "",
    "text": "Diskuter i grupper (3-4 pers, 15 min):\n\nHvilke verktøy for reproduserbar dataanalyse er du kjent med i dag? På hvilken måte kan disse verktøyene brukes for reproduserbar dataanalyse?\nDiskuter noen scenarioer hvor du, i forskjellige yrkesroller (f.eks. helse-omsorg, idrett, undervisning, forskning) ville hatt bruk av PPDAC (eller lignende datadrevet grunnlag for beslutninger). Vær realistisk! Tenk på noen spesifikke arbeidsoppgaver.\nHvilke er de største hindringene for lære «datavitenskap» som beskrevet i pensum/forelesning?"
  },
  {
    "objectID": "ws1-data-science-intro.html#data-in-the-wild",
    "href": "ws1-data-science-intro.html#data-in-the-wild",
    "title": "Introduksjon til datavitenskap og data i praksis",
    "section": "",
    "text": "Bruk Broman og Woo (Broman and Woo 2018) for å analysere et datasett. Oppgaven er å finne problemer og foreslå løsninger. Gjennomfør øvelsen i gruppe (3-4 pers, 20 min):\n\nAlle leser punkt 1 (Introduction)\nGruppe 1 analysere datasett ved hjelp av punktene 2-5\nGruppe 2 analysere ved hjelp av 6-8\nGruppe 3 bruker punktene 10-13\n\nNår det er mulig, forandre filen til det bedre. Noter når det er ikke er mulig eller gir uønskede konsekvenser."
  },
  {
    "objectID": "ws2-installing-r.html",
    "href": "ws2-installing-r.html",
    "title": "Starting up R",
    "section": "",
    "text": "Different statistical software packages. Source: The internet!\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAs the image above suggests, the software we will use in this class is Exciting and a bit Dangerous!\n\n\n\n\nR is an open-source programming language1 that has a well developed system for user contribution. This means that it is possible to write extensions, these are also called packages. Packages contain specialized programs designed to solve different tasks.\nThe R software, together with packages can be found at CRAN, the Comprehensive R Archive Network. You can access CRAN from R.\n\n\nRStudio is an Integrated Development Environment. This software makes it easy to “talk” to R by keeping track of your files, any data stored in your computers memory and so on. RStudio contains an editor that makes it easy to edit text files that can be interpreted by R.\nWe will get to know RStudio by checking out:\n\nThe console\nEnvironment and history\nFiles, Plots, Packages, Help and Viewer\nGlobal options\n\nImportantly we will change the option concerning “Save workspace to .RData on exit” to NEVER. This will make your work dependent on what you write in your “programs” which is a good thing for reproducibility.\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nTell a friend what the words package, CRAN, and open-source means\nExplain to a friend what you do with the R console\nExplain what the environment and history contains\nWhere do you install packages?\nWhy would you change “Save workspace to .RData on exit” to never under Global options?\n\n\n\n\n\n\n\n\nA good thing about R is that you will have to script your work. This essentially creates a “program” that R will interpret. The interpretation of your program will result in some output (e.g., a graph, table, a single number or a report).\nThis separates R from point-and-click or spreadsheet software. This also makes it easier to reproduce an analysis.\nR has many flavors of scripts as some combines text with statistical programming that in the end creates a nice looking report. We will talk more about these variants later.\n\n\n\n\n\nAn object can be a number, a collection of numbers or many other representations of data that is stored in the workspace.\nStart a new script from the File menu (File &gt; New File &gt; R Script), copy the code from the code block below to your script.\nExecute the code by placing the cursor on the row containing the code and pres Ctrl+Enter (Cmd+Enter on Mac).\n\na &lt;- 12\n\nThe &lt;-operator assigns the number 12 to an object called a. This object is stored in the workspace. We can point the arrow in the other direction also.\n\n12 -&gt; b\n\nAssignment using &lt;- or -&gt; can go either way. Alternatively you could use = which will have the same function as &lt;-. Try to assign 12 to c by using the following code 12 = c. You should encounter an error message.\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\nExplain the following sentence in your own words: “The &lt;-operator assigns the number 12 to an object called a. This object is stored in the workspace.”\n\n\n\n\n\n\nObjects can be, depending on their type, combined. Numeric objects can be used in mathematical operations. What is the result of the following code?\n\na + b\n\nMore complex mathematical treatment is also possible using objects.\n\nx &lt;- 12.3 + 3 * a + -1.2 * sqrt(b)\n\nThe sqrt(b) part in the formula above is the first occurance of a R function. It takes a number (or a vector of numbers) and return the square root.\nFor every function, there is a help page. The help pages for sqrt() can be accessed by typing ?sqrt() in the console.\n\n\n\n\nVectors are a combination of data that can be manipulated. The R function c() combines data into a vector.\n\nv &lt;- c(2, 5, 7, 3, 1)\n\nThe vector v has five numbers (2, 5, 7, 3, 1), we can combine these with another value, let’s say the value stored in a\n\nv + a\n\nHowever, if we combine it with a and b, we run into problems.\n\nv + c(a, b)\n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\nExplain the following in your own words why v + c(a, b) resulted in a warning message.\n\n\n\nLet’s add another value to v and try to combine the values stored in v with a and b.\n\nv &lt;- c(v, 1)\n\nv + c(a, b)\n\nWe can examine objects by using R functions such as:\n\nlength(v)\n\nstr(v)\n\nlength(a)\n\nclass(a)\n\n\n\n\nData can be further combined into data frames. These are tabular representations of data with where a row indicate an observation and a column represents a variable. We can create a new data frame using the following code:\n\ndf &lt;- data.frame(v = v, \n                 a = a, \n                 color = c(\"red\", \"red\", \"red\", \"green\", \"green\", \"green\"))\n\nNote that I use the = operator inside the data.frame function. This is a convenient way of separating assignments to the environment/objects from isnside e.g. data abojects or functions.\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\nExplain how the object a multiplies in the data frame function. Explore, what is the class of df?\n\n\n\nData in a data frame can be accessed in multiple ways. First, to extract a specific variable, we can use the $ operator.\n\ndf$color\n\nThe code above prints the content of the variable color as a vector of characters. A character vector is a collection of text that has no further meaning to R (but might have to us!).\nWe can also extract a specific value using brackets. To extract the second row, from the third column, we would write:\n\ndf[2, 3]\n\n(We can remember this by saying out loud ROW-COLUMN).\nNew variables can be added to the data frame by assignment\n\ndf$new_variable &lt;- df$v * 3.14 \n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\nExplain in your own words what happend in the code chunk above.\n\n\n\nWe can also add variables to the data frame that are the results of “tests” of the data.\n\ndf$another_variable &lt;- df$new_variable &gt; 10 \n\nThe operator &gt; tests if observations in the variable new_variable are larger than 10. This results in a logical vector containing TRUE or FALSE. These are special vectors called logical vectors that can be either TRUE or FALSE.\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\nWe have covered numeric, logical and character vectors. Explain the differences to your friend.\n\n\n\n\n\nThe character vector color contained in the data frame df is a character vector. However, we can transform it to a factor. A factor can be ordered by your specifications. We can also add labels. Let’s reorganize the color variable.\n\ndf$color &lt;- factor(df$color) \n\ndf$color\n\nRun the code above and you will see that by converting the character vector to a factor we ge a new data type that has levels in alphabetical order. We can change the levels to what ever we want using the factor function and the argument levels.\n\ndf$color &lt;- factor(df$color, levels = c(\"red\", \"green\")) \n\ndf$color\n\nThis was the first mention of arguments. Every function usually takes user specified arguments that determines the output from the function. Possible arguments can be reviewed by accessing e.g. ?factor.\n\n\n\n\nIt is good practice to name variables or objects with words that are meaningful to what you are doing. A variable/object name cannot start with a number, it cannot contain spaces or special characters.\nA good programmer is lazy. Avoid mixing large and small letters, reduce the number of symbols in variable names without loosing meaning and be consistent.\n\n\n\nUsing the built in system for plotting, we can create a first figure of our data. This plot has a lot of potential for improvements. We will explore this potential in the next workshop!\n\nplot(df$v, df$new_variable)"
  },
  {
    "objectID": "ws2-installing-r.html#the-r-ecosystem",
    "href": "ws2-installing-r.html#the-r-ecosystem",
    "title": "Starting up R",
    "section": "",
    "text": "R is an open-source programming language1 that has a well developed system for user contribution. This means that it is possible to write extensions, these are also called packages. Packages contain specialized programs designed to solve different tasks.\nThe R software, together with packages can be found at CRAN, the Comprehensive R Archive Network. You can access CRAN from R.\n\n\nRStudio is an Integrated Development Environment. This software makes it easy to “talk” to R by keeping track of your files, any data stored in your computers memory and so on. RStudio contains an editor that makes it easy to edit text files that can be interpreted by R.\nWe will get to know RStudio by checking out:\n\nThe console\nEnvironment and history\nFiles, Plots, Packages, Help and Viewer\nGlobal options\n\nImportantly we will change the option concerning “Save workspace to .RData on exit” to NEVER. This will make your work dependent on what you write in your “programs” which is a good thing for reproducibility.\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nTell a friend what the words package, CRAN, and open-source means\nExplain to a friend what you do with the R console\nExplain what the environment and history contains\nWhere do you install packages?\nWhy would you change “Save workspace to .RData on exit” to never under Global options?"
  },
  {
    "objectID": "ws2-installing-r.html#scripts",
    "href": "ws2-installing-r.html#scripts",
    "title": "Starting up R",
    "section": "",
    "text": "A good thing about R is that you will have to script your work. This essentially creates a “program” that R will interpret. The interpretation of your program will result in some output (e.g., a graph, table, a single number or a report).\nThis separates R from point-and-click or spreadsheet software. This also makes it easier to reproduce an analysis.\nR has many flavors of scripts as some combines text with statistical programming that in the end creates a nice looking report. We will talk more about these variants later."
  },
  {
    "objectID": "ws2-installing-r.html#r-basics",
    "href": "ws2-installing-r.html#r-basics",
    "title": "Starting up R",
    "section": "",
    "text": "An object can be a number, a collection of numbers or many other representations of data that is stored in the workspace.\nStart a new script from the File menu (File &gt; New File &gt; R Script), copy the code from the code block below to your script.\nExecute the code by placing the cursor on the row containing the code and pres Ctrl+Enter (Cmd+Enter on Mac).\n\na &lt;- 12\n\nThe &lt;-operator assigns the number 12 to an object called a. This object is stored in the workspace. We can point the arrow in the other direction also.\n\n12 -&gt; b\n\nAssignment using &lt;- or -&gt; can go either way. Alternatively you could use = which will have the same function as &lt;-. Try to assign 12 to c by using the following code 12 = c. You should encounter an error message.\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\nExplain the following sentence in your own words: “The &lt;-operator assigns the number 12 to an object called a. This object is stored in the workspace.”\n\n\n\n\n\n\nObjects can be, depending on their type, combined. Numeric objects can be used in mathematical operations. What is the result of the following code?\n\na + b\n\nMore complex mathematical treatment is also possible using objects.\n\nx &lt;- 12.3 + 3 * a + -1.2 * sqrt(b)\n\nThe sqrt(b) part in the formula above is the first occurance of a R function. It takes a number (or a vector of numbers) and return the square root.\nFor every function, there is a help page. The help pages for sqrt() can be accessed by typing ?sqrt() in the console."
  },
  {
    "objectID": "ws2-installing-r.html#vectors",
    "href": "ws2-installing-r.html#vectors",
    "title": "Starting up R",
    "section": "",
    "text": "Vectors are a combination of data that can be manipulated. The R function c() combines data into a vector.\n\nv &lt;- c(2, 5, 7, 3, 1)\n\nThe vector v has five numbers (2, 5, 7, 3, 1), we can combine these with another value, let’s say the value stored in a\n\nv + a\n\nHowever, if we combine it with a and b, we run into problems.\n\nv + c(a, b)\n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\nExplain the following in your own words why v + c(a, b) resulted in a warning message.\n\n\n\nLet’s add another value to v and try to combine the values stored in v with a and b.\n\nv &lt;- c(v, 1)\n\nv + c(a, b)\n\nWe can examine objects by using R functions such as:\n\nlength(v)\n\nstr(v)\n\nlength(a)\n\nclass(a)"
  },
  {
    "objectID": "ws2-installing-r.html#data-frames",
    "href": "ws2-installing-r.html#data-frames",
    "title": "Starting up R",
    "section": "",
    "text": "Data can be further combined into data frames. These are tabular representations of data with where a row indicate an observation and a column represents a variable. We can create a new data frame using the following code:\n\ndf &lt;- data.frame(v = v, \n                 a = a, \n                 color = c(\"red\", \"red\", \"red\", \"green\", \"green\", \"green\"))\n\nNote that I use the = operator inside the data.frame function. This is a convenient way of separating assignments to the environment/objects from isnside e.g. data abojects or functions.\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\nExplain how the object a multiplies in the data frame function. Explore, what is the class of df?\n\n\n\nData in a data frame can be accessed in multiple ways. First, to extract a specific variable, we can use the $ operator.\n\ndf$color\n\nThe code above prints the content of the variable color as a vector of characters. A character vector is a collection of text that has no further meaning to R (but might have to us!).\nWe can also extract a specific value using brackets. To extract the second row, from the third column, we would write:\n\ndf[2, 3]\n\n(We can remember this by saying out loud ROW-COLUMN).\nNew variables can be added to the data frame by assignment\n\ndf$new_variable &lt;- df$v * 3.14 \n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\nExplain in your own words what happend in the code chunk above.\n\n\n\nWe can also add variables to the data frame that are the results of “tests” of the data.\n\ndf$another_variable &lt;- df$new_variable &gt; 10 \n\nThe operator &gt; tests if observations in the variable new_variable are larger than 10. This results in a logical vector containing TRUE or FALSE. These are special vectors called logical vectors that can be either TRUE or FALSE.\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\nWe have covered numeric, logical and character vectors. Explain the differences to your friend.\n\n\n\n\n\nThe character vector color contained in the data frame df is a character vector. However, we can transform it to a factor. A factor can be ordered by your specifications. We can also add labels. Let’s reorganize the color variable.\n\ndf$color &lt;- factor(df$color) \n\ndf$color\n\nRun the code above and you will see that by converting the character vector to a factor we ge a new data type that has levels in alphabetical order. We can change the levels to what ever we want using the factor function and the argument levels.\n\ndf$color &lt;- factor(df$color, levels = c(\"red\", \"green\")) \n\ndf$color\n\nThis was the first mention of arguments. Every function usually takes user specified arguments that determines the output from the function. Possible arguments can be reviewed by accessing e.g. ?factor."
  },
  {
    "objectID": "ws2-installing-r.html#naming-variables",
    "href": "ws2-installing-r.html#naming-variables",
    "title": "Starting up R",
    "section": "",
    "text": "It is good practice to name variables or objects with words that are meaningful to what you are doing. A variable/object name cannot start with a number, it cannot contain spaces or special characters.\nA good programmer is lazy. Avoid mixing large and small letters, reduce the number of symbols in variable names without loosing meaning and be consistent."
  },
  {
    "objectID": "ws2-installing-r.html#a-first-plot",
    "href": "ws2-installing-r.html#a-first-plot",
    "title": "Starting up R",
    "section": "",
    "text": "Using the built in system for plotting, we can create a first figure of our data. This plot has a lot of potential for improvements. We will explore this potential in the next workshop!\n\nplot(df$v, df$new_variable)"
  },
  {
    "objectID": "ws2-installing-r.html#footnotes",
    "href": "ws2-installing-r.html#footnotes",
    "title": "Starting up R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA more elaborate description can be found at the CRAN FAQ↩︎"
  },
  {
    "objectID": "ws3-first-graph.html",
    "href": "ws3-first-graph.html",
    "title": "Your first “real” R plot",
    "section": "",
    "text": "In this workshop, many of us will create our first real plot. For this purpose we will use the ggplot2 package. This choice of package is based on usage, many people use it and therefore you can easily find help online. ggplot2 is also integrated or highly compatible with other commonly used packages in R.\nIt is a good idea to write your code in a R script in this session. Be sure to comment your code extensively, this will help you explain to yourself what you are doing and make it easy to reuse parts of your code.\nA commented line in R code starts with #:\n\n# This is a comments\na &lt;- c(\"roses\", \"are\", \"red\")\n# This is another comment\n\n#### Sections can be specified with several number/hash/pound signs ####\n\n# Sections in scripts and code chunks help you structure your work. \n# In R studio, sections can be located from the editor.\n\nIt is also a good idea to use the comments to write a statement about the purpose of the script or analysis your are writing. Later we will talk about keeping files in a structured way in projects.\n\n\nWe need to start by installing required packages. From the console we can type\n\ninstall.packages(\"tidyverse\")\n\nThis will install the tidyverse package, a package containing many package. On the tidyverse website you can read:\n\nThe tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures.\n\nBy using the tidyverse you will adopt a special dialect of R. A dialect that is very efficient and fairly easy to read (as a human).\nTidyverse will install ggplot2 for you. Notice that you only need to install a package once. It is therefore not a good idea to have an unconditional install.packages() in your script.\nTo get ggplot2 to start working we need to use another command:\n\nlibrary(\"ggplot2\")\n\nNotice that I’ve put ggplot2 inside citation marks \". This is optional!\nThe library function loads all function contained in the package to your R session. This means that you can access and use them.\nFor these exercises we will use another package. The exscidata package contains data sets related to exercise physiology. To install it we need a bit more code. Since exscidata is not on CRAN, but on github we can use the remotes package.\n\n# Install package from cran\nif(!(\"remotes\" %in% .packages(all.available = TRUE))) {\n        install.packages(\"remotes\")\n}\n\n\n# Using remotes, install exscidata from github\nif(!(\"exscidata\" %in% .packages(all.available = TRUE))) {\n        remotes::install_github(\"dhammarstrom/exscidata\")\n}\n\n\n\n# Load the exscidata package\nlibrary(exscidata)\n\n\n\n\n\n\n\nNote\n\n\n\nAbove is a if statement. Ordinarily, the statement can be read as: “If the condition is TRUE, then do whatever is in the brackets”. However, we also use a ! around a parentheses containing the %in% operator. The ! negates the test. If the package name is not contained in the vector of all packages created by the .packages(all.available = TRUE), then we want to install the package.\nThis is a way not having to install packages that are already installed when running your script.\n\n\nWe are now set to load data into our session.\n\n\n\nThe data set we will use in these exercises is called cyclingstudy. You can have a look at the variables by using the help command ?cyclingstudy.\nTo load data from a package we can use data(\"cyclingstudy\")\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nExplain the difference between install.packages() and library()\nWhat is the tidyverse, what will we use ggplot2 for?\nWhat does it mean when a package is not on CRAN?\nIdentify at least one numeric variable in the help pages for cyclingstudy, identify at least one categorical variable.\nWhat happens in your environment when you type data(\"cyclingstudy\")?\n\n\n\n\nA data set can be accessed in multiple ways. We may want to see the data. We can do this by typing View(cyclingstudy) in the console (notice the capital V). Or we can show a couple of rows and columns in the console by typing cyclingstudy.\n\n\n\nThe ggplot function (from the ggplot2 package) takes quite a lot of arguments. However, very few are needed to create a graph.\nThe ggplot2 system uses:\n\ndata → The dataset containing variables to plot\naesthetics → Scales where the data are mapped\ngeometries → Geometric representations of the data\nfacet → A part of the dataset\nstatistical transformations → Summaries of data\ncoordinates → The coordinate space\nthemes → Plot components not linked to data\n\nWe build a graph by mapping variables to different locations and visual characteristics of what is called geoms. This system makes it easy to build different types of graphs using similar syntax.\nWe will start by mapping to continuous variables to the coordinate system. For this exercise, use the variables weight.T1 and sj.max.\nggplot needs to know were the variables can be found, we therefore have to specify the data argument first. Next we map the variables to the x and y coordinates of the graph. You can copy the code below to your R script.\n\nggplot(data = cyclingstudy, aes(x = cmj.max, y = sj.max))\n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nExplain to your friend what we have done so far.\nWhat is mapping in this context?\nDefine “continuous variables”\n\n\n\n\n\n\n\nWe have not yet added graphical representations of the data mapped to coordinates (x and y). These can be added to the plot using the + operator, as we will do below.\nThink about a ggplot as a layered construction. Layers can be added (+) to build the graph you want. Layers are added to the graph sequentially, this means it matters in which order you add them.\nBut what to add?\nThere are many geoms, for an overview go to Help &gt; Cheat Sheets &gt; Data Visualization with ggplot2\n\nTask 1: Identify a geom suitable for two continuous variables, that will show individual data points on x- and y- coordinates.\nTask 2: Call up the help page of the selected geom and find out what you need to add as arguments to the geom\n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nExplain what the argument inherit.aes = TRUE means.\nExplain the sentence (from the help page): “If NULL, the default, the data is inherited from the plot data as specified in the call to ggplot().\n\n\n\n\nBy adding, for example, points to the plot, we will be able to see the data. How would you add the geom that creates points to your plot?\n\n\nShow the code for a possible solution\nggplot(data = cyclingstudy, aes(x = cmj.max, y = sj.max)) + geom_point()\n\n\n\n\n\nTask 1: Using the cheat sheet, beside x and y. What other aesthetics (aes) may be added to the plot that will affect the appearance of the points?\nTask 2: Using pen and paper, draw a figure of the cyclingstudy data using one categorical variable and one continuous variable and hand the figure to the next group.\nTask 3: Code the figure! Create the figure that the other group has drafted for you.\n\n\n\n\n\nCharacteristics such as shapes or colors can also be added to geoms outside the aes(). This means we will override any mapping already given in aes(). As we already have seen, mappings that are inherited from the ggplot function to geoms.\n\n\n\n\nTask 1: Change characteristics of your plot outside data mapping using color and fill, linetype, size and shape. What geoms are responsive to each change?\n\n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nExplain the what the code will produce, without running the code below (google “r shapes” to see what number each shape has.)\n\n\n# Example 1\nggplot(data = cyclingstudy, aes(x = cmj.max, y = sj.max, color = group)) + geom_point(color = \"blue\")\n\n# Example 2\nggplot(data = cyclingstudy, aes(x = cmj.max, y = sj.max, color = group)) + geom_point(shape = 8)\n\n# Example 3\nggplot(data = cyclingstudy, aes(x = cmj.max, y = sj.max, color = weight.T1)) + geom_point(shape = 8)\n\n# Example 4\nggplot(data = cyclingstudy, aes(x = cmj.max, y = sj.max, shape = timepoint)) + geom_point(color = \"red\")\n\n\n\n\n\n\n\n\nIn ggplot2 there are pre-set palettes for colors, orders of shapes and line types etc. Often you would want to control such settings.\nThere are many sources for informed selection of colors, one is colorbrewer2. We\nTo change color scales we use scale_*_* functions that will help you set, e.g., colors manually. In the example below we create a gradient from two colors\n\nggplot(data = cyclingstudy, \n       aes(x = cmj.max, y = sj.max, color = weight.T1)) + \n        geom_point() +\n        scale_colour_gradient(\n                low = \"#e41a1c\",  \n                high = \"#4daf4a\")\n\nDiscrete variables can also be set with colors using scale_color_manual\n\nggplot(data = cyclingstudy, \n       aes(x = cmj.max, y = sj.max, color = timepoint)) + \n        geom_point() +\n        scale_color_manual(values = c(\"#66c2a5\",\"#fc8d62\",\"#8da0cb\",\"#e78ac3\"))\n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nExplain to a friend what scales do\n\n\n\n\n\n\n\nSome aspects of a plot requires that data points are connected together. This essentially means that some a variable needs to group data points. In our example data set, subject gives the identity of each participant. We may use this information to group data points, or connect them with e.g. geom_line(). By adding group = subject to the aes() call in ggplot we will group all geoms that allow grouping.\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nBefore running the code below. Explain what you expect it will show.\n\n\n\n\n\n# Example 1\nggplot(data = cyclingstudy, aes(x = cmj.max, \n                                y = sj.max, \n                                color = group,\n                                shape = timepoint,\n                                group = subject)) + \n        geom_point() +\n        geom_line()\n\n\n\n\nA plot can be quite cluttered, and in this case, give a false impression of a lot of data. The design of this study results in a data set where each participant is tested at multiple time-points. We can therefore create facets based on some aspect of the data, such as time-point.\nThe facet_wrap() and facet_grid() creates facets.\n\n# Example 1\nggplot(data = cyclingstudy, aes(x = cmj.max, \n                                y = sj.max, \n                                color = group)) + \n        geom_point() +\n        facet_wrap(~ timepoint)\n\nAs can be seen in the code above, facet_wrap takes a one-handed formula, the ~ (tilde), indicates a formula. We can read this as “wrap by time-point”.\nIn facet_grid() we will use a two-handed formula, meaning that both sides of the tilde needs information. If we want to group only by rows, we will use a . to indicate that nothing will be used to group by column.\n\nggplot(data = cyclingstudy, aes(x = cmj.max, \n                                y = sj.max, \n                                color = group)) + \n        geom_point() +\n        facet_grid( timepoint ~ .)\n\nIn facet_grid above, we could replace the . with another variable. How would you write the code to facet the graph by group in rows and time-point in columns?\n\n\nShow the code\nggplot(data = cyclingstudy, aes(x = cmj.max, \n                                y = sj.max, \n                                color = group)) + \n        geom_point() +\n        facet_grid( group ~ timepoint)\n\n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nExplain to a friend, what do the plot produced by the code above show? Include everything that is important to reproduce the plot in your description. Try to describe it without pointing at the plot!\n\n\n\n\n\n\n\nAnnotations can be added to plots, these are often user specified, such as labels and plot titles.\nWe specify labels with the labs() function and add annotations with the annotate() function.\n\nggplot(data = cyclingstudy, aes(x = cmj.max, \n                                y = sj.max, \n                                color = group)) + \n        geom_point() +\n        labs(x = \"Maximal Counter movement jump height (cm)\", \n             y = \"Maximal squat jump height (cm)\", \n             title = \"This is the title\", \n             subtitle = \"This is the subtitle\", \n             caption = \"This is a caption\", \n             color = \"This is the group aesthetics\") +\n        \n        annotate(geom = \"text\", x = 25, y = 35, label = \"This is a text annotation\")\n\nNotice that labs()makes use of all aesthetic mappings and annotate requires a geom.\n\n\n\nThe theme() function is used for non-data layers in the plot.\n\n\n\nThere are two commonly used system for combining individual plots, patchwork, https://patchwork.data-imaginist.com/ and cowplot, https://cran.r-project.org/web/packages/cowplot/vignettes/introduction.html.\nThe idea here is create figures with multiple individual figures.\npatchwork has a very simple syntax.\n\nlibrary(patchwork)\n\n\na &lt;- ggplot(data = cyclingstudy, aes(x = cmj.max, \n                                y = sj.max, \n                                color = group)) + \n        geom_point() \n\nb &lt;- ggplot(data = cyclingstudy, aes(y = sj.max, \n                                x = group)) + \n        geom_boxplot() \n\n\nc &lt;- ggplot(data = cyclingstudy, aes(x = timepoint, \n                                y = sj.max, \n                                group = subject)) + \n        geom_line() \n\n\n\n(a | b) / c\n\ncowplot uses plot_grid to arrange plots\n\nlibrary(cowplot)\n\nplot_grid(a, b, c, nrow = 2)\n\nplot_grid can also use a “nested” structure.\n\nplot_grid(plot_grid(a, b, nrow = 1), \n          c, nrow = 2)\n\nIn both frameworks, annotations can be added to plots to indicate panels/sub-plots.\n\n\n\nOutput from ggplot2 can be saved from RStudio using the export buttom. However, a more reproducible manner is to save the output using ggsave\n\n\n\n\nTask 1: Create three separate plots from the cycling data set and save them as objects in your environment.\nTask 2: Use cowplot and patchwork to group the plots together.\nTask 3: Save the plot using ggsave, explore the help pages to find what arguments are needed!\n\n\n\n\n\n\n\nReproduce figure 1.3 from Spiegelhalter (2019)\n\n\n# download data\nchild_heart &lt;- read_csv(\"https://raw.githubusercontent.com/dspiegel29/ArtofStatistics/master/01-1-2-3-child-heart-survival-times/01-1-child-heart-survival-x.csv\")\n\n\nReproduce figure 2.2 and 2.3\n\n\nbeans &lt;- read.csv(\"https://raw.githubusercontent.com/dspiegel29/ArtofStatistics/master/02-2-3-jelly-bean-counts/02-1-bean-data-full-x.csv\", header = FALSE)"
  },
  {
    "objectID": "ws3-first-graph.html#installing-packages",
    "href": "ws3-first-graph.html#installing-packages",
    "title": "Your first “real” R plot",
    "section": "",
    "text": "We need to start by installing required packages. From the console we can type\n\ninstall.packages(\"tidyverse\")\n\nThis will install the tidyverse package, a package containing many package. On the tidyverse website you can read:\n\nThe tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures.\n\nBy using the tidyverse you will adopt a special dialect of R. A dialect that is very efficient and fairly easy to read (as a human).\nTidyverse will install ggplot2 for you. Notice that you only need to install a package once. It is therefore not a good idea to have an unconditional install.packages() in your script.\nTo get ggplot2 to start working we need to use another command:\n\nlibrary(\"ggplot2\")\n\nNotice that I’ve put ggplot2 inside citation marks \". This is optional!\nThe library function loads all function contained in the package to your R session. This means that you can access and use them.\nFor these exercises we will use another package. The exscidata package contains data sets related to exercise physiology. To install it we need a bit more code. Since exscidata is not on CRAN, but on github we can use the remotes package.\n\n# Install package from cran\nif(!(\"remotes\" %in% .packages(all.available = TRUE))) {\n        install.packages(\"remotes\")\n}\n\n\n# Using remotes, install exscidata from github\nif(!(\"exscidata\" %in% .packages(all.available = TRUE))) {\n        remotes::install_github(\"dhammarstrom/exscidata\")\n}\n\n\n\n# Load the exscidata package\nlibrary(exscidata)\n\n\n\n\n\n\n\nNote\n\n\n\nAbove is a if statement. Ordinarily, the statement can be read as: “If the condition is TRUE, then do whatever is in the brackets”. However, we also use a ! around a parentheses containing the %in% operator. The ! negates the test. If the package name is not contained in the vector of all packages created by the .packages(all.available = TRUE), then we want to install the package.\nThis is a way not having to install packages that are already installed when running your script.\n\n\nWe are now set to load data into our session."
  },
  {
    "objectID": "ws3-first-graph.html#loading-data",
    "href": "ws3-first-graph.html#loading-data",
    "title": "Your first “real” R plot",
    "section": "",
    "text": "The data set we will use in these exercises is called cyclingstudy. You can have a look at the variables by using the help command ?cyclingstudy.\nTo load data from a package we can use data(\"cyclingstudy\")\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nExplain the difference between install.packages() and library()\nWhat is the tidyverse, what will we use ggplot2 for?\nWhat does it mean when a package is not on CRAN?\nIdentify at least one numeric variable in the help pages for cyclingstudy, identify at least one categorical variable.\nWhat happens in your environment when you type data(\"cyclingstudy\")?\n\n\n\n\nA data set can be accessed in multiple ways. We may want to see the data. We can do this by typing View(cyclingstudy) in the console (notice the capital V). Or we can show a couple of rows and columns in the console by typing cyclingstudy."
  },
  {
    "objectID": "ws3-first-graph.html#mapping-data-into-aes",
    "href": "ws3-first-graph.html#mapping-data-into-aes",
    "title": "Your first “real” R plot",
    "section": "",
    "text": "The ggplot function (from the ggplot2 package) takes quite a lot of arguments. However, very few are needed to create a graph.\nThe ggplot2 system uses:\n\ndata → The dataset containing variables to plot\naesthetics → Scales where the data are mapped\ngeometries → Geometric representations of the data\nfacet → A part of the dataset\nstatistical transformations → Summaries of data\ncoordinates → The coordinate space\nthemes → Plot components not linked to data\n\nWe build a graph by mapping variables to different locations and visual characteristics of what is called geoms. This system makes it easy to build different types of graphs using similar syntax.\nWe will start by mapping to continuous variables to the coordinate system. For this exercise, use the variables weight.T1 and sj.max.\nggplot needs to know were the variables can be found, we therefore have to specify the data argument first. Next we map the variables to the x and y coordinates of the graph. You can copy the code below to your R script.\n\nggplot(data = cyclingstudy, aes(x = cmj.max, y = sj.max))\n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nExplain to your friend what we have done so far.\nWhat is mapping in this context?\nDefine “continuous variables”"
  },
  {
    "objectID": "ws3-first-graph.html#adding-geometric-representations",
    "href": "ws3-first-graph.html#adding-geometric-representations",
    "title": "Your first “real” R plot",
    "section": "",
    "text": "We have not yet added graphical representations of the data mapped to coordinates (x and y). These can be added to the plot using the + operator, as we will do below.\nThink about a ggplot as a layered construction. Layers can be added (+) to build the graph you want. Layers are added to the graph sequentially, this means it matters in which order you add them.\nBut what to add?\nThere are many geoms, for an overview go to Help &gt; Cheat Sheets &gt; Data Visualization with ggplot2\n\nTask 1: Identify a geom suitable for two continuous variables, that will show individual data points on x- and y- coordinates.\nTask 2: Call up the help page of the selected geom and find out what you need to add as arguments to the geom\n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nExplain what the argument inherit.aes = TRUE means.\nExplain the sentence (from the help page): “If NULL, the default, the data is inherited from the plot data as specified in the call to ggplot().\n\n\n\n\nBy adding, for example, points to the plot, we will be able to see the data. How would you add the geom that creates points to your plot?\n\n\nShow the code for a possible solution\nggplot(data = cyclingstudy, aes(x = cmj.max, y = sj.max)) + geom_point()\n\n\n\n\n\nTask 1: Using the cheat sheet, beside x and y. What other aesthetics (aes) may be added to the plot that will affect the appearance of the points?\nTask 2: Using pen and paper, draw a figure of the cyclingstudy data using one categorical variable and one continuous variable and hand the figure to the next group.\nTask 3: Code the figure! Create the figure that the other group has drafted for you."
  },
  {
    "objectID": "ws3-first-graph.html#changing-colors-and-shapes-outside-mapping",
    "href": "ws3-first-graph.html#changing-colors-and-shapes-outside-mapping",
    "title": "Your first “real” R plot",
    "section": "",
    "text": "Characteristics such as shapes or colors can also be added to geoms outside the aes(). This means we will override any mapping already given in aes(). As we already have seen, mappings that are inherited from the ggplot function to geoms.\n\n\n\n\nTask 1: Change characteristics of your plot outside data mapping using color and fill, linetype, size and shape. What geoms are responsive to each change?\n\n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nExplain the what the code will produce, without running the code below (google “r shapes” to see what number each shape has.)\n\n\n# Example 1\nggplot(data = cyclingstudy, aes(x = cmj.max, y = sj.max, color = group)) + geom_point(color = \"blue\")\n\n# Example 2\nggplot(data = cyclingstudy, aes(x = cmj.max, y = sj.max, color = group)) + geom_point(shape = 8)\n\n# Example 3\nggplot(data = cyclingstudy, aes(x = cmj.max, y = sj.max, color = weight.T1)) + geom_point(shape = 8)\n\n# Example 4\nggplot(data = cyclingstudy, aes(x = cmj.max, y = sj.max, shape = timepoint)) + geom_point(color = \"red\")"
  },
  {
    "objectID": "ws3-first-graph.html#scales",
    "href": "ws3-first-graph.html#scales",
    "title": "Your first “real” R plot",
    "section": "",
    "text": "In ggplot2 there are pre-set palettes for colors, orders of shapes and line types etc. Often you would want to control such settings.\nThere are many sources for informed selection of colors, one is colorbrewer2. We\nTo change color scales we use scale_*_* functions that will help you set, e.g., colors manually. In the example below we create a gradient from two colors\n\nggplot(data = cyclingstudy, \n       aes(x = cmj.max, y = sj.max, color = weight.T1)) + \n        geom_point() +\n        scale_colour_gradient(\n                low = \"#e41a1c\",  \n                high = \"#4daf4a\")\n\nDiscrete variables can also be set with colors using scale_color_manual\n\nggplot(data = cyclingstudy, \n       aes(x = cmj.max, y = sj.max, color = timepoint)) + \n        geom_point() +\n        scale_color_manual(values = c(\"#66c2a5\",\"#fc8d62\",\"#8da0cb\",\"#e78ac3\"))\n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nExplain to a friend what scales do"
  },
  {
    "objectID": "ws3-first-graph.html#grouping-data",
    "href": "ws3-first-graph.html#grouping-data",
    "title": "Your first “real” R plot",
    "section": "",
    "text": "Some aspects of a plot requires that data points are connected together. This essentially means that some a variable needs to group data points. In our example data set, subject gives the identity of each participant. We may use this information to group data points, or connect them with e.g. geom_line(). By adding group = subject to the aes() call in ggplot we will group all geoms that allow grouping.\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nBefore running the code below. Explain what you expect it will show.\n\n\n\n\n\n# Example 1\nggplot(data = cyclingstudy, aes(x = cmj.max, \n                                y = sj.max, \n                                color = group,\n                                shape = timepoint,\n                                group = subject)) + \n        geom_point() +\n        geom_line()"
  },
  {
    "objectID": "ws3-first-graph.html#faceting-plots",
    "href": "ws3-first-graph.html#faceting-plots",
    "title": "Your first “real” R plot",
    "section": "",
    "text": "A plot can be quite cluttered, and in this case, give a false impression of a lot of data. The design of this study results in a data set where each participant is tested at multiple time-points. We can therefore create facets based on some aspect of the data, such as time-point.\nThe facet_wrap() and facet_grid() creates facets.\n\n# Example 1\nggplot(data = cyclingstudy, aes(x = cmj.max, \n                                y = sj.max, \n                                color = group)) + \n        geom_point() +\n        facet_wrap(~ timepoint)\n\nAs can be seen in the code above, facet_wrap takes a one-handed formula, the ~ (tilde), indicates a formula. We can read this as “wrap by time-point”.\nIn facet_grid() we will use a two-handed formula, meaning that both sides of the tilde needs information. If we want to group only by rows, we will use a . to indicate that nothing will be used to group by column.\n\nggplot(data = cyclingstudy, aes(x = cmj.max, \n                                y = sj.max, \n                                color = group)) + \n        geom_point() +\n        facet_grid( timepoint ~ .)\n\nIn facet_grid above, we could replace the . with another variable. How would you write the code to facet the graph by group in rows and time-point in columns?\n\n\nShow the code\nggplot(data = cyclingstudy, aes(x = cmj.max, \n                                y = sj.max, \n                                color = group)) + \n        geom_point() +\n        facet_grid( group ~ timepoint)\n\n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nExplain to a friend, what do the plot produced by the code above show? Include everything that is important to reproduce the plot in your description. Try to describe it without pointing at the plot!"
  },
  {
    "objectID": "ws3-first-graph.html#plot-annotations",
    "href": "ws3-first-graph.html#plot-annotations",
    "title": "Your first “real” R plot",
    "section": "",
    "text": "Annotations can be added to plots, these are often user specified, such as labels and plot titles.\nWe specify labels with the labs() function and add annotations with the annotate() function.\n\nggplot(data = cyclingstudy, aes(x = cmj.max, \n                                y = sj.max, \n                                color = group)) + \n        geom_point() +\n        labs(x = \"Maximal Counter movement jump height (cm)\", \n             y = \"Maximal squat jump height (cm)\", \n             title = \"This is the title\", \n             subtitle = \"This is the subtitle\", \n             caption = \"This is a caption\", \n             color = \"This is the group aesthetics\") +\n        \n        annotate(geom = \"text\", x = 25, y = 35, label = \"This is a text annotation\")\n\nNotice that labs()makes use of all aesthetic mappings and annotate requires a geom."
  },
  {
    "objectID": "ws3-first-graph.html#themes",
    "href": "ws3-first-graph.html#themes",
    "title": "Your first “real” R plot",
    "section": "",
    "text": "The theme() function is used for non-data layers in the plot."
  },
  {
    "objectID": "ws3-first-graph.html#combine-separate-plots",
    "href": "ws3-first-graph.html#combine-separate-plots",
    "title": "Your first “real” R plot",
    "section": "",
    "text": "There are two commonly used system for combining individual plots, patchwork, https://patchwork.data-imaginist.com/ and cowplot, https://cran.r-project.org/web/packages/cowplot/vignettes/introduction.html.\nThe idea here is create figures with multiple individual figures.\npatchwork has a very simple syntax.\n\nlibrary(patchwork)\n\n\na &lt;- ggplot(data = cyclingstudy, aes(x = cmj.max, \n                                y = sj.max, \n                                color = group)) + \n        geom_point() \n\nb &lt;- ggplot(data = cyclingstudy, aes(y = sj.max, \n                                x = group)) + \n        geom_boxplot() \n\n\nc &lt;- ggplot(data = cyclingstudy, aes(x = timepoint, \n                                y = sj.max, \n                                group = subject)) + \n        geom_line() \n\n\n\n(a | b) / c\n\ncowplot uses plot_grid to arrange plots\n\nlibrary(cowplot)\n\nplot_grid(a, b, c, nrow = 2)\n\nplot_grid can also use a “nested” structure.\n\nplot_grid(plot_grid(a, b, nrow = 1), \n          c, nrow = 2)\n\nIn both frameworks, annotations can be added to plots to indicate panels/sub-plots."
  },
  {
    "objectID": "ws3-first-graph.html#saving-output",
    "href": "ws3-first-graph.html#saving-output",
    "title": "Your first “real” R plot",
    "section": "",
    "text": "Output from ggplot2 can be saved from RStudio using the export buttom. However, a more reproducible manner is to save the output using ggsave\n\n\n\n\nTask 1: Create three separate plots from the cycling data set and save them as objects in your environment.\nTask 2: Use cowplot and patchwork to group the plots together.\nTask 3: Save the plot using ggsave, explore the help pages to find what arguments are needed!"
  },
  {
    "objectID": "ws3-first-graph.html#exercises",
    "href": "ws3-first-graph.html#exercises",
    "title": "Your first “real” R plot",
    "section": "",
    "text": "Reproduce figure 1.3 from Spiegelhalter (2019)\n\n\n# download data\nchild_heart &lt;- read_csv(\"https://raw.githubusercontent.com/dspiegel29/ArtofStatistics/master/01-1-2-3-child-heart-survival-times/01-1-child-heart-survival-x.csv\")\n\n\nReproduce figure 2.2 and 2.3\n\n\nbeans &lt;- read.csv(\"https://raw.githubusercontent.com/dspiegel29/ArtofStatistics/master/02-2-3-jelly-bean-counts/02-1-bean-data-full-x.csv\", header = FALSE)"
  }
]