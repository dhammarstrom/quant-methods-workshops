[
  {
    "objectID": "ws9-linear-model-categorical.html",
    "href": "ws9-linear-model-categorical.html",
    "title": "The linear model: Categorical predictors and diagnostics",
    "section": "",
    "text": "So far we used a continuous predictor variable (\\(x\\)) in our regression models.\nWe could replace this with a variable that either takes the value 0 or 1. This could represent a categorical predictor variable, e.g., Group A = 0, Group B = 1.\nThe model can still be described as\n\n\\[y_i = \\beta_0 + \\beta_1x_{i} + \\epsilon_i\\] - where \\(\\beta_0\\) is the intercept, \\(\\hat{y_i}\\) when \\(x=0\\) which will be the average of group A. - The slope, \\(\\beta_1\\) represents the difference between the groups which equals to the change in \\(\\hat{y}\\) for a one-unit change in \\(x\\).\n\n\n\nWe will create a bad example of a categorical predictor variable to predict VO2.max with height.T1 in the cyclingstudy data set. Why bad? Let’s find out!\nFirst, let’s dichotomize the height variable into two groups. In our analysis we want to compare the group of “tall” individuals to “short” individuals from the pre time-point.\nThe threshold for the grouping will be above/below the median height.\nIn the code chunk below, we add a new variable height which takes the value tall if height.T1 is greater then the median, otherwise “short”. For clarity, we will also convert the variable to a factor variable.\n\n\n\n\n\n\n\nPractice\n\n\n\n\n\nBased on the description above, and before you look at the example below, write the required commands and save a data set in an object called dat.\n\n\n\n\n\nCode\nlibrary(exscidata); library(tidyverse)\n\ndata(\"cyclingstudy\")\n\n# Reduce the data set \ndat &lt;- cyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        select(subject, height.T1, VO2.max) %&gt;%\n        mutate(height = if_else(height.T1 &gt; median(height.T1), \"tall\", \"short\"), \n               height = factor(height)) %&gt;%\n        print()\n\n\nThis operation results in our new grouping variable as shown in the figure below.\n\n\n\n\n\nDichotomizing height into short/tall groups\n\n\n\n\n\nWe may now formulate a model to assess differences in VO2max between height groups.\nIn the model formulation we will write our factor variable. R will “dummy code” this variable to take the values 0 and 1 in the model.\nDummy coding will translate an ordered factor with two levels to a 0/1 variable where 0 can be called the reference level. Using contrasts(dat$height) we can see the how R has coded the variable. Notice that the variable is called tall.\nOur model formulation will be:\n\n\nVO2.max ~ height\n\n\nThis translates to a more formal description\n\n\\[\\text{VO}_\\text{2max} = \\beta_0 + \\beta_{\\text{tall}}x_i\\]\n\n\n\n\n\n\nGroup work\n\n\n\nFit the model VO2.max ~ height using the dichotomized factor variable for height and calculate the average VO2max for the tall group.\n\n\n\n\n\nIt turns out that many commonly used statistical tests can be performed in the regression model framework.1. We will revisit this idea further on but start here with the simple t-test.\nThe t-test is often the first statistical test taught in a statistics course. The goal is to compare two means (paired- or un-paired observations) or compare a mean to a reference value (one sample t-test).\nThe t-test provides us with an apparatus to test a statistical hypothesis.\nAn interpretation of a part of the results from the test, the p-value can be:\n\nIf we performed sampling from, and comparison of, two populations 1000 times, how often would we see a result so extreme or more extreme than the observed value, if the null-hypothesis of no difference is true.\n\nIf the p-value is low, say below 0.05, we have estimated that the results are pretty unlikely if the null-hypothesis is true (less than 5% of repeated studies will show this result if the null hypothesis is true).\nThe differences in two means (tall vs. short) in a t-test is the same as the slope in an ordinary regression model:\n\n\nlibrary(gt)\n\nWarning: package 'gt' was built under R version 4.4.1\n\nttest &lt;- t.test(VO2.max ~ height, data = dat, var.equal = TRUE)\n\nreg &lt;- lm(VO2.max ~ height, data = dat)\n\ndata.frame(model = c(\"t-test\", \"Regression\"), \n           t.val = c(ttest$statistic, coef(summary(reg))[2,3]),\n           p.val = c(ttest$p.value, coef(summary(reg))[2,4]), \n           row.names = NULL) %&gt;%\n        gt() %&gt;%\n        fmt_auto()\n\n\n\n\n\n\n\n\nmodel\nt.val\np.val\n\n\n\n\nt-test\n−1.574\n0.133\n\n\nRegression\n 1.574\n0.133\n\n\n\n\n\n\n\n\nNotice that:\n\nWe use var.equal = TRUE to fully reproduce the results of the regression analysis. This option assumes equal variation in the two groups.\nThe minus sign in the t-statistic of the t-test indicate that the comparison is reversed in the t-test.\n\n\n\n\n\n\n\nGroup work\n\n\n\nHow do you interpret the model summary of the model VO2.max ~ height using the dichotomized factor variable for height? What does the p-value and slope estimate tell us?\n\n\n\n\n\n\n\nWhen we dichotomize a variable we add assumptions to our analysis.\nIn this case, we add an assumption of a non-important relationship between the dependent and independent variable within each group.\nThis assumption can easily be questioned by creating a plot with height.T1 on the x-axis, VO2.max on the y-axis and different color per grouping.\nWe could additionally add regression lines using geom_smooth(method = \"lm\"). Remove the color argument (aes(color = NULL)) from an additional geom_smooth() to include all data in a linear model.\n\n\n\nTry to create the graph based on the description above (before peeking at a possible solution below). Can we reasonably neglect the linear relationship between variables within groups?\n\n\n\nA possible solution\ndat %&gt;%\n        ggplot(aes(height.T1, VO2.max, color = height))  + geom_point() + \n        geom_smooth(method = \"lm\") +\n        geom_smooth(method = \"lm\", aes(color = NULL))\n\n\n\nSince we are actually removing data before fitting the model we also create a model that is difficult to interpret.\n\n\n\n\n\n\n\nGroup work\n\n\n\nInterpret the estimates from\n\n\na model with height treated as a dichotomized variable\n\n\na model with height included as a continuous variable.\n\n\nExplain to your friend what the estimate tells you about the real-world relationship between VO2max and height from each model.\n\n\n\n\nA possible solution\n# Dichotomized data\nm1 &lt;- lm(VO2.max ~ height, data = dat)\n\n\n# Contentious data\nm2 &lt;- lm(VO2.max ~ height.T1, data = dat)\n\n\nsummary(m1);summary(m2)\n\n\n\n\n\nA linear regression model comes with a set of assumptions, these concern\n\nThe sampling of data\nThe relationship between variables\nThe resulting errors of the model\n\n\n\n*A single row of a data set, possibly containing multiple variables.\n\nWhen collecting data for a regression analysis we must make sure that each observation* is independent.\nThis means that the ordinary regression model cannot account for multiple observations sampled from the same individual.\nIf this is done, we commit a crime called pseudo-replication!\nThis can also be the case if we sample individuals that are structured together in teams or classrooms etc.\nMore elaborate models are needed to account for such structures in the data.\n\n\n\n\n\n\n\nGroup work\n\n\n\nInterpret the estimates from a model with height treated as a continuous variable but containing all time-points. Compare the results to the previous model containing data only from timepoint == \"pre\"\nDiscuss how this changes the standard error (Std Error) of the model.\nNotice that height was only recorded at pre, to fill height data to all observations:\n\ncyclingstudy %&gt;%\n        select(subject, timepoint, height.T1, VO2.max) %&gt;%\n        ## To get height at all time-points\n        group_by(subject) %&gt;%\n        mutate(height.T1 = mean(height.T1, na.rm = TRUE)) %&gt;%\n        ungroup()\n\n\n\n\n\nA possible solution\ndat.all &lt;- cyclingstudy %&gt;%\n        select(subject, timepoint, height.T1, VO2.max) %&gt;%\n        \n        ## To get height at all time-points\n        group_by(subject) %&gt;%\n        mutate(height.T1 = mean(height.T1, na.rm = TRUE)) %&gt;%\n        ungroup() %&gt;%\n        print()\n\n# Contentious data, all observations\nm1 &lt;- lm(VO2.max ~ height.T1, data = dat.all)\n\n# Contentious data, only pre-data\nm2 &lt;- lm(VO2.max ~ height.T1, data = dat)\n\nsummary(m1);summary(m2)\n\n\n\n\n\n\n\nThe regression model expects straight-line relationships.\nCurve-linear relationships needs more elaborate model formulations.\nIf the underlying data has some curve linear characteristics, a residual plot will aid in detecting it.\n\n\n\nCode\n# create some fake, non-linear data\n\na &lt;- 0.2\nb &lt;- 1\n\nx &lt;- seq(from = 0.2, to = 10, by = 0.2)\n# Adding curve-linear relationship\n# and random error to each data point\ny &lt;- a + b*sin(x) + rnorm(length(x), 0, 0.2) \ny_straight &lt;- a + b*x + rnorm(length(x), 0, 1) \n\n\n# Store in data frame\nfake &lt;- data.frame(x = x, y = y, y_straight = y_straight)\n\n# Plot the raw data, including a model\nraw_data_plot_sig &lt;- fake %&gt;%\n        ggplot(aes(x, y)) + geom_point() + geom_smooth(method = \"lm\", se = FALSE) + \n        labs(title = \"Raw data\",\n             subtitle =  \"Best fit model y ~ x\")\n\nraw_data_plot_straight &lt;- fake %&gt;%\n        ggplot(aes(x, y_straight)) + geom_point() + geom_smooth(method = \"lm\", se = FALSE) + \n        labs(title = \"Raw data\",\n             subtitle =  \"Best fit model y ~ x\")\n\n\n# Model the data with a straight line\nm &lt;- lm(y ~ x, data = fake)\nm_straight &lt;- lm(y_straight ~ x, data = fake)\n\n# Extract residuals \nfake$resid &lt;- resid(m)\nfake$resid_straight &lt;- resid(m_straight)\n\n# Extract predicted/fitted valus\nfake$fitted &lt;- fitted(m)\nfake$fitted_straight &lt;- fitted(m_straight)\n\n# Plot fitted vs. residual values\nresidual_plot_sig &lt;- fake %&gt;%\n        ggplot(aes(fitted, resid)) + geom_point() + \n        labs(title = \"Analysis of residuals\", \n             subtitle =  \"Generated from the model y ~ x\")\n\nresidual_plot_straight &lt;- fake %&gt;%\n        ggplot(aes(fitted_straight, resid_straight)) + geom_point() + \n        labs(title = \"Analysis of residuals\", \n             subtitle =  \"Generated from the model y ~ x\")\n\n# Using patchwork to arrange plots\nlibrary(patchwork)\n\n(raw_data_plot_sig + residual_plot_sig) / (raw_data_plot_straight + residual_plot_straight)\n\n\n\n\n\nModelling a curve linear vs. a linear relationship using a straight line\n\n\n\n\n\n\n\n\n\n\nGroup work\n\n\n\nExplain to a friend:\n\nWhat is the difference between the raw data and the residual vs. fitted plot?\nWhat can go wrong when we assume linear relationships?\nHow can we diagnose the model and know what to expect when fitting it?\n\n\n\n\n\n\n\nAbove we clearly identified patterns in the residual plot that violated the assumption of a straight line (or linear) relationship between variables.\nAnother assumption can also be assessed using the residual plot (residuals vs. fitted values).\nThis assumption is that errors are equally distributed across the fitted values.\nIn the case of a continuous predictor, the spread of the residuals should vary similarly across the whole range of fitted values.\nIn the case of a categorical variable, residuals should have similar spread in all categories.\nLet’s examine the models formulated above with height, both as a categorical predictor as well as a continuous predictor.\n\n\n\n\nFit the models, only containing the pre data\nSave the residuals to your data set using resid(mod), where mod is the model object\nSimilarly, save the fitted values to your data set using fitted(mod).\nCreate a ggplot of the fitted (x axis) and residual values (y axis).\nInterpret the resulting plots.\n\n\n\nA possible solution\n# Continuous variable\nm1 &lt;- lm(VO2.max ~ height.T1, data = dat)\n\n# Categorical variable \nm2 &lt;- lm(VO2.max ~ height, data = dat)\n\n# Retreive fitted and residual values\ndat$fitted_cont &lt;- fitted(m1)\ndat$resid_cont &lt;- resid(m1)\n\ndat$fitted_cat &lt;- fitted(m2)\ndat$resid_cat &lt;- resid(m2)\n\n# Create figures\nresid_plot_cont &lt;- dat %&gt;%\n        ggplot(aes(fitted_cont, resid_cont)) + geom_point() +\n        labs(title = \"Continous predictor\")\n\n\nresid_plot_cat &lt;- dat %&gt;%\n        ggplot(aes(fitted_cat, resid_cat)) + geom_point()+\n        labs(title = \"Categoricals predictor\")\n\n# Using patchwork to combine the figures\nlibrary(patchwork)\n\nresid_plot_cont + resid_plot_cat\n\n\n\n\nIt is not always easy to tell if there is heteroscedasticity, especially with small number of observations.\nThe graphical method is convenient to use, but there are also tests to detect it. One such test is Breusch–Pagan test.\n\nIn the figure below, two scenarios are presented, one homoscedastic and one heteroscedastic scenario.\n\n\n\n\n\n\nHomoscedasticity vs. heteroscedasticity\n\n\n\n\n\nIt may be difficult to detect heteroscedasticity from a single model created with a small sample size.\n\n\n\n\n\n\nSampling data from population data with equal vs. unequal variation across x-values.\n\n\n\n\n\nIn fact, graphical analysis and formal tests of population characteristics are dependent on the sample size.\nA larger sample size will more often detect heteroscedasticity in the population data.\n\n\n\n\n\n\nSampling data with different sizes from population data with equal vs. unequal variation across x-values and comparing p-values from a test of homoscedasticity.\n\n\n\n\n\n\n\n\n\nSampling data with different sizes from population data with equal vs. unequal variation across x-values and plotting the ability to detect violoations of equal variance.\n\n\n\n\n\nIf we detect violations of the assumption of equal variation across the fitted values we can attempt transformations.\nThe log-transformation can be used when the variance increases with larger values (positive correlation between fitted values and variance).\nNote that log transformation of the dependent variable changes the interpretation of the model. Instead of additive, we evaluate the multiplicative effect. For a unit difference in x, y changes by a percentage.\n\n\n\nCode\n# create some fake, non-linear data\nset.seed(10)\na &lt;- 20\nb &lt;- 15\n\nx &lt;- seq(from = 5, to = 20, by = 0.2)\n\n# and random error to each data point\ny &lt;- a + b*x + rnorm(length(x), 0, 2*x) \n\n\n\n# Store in data frame\nfake &lt;- data.frame(x = x, y = y)\n\n# Plot the raw data, including a model\nraw_data_plot &lt;- fake %&gt;%\n        ggplot(aes(x, y)) + geom_point() + geom_smooth(method = \"lm\", se = FALSE) + \n        labs(title = \"Raw data\",\n             subtitle =  \"Best fit model y ~ x\")\n\nraw_data_plot_log &lt;- fake %&gt;%\n        ggplot(aes(x, log(y))) + geom_point() + geom_smooth(method = \"lm\", se = FALSE) + \n        labs(title = \"Raw data\",\n             subtitle =  \"Best fit model log(y) ~ x\")\n\n# Model the data with a straight line\nm &lt;- lm(y ~ x, data = fake)\nlogm &lt;- lm(log(y) ~ x, data = fake)\n\n# Test for heteroscedasticity\nbp_m &lt;- lmtest::bptest(m)\nbp_logm &lt;- lmtest::bptest(logm)\n\n\n# Extract residuals \nfake$resid &lt;- resid(m)\nfake$residlog &lt;- resid(logm)\n\n# Extract predicted/fitted valus\nfake$fitted &lt;- fitted(m)\nfake$fittedlog &lt;- fitted(logm)\n\n# Plot fitted vs. residual values\nresidual_plot &lt;- fake %&gt;%\n        ggplot(aes(fitted, resid)) + geom_point() + \n        labs(title = \"Analysis of residuals\", \n             subtitle =  \"Generated from the model y ~ x\", \n             caption = paste0(\"Breusch–Pagan test p-value: \", round(bp_m$p.value, 5))) \n\nresidual_plot_log &lt;- fake %&gt;%\n        ggplot(aes(fittedlog, residlog)) + geom_point() + \n        labs(title = \"Analysis of residuals\", \n             subtitle =  \"Generated from the model log(y) ~ x\", \n             caption = paste0(\"Breusch–Pagan test p-value: \", round(bp_logm$p.value, 3)))\n\n\n# Using patchwork to arrange plots\nlibrary(patchwork)\n\n(raw_data_plot + residual_plot) / (raw_data_plot_log + residual_plot_log) \n\n\n\n\n\nDealing with heteroscedasticity using log-transformation of the data\n\n\n\n\n\n\n\nI addition the assumption of errors being equally distributed (homoscedasticity), we also assume that they are normally distributed.\nThis is also conveniently evaluated graphically using a histogram of the residuals and a qq-plot.\n\n\n\n\n\n\nAssessing normality of residuals using graphical methods\n\n\n\n\n\nThe qq-plot indicates deviations from the normal distribution as points that fall far away from the line deviates from a theoretical distribution.\nDeviations from the assumption of normal residuals may be due to a number of “outliers”.\nLet’s add 200 to 6 observations from the previous diagnostic plots and re-fit the model.\n\n\n\n\n\n\nAssessing normality of residuals using graphical methods with 6 observations inflated by 200 units\n\n\n\n\n\n\n\n\n\nR has nice built in functions for graphical assessment of regression models\nAfter fitting a model, simply use plot(model)\nThe resulting plots are similar to what we have examined so far\n\n\n# Copy and run the code on your own!\n\ndat &lt;- cyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        select(subject, height.T1, VO2.max) \n\nm &lt;- lm(VO2.max ~ height.T1, data = dat)\n\nplot(m)\n\n\nThe first plot is the ordinary Residuals vs. Fitted plot that we created above. Here we can assess the assumption regarding homoscedasticity.\nThe second plot is a Q-Q plot where we can assess the assumption regarding normality of the residuals\nThe third plot provides a slightly different version of the residual vs. fitted plot. Scale-location has the advantage of showing absolute values, this may make it easier to detect heteroscedasticity.\nThe forth plot contains residuals vs. leverage where leverage indicates influential data points.\nAn influential data point has a lot of influence on the regression model. Let’s see how.\n\n\n\nCode\ndat2 &lt;- dat %&gt;%\n        mutate(added = \"NO\") %&gt;%\n        add_row(subject = 200, height.T1 = 200, VO2.max = 7000, added = \"YES\")\n\n\nm &lt;- lm(VO2.max ~ height.T1, data = dat2) \n\n\n\n\nraw_data &lt;- dat2 %&gt;%\n        ggplot(aes(height.T1, VO2.max, color = added)) + \n        geom_point() +\n        geom_smooth(method = \"lm\", se = FALSE) +\n        geom_smooth(method = \"lm\", aes(color = NULL), se = FALSE) +\n        labs(title = \"An influential data point added\")\n\n\n### These functions are needed to plot the cooks distance\ncd_cont_pos &lt;- function(leverage, level, model) {sqrt(level*length(coef(model))*(1-leverage)/leverage)}\ncd_cont_neg &lt;- function(leverage, level, model) {-cd_cont_pos(leverage, level, model)}\n\n\n\nleverage_plot &lt;- data.frame(leverage = hatvalues(m), \n                            scaled.resid = resid(m)/ sd(resid(m)), \n                            Added = dat2$added) %&gt;%\n        ggplot(aes(leverage, scaled.resid, fill = Added)) + geom_point(shape = 21, size = 2) +\n        \n        stat_function(fun = cd_cont_pos, args = list(level = 0.5, model = m), xlim = c(0, 0.5), lty = 2, colour = \"red\") +\n        stat_function(fun = cd_cont_neg, args = list(level = 0.5, model = m), xlim = c(0, 0.5), lty = 2, colour = \"red\")  +\n        \n        stat_function(fun = cd_cont_pos, args = list(level = 1, model = m), xlim = c(0, 0.5), lty = 1, colour = \"red\") +\n        stat_function(fun = cd_cont_neg, args = list(level = 1, model = m), xlim = c(0, 0.5), lty = 1, colour = \"red\")  +\n        \n        scale_y_continuous(limits = c(-2, 2.5)) +\n        labs(x = \"Leverage\", \n             y = \"Standardized residuals\", \n             title = \"Identifying an influential data point\", \n             subtitle = \"Cook's distance indicated by red lines\")\n\n\nraw_data + leverage_plot\n\n\n\n\n\nExamining the effect of an influential data point\n\n\n\n\n\n\n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\nExplain these concepts:\n\nPredictor variable, dependent variable, independent variable\nContinuous variable, dummy variable\nFitted values, residuals, standardized residuals\nHomoscedasticity, heteroscedasticity, equal variance, normality of residuals\nPseudo-replication\n\n\n\n\n\n\n\n\nThe ordinary linear model is easily extended to include more predictor variables.\nIn fact, when we add a categorical variable with more than 2 levels, we effectively perform multiple regression.\nLet’s say that we want to estimate the differences in change in VO2max (ml min-1) between the three groups in the cycling study data set (see figure).\n\n\n\nCode\nlibrary(ggtext) # needed to use markdown in legends/axis texts\n\ncyclingstudy %&gt;% \n        select(subject, group, timepoint, VO2.max) %&gt;%\n        pivot_wider(names_from = timepoint, \n                    values_from = VO2.max) %&gt;%\n        mutate(change = meso3 - pre) %&gt;%\n        ggplot(aes(group, change)) + geom_boxplot() +\n        labs(y = \"VO&lt;sub&gt;2max&lt;/sub&gt; (ml min&lt;sup&gt;-1&lt;/sup&gt;) change from pre to meso3\", \n             x = \"Group\", \n             title = \"Absolute change in maximal aerobic power\") + \n        \n        theme(axis.title.y = element_markdown())\n\n\n\n\n\nChanges in VO2max from pre to meso3 in three groups in the cycling study data set\n\n\n\n\n\nWhen R dummy codes the factor variable group, DECR will be the reference level. A coefficient will be used to indicate each of the other two groups (INCR and MIX).\nThis can be written as\n\nchange = intercept + group2 + group3 + error\nor\n\\[\\hat{y_i} = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\epsilon_i\\] - If e.g., \\(x_1\\) is 0, then \\(\\beta_1\\) drops from the equation, otherwise it is added to the estimate. - Let’s fit the model (copy the code in to your own session).\n\n\nCode\ndat &lt;- cyclingstudy %&gt;% \n        select(subject, group, timepoint, VO2.max) %&gt;%\n        pivot_wider(names_from = timepoint, \n                    values_from = VO2.max) %&gt;%\n        mutate(change = meso3 - pre) %&gt;%\n        print()\n\nm &lt;- lm(change ~ group, data = dat)\n\n\n# Use summary to retrieve the results\n\nsummary(m)\n\n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\nLook at the figure, and the output from the model.\n\nExplain the estimates for (Intercept), groupINCR and groupMIX.\nThe order of the factor variable group determines the reference level. Change the reference level to MIX using mutate(group = factor(group, levels = c(\"MIX\", \"DECR\", \"INCR\")))\nFit a new model and reinterpret the results.\n\n\n\nA possible solution\ndat &lt;- cyclingstudy %&gt;% \n        select(subject, group, timepoint, VO2.max) %&gt;%\n        pivot_wider(names_from = timepoint, \n                    values_from = VO2.max) %&gt;%\n        mutate(change = meso3 - pre, \n               group = factor(group, levels = c(\"MIX\", \"DECR\", \"INCR\"))) %&gt;%\n        print()\n\nm &lt;- lm(change ~ group, data = dat)\n\n\n# Use summary to retrieve the results\n\nsummary(m)\n\n\n\n\n\n\nThe interpretation of model coefficients in multiple linear regression is the effect when the other coefficients are held constant (at zero).\n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nSelect two or more variables (continuous or categorical) from the cyclingstudy data set and fit a model.\nInterpret the results.\n\n\n\n\n\n\n\n\nHaun et al. (2019) investigated the predictors of responses to a resistance training intervention\nThe composite score of muscle hypertrophy is calculated as:\n\n\nright leg VL muscle thickness assessed via ultrasound\nupper right leg lean soft tissue assessed via DXA\nright leg mid-thigh circumference\nright leg VL mean (type I and type II) muscle fCSA.\n\n(See Haun et al. (2019)).\nTo load the data set:\n\n\nCode\nlibrary(exscidata)\n\nexscidata::hypertrophy\n\n\n\n\n\n\n\n\nGroup work\n\n\n\nUsing the hypertrophy data set:\n\nReproduce the results in figure 9B\nIdentify variables of interest (e.g. strength or muscle mass gains) and explain the using a regression model with a selected set of predictor variables."
  },
  {
    "objectID": "ws9-linear-model-categorical.html#a-bad-example",
    "href": "ws9-linear-model-categorical.html#a-bad-example",
    "title": "The linear model: Categorical predictors and diagnostics",
    "section": "",
    "text": "We will create a bad example of a categorical predictor variable to predict VO2.max with height.T1 in the cyclingstudy data set. Why bad? Let’s find out!\nFirst, let’s dichotomize the height variable into two groups. In our analysis we want to compare the group of “tall” individuals to “short” individuals from the pre time-point.\nThe threshold for the grouping will be above/below the median height.\nIn the code chunk below, we add a new variable height which takes the value tall if height.T1 is greater then the median, otherwise “short”. For clarity, we will also convert the variable to a factor variable.\n\n\n\n\n\n\n\nPractice\n\n\n\n\n\nBased on the description above, and before you look at the example below, write the required commands and save a data set in an object called dat.\n\n\n\n\n\nCode\nlibrary(exscidata); library(tidyverse)\n\ndata(\"cyclingstudy\")\n\n# Reduce the data set \ndat &lt;- cyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        select(subject, height.T1, VO2.max) %&gt;%\n        mutate(height = if_else(height.T1 &gt; median(height.T1), \"tall\", \"short\"), \n               height = factor(height)) %&gt;%\n        print()\n\n\nThis operation results in our new grouping variable as shown in the figure below.\n\n\n\n\n\nDichotomizing height into short/tall groups\n\n\n\n\n\nWe may now formulate a model to assess differences in VO2max between height groups.\nIn the model formulation we will write our factor variable. R will “dummy code” this variable to take the values 0 and 1 in the model.\nDummy coding will translate an ordered factor with two levels to a 0/1 variable where 0 can be called the reference level. Using contrasts(dat$height) we can see the how R has coded the variable. Notice that the variable is called tall.\nOur model formulation will be:\n\n\nVO2.max ~ height\n\n\nThis translates to a more formal description\n\n\\[\\text{VO}_\\text{2max} = \\beta_0 + \\beta_{\\text{tall}}x_i\\]\n\n\n\n\n\n\nGroup work\n\n\n\nFit the model VO2.max ~ height using the dichotomized factor variable for height and calculate the average VO2max for the tall group.\n\n\n\n\n\nIt turns out that many commonly used statistical tests can be performed in the regression model framework.1. We will revisit this idea further on but start here with the simple t-test.\nThe t-test is often the first statistical test taught in a statistics course. The goal is to compare two means (paired- or un-paired observations) or compare a mean to a reference value (one sample t-test).\nThe t-test provides us with an apparatus to test a statistical hypothesis.\nAn interpretation of a part of the results from the test, the p-value can be:\n\nIf we performed sampling from, and comparison of, two populations 1000 times, how often would we see a result so extreme or more extreme than the observed value, if the null-hypothesis of no difference is true.\n\nIf the p-value is low, say below 0.05, we have estimated that the results are pretty unlikely if the null-hypothesis is true (less than 5% of repeated studies will show this result if the null hypothesis is true).\nThe differences in two means (tall vs. short) in a t-test is the same as the slope in an ordinary regression model:\n\n\nlibrary(gt)\n\nWarning: package 'gt' was built under R version 4.4.1\n\nttest &lt;- t.test(VO2.max ~ height, data = dat, var.equal = TRUE)\n\nreg &lt;- lm(VO2.max ~ height, data = dat)\n\ndata.frame(model = c(\"t-test\", \"Regression\"), \n           t.val = c(ttest$statistic, coef(summary(reg))[2,3]),\n           p.val = c(ttest$p.value, coef(summary(reg))[2,4]), \n           row.names = NULL) %&gt;%\n        gt() %&gt;%\n        fmt_auto()\n\n\n\n\n\n\n\n\nmodel\nt.val\np.val\n\n\n\n\nt-test\n−1.574\n0.133\n\n\nRegression\n 1.574\n0.133\n\n\n\n\n\n\n\n\nNotice that:\n\nWe use var.equal = TRUE to fully reproduce the results of the regression analysis. This option assumes equal variation in the two groups.\nThe minus sign in the t-statistic of the t-test indicate that the comparison is reversed in the t-test.\n\n\n\n\n\n\n\nGroup work\n\n\n\nHow do you interpret the model summary of the model VO2.max ~ height using the dichotomized factor variable for height? What does the p-value and slope estimate tell us?"
  },
  {
    "objectID": "ws9-linear-model-categorical.html#why-was-this-a-bad-example-a-short-lesson-on-dichotomization",
    "href": "ws9-linear-model-categorical.html#why-was-this-a-bad-example-a-short-lesson-on-dichotomization",
    "title": "The linear model: Categorical predictors and diagnostics",
    "section": "",
    "text": "When we dichotomize a variable we add assumptions to our analysis.\nIn this case, we add an assumption of a non-important relationship between the dependent and independent variable within each group.\nThis assumption can easily be questioned by creating a plot with height.T1 on the x-axis, VO2.max on the y-axis and different color per grouping.\nWe could additionally add regression lines using geom_smooth(method = \"lm\"). Remove the color argument (aes(color = NULL)) from an additional geom_smooth() to include all data in a linear model.\n\n\n\nTry to create the graph based on the description above (before peeking at a possible solution below). Can we reasonably neglect the linear relationship between variables within groups?\n\n\n\nA possible solution\ndat %&gt;%\n        ggplot(aes(height.T1, VO2.max, color = height))  + geom_point() + \n        geom_smooth(method = \"lm\") +\n        geom_smooth(method = \"lm\", aes(color = NULL))\n\n\n\nSince we are actually removing data before fitting the model we also create a model that is difficult to interpret.\n\n\n\n\n\n\n\nGroup work\n\n\n\nInterpret the estimates from\n\n\na model with height treated as a dichotomized variable\n\n\na model with height included as a continuous variable.\n\n\nExplain to your friend what the estimate tells you about the real-world relationship between VO2max and height from each model.\n\n\n\n\nA possible solution\n# Dichotomized data\nm1 &lt;- lm(VO2.max ~ height, data = dat)\n\n\n# Contentious data\nm2 &lt;- lm(VO2.max ~ height.T1, data = dat)\n\n\nsummary(m1);summary(m2)"
  },
  {
    "objectID": "ws9-linear-model-categorical.html#assumptions-and-diagnostics",
    "href": "ws9-linear-model-categorical.html#assumptions-and-diagnostics",
    "title": "The linear model: Categorical predictors and diagnostics",
    "section": "",
    "text": "A linear regression model comes with a set of assumptions, these concern\n\nThe sampling of data\nThe relationship between variables\nThe resulting errors of the model\n\n\n\n*A single row of a data set, possibly containing multiple variables.\n\nWhen collecting data for a regression analysis we must make sure that each observation* is independent.\nThis means that the ordinary regression model cannot account for multiple observations sampled from the same individual.\nIf this is done, we commit a crime called pseudo-replication!\nThis can also be the case if we sample individuals that are structured together in teams or classrooms etc.\nMore elaborate models are needed to account for such structures in the data.\n\n\n\n\n\n\n\nGroup work\n\n\n\nInterpret the estimates from a model with height treated as a continuous variable but containing all time-points. Compare the results to the previous model containing data only from timepoint == \"pre\"\nDiscuss how this changes the standard error (Std Error) of the model.\nNotice that height was only recorded at pre, to fill height data to all observations:\n\ncyclingstudy %&gt;%\n        select(subject, timepoint, height.T1, VO2.max) %&gt;%\n        ## To get height at all time-points\n        group_by(subject) %&gt;%\n        mutate(height.T1 = mean(height.T1, na.rm = TRUE)) %&gt;%\n        ungroup()\n\n\n\n\n\nA possible solution\ndat.all &lt;- cyclingstudy %&gt;%\n        select(subject, timepoint, height.T1, VO2.max) %&gt;%\n        \n        ## To get height at all time-points\n        group_by(subject) %&gt;%\n        mutate(height.T1 = mean(height.T1, na.rm = TRUE)) %&gt;%\n        ungroup() %&gt;%\n        print()\n\n# Contentious data, all observations\nm1 &lt;- lm(VO2.max ~ height.T1, data = dat.all)\n\n# Contentious data, only pre-data\nm2 &lt;- lm(VO2.max ~ height.T1, data = dat)\n\nsummary(m1);summary(m2)"
  },
  {
    "objectID": "ws9-linear-model-categorical.html#straight-lines",
    "href": "ws9-linear-model-categorical.html#straight-lines",
    "title": "The linear model: Categorical predictors and diagnostics",
    "section": "",
    "text": "The regression model expects straight-line relationships.\nCurve-linear relationships needs more elaborate model formulations.\nIf the underlying data has some curve linear characteristics, a residual plot will aid in detecting it.\n\n\n\nCode\n# create some fake, non-linear data\n\na &lt;- 0.2\nb &lt;- 1\n\nx &lt;- seq(from = 0.2, to = 10, by = 0.2)\n# Adding curve-linear relationship\n# and random error to each data point\ny &lt;- a + b*sin(x) + rnorm(length(x), 0, 0.2) \ny_straight &lt;- a + b*x + rnorm(length(x), 0, 1) \n\n\n# Store in data frame\nfake &lt;- data.frame(x = x, y = y, y_straight = y_straight)\n\n# Plot the raw data, including a model\nraw_data_plot_sig &lt;- fake %&gt;%\n        ggplot(aes(x, y)) + geom_point() + geom_smooth(method = \"lm\", se = FALSE) + \n        labs(title = \"Raw data\",\n             subtitle =  \"Best fit model y ~ x\")\n\nraw_data_plot_straight &lt;- fake %&gt;%\n        ggplot(aes(x, y_straight)) + geom_point() + geom_smooth(method = \"lm\", se = FALSE) + \n        labs(title = \"Raw data\",\n             subtitle =  \"Best fit model y ~ x\")\n\n\n# Model the data with a straight line\nm &lt;- lm(y ~ x, data = fake)\nm_straight &lt;- lm(y_straight ~ x, data = fake)\n\n# Extract residuals \nfake$resid &lt;- resid(m)\nfake$resid_straight &lt;- resid(m_straight)\n\n# Extract predicted/fitted valus\nfake$fitted &lt;- fitted(m)\nfake$fitted_straight &lt;- fitted(m_straight)\n\n# Plot fitted vs. residual values\nresidual_plot_sig &lt;- fake %&gt;%\n        ggplot(aes(fitted, resid)) + geom_point() + \n        labs(title = \"Analysis of residuals\", \n             subtitle =  \"Generated from the model y ~ x\")\n\nresidual_plot_straight &lt;- fake %&gt;%\n        ggplot(aes(fitted_straight, resid_straight)) + geom_point() + \n        labs(title = \"Analysis of residuals\", \n             subtitle =  \"Generated from the model y ~ x\")\n\n# Using patchwork to arrange plots\nlibrary(patchwork)\n\n(raw_data_plot_sig + residual_plot_sig) / (raw_data_plot_straight + residual_plot_straight)\n\n\n\n\n\nModelling a curve linear vs. a linear relationship using a straight line\n\n\n\n\n\n\n\n\n\n\nGroup work\n\n\n\nExplain to a friend:\n\nWhat is the difference between the raw data and the residual vs. fitted plot?\nWhat can go wrong when we assume linear relationships?\nHow can we diagnose the model and know what to expect when fitting it?"
  },
  {
    "objectID": "ws9-linear-model-categorical.html#model-errors",
    "href": "ws9-linear-model-categorical.html#model-errors",
    "title": "The linear model: Categorical predictors and diagnostics",
    "section": "",
    "text": "Above we clearly identified patterns in the residual plot that violated the assumption of a straight line (or linear) relationship between variables.\nAnother assumption can also be assessed using the residual plot (residuals vs. fitted values).\nThis assumption is that errors are equally distributed across the fitted values.\nIn the case of a continuous predictor, the spread of the residuals should vary similarly across the whole range of fitted values.\nIn the case of a categorical variable, residuals should have similar spread in all categories.\nLet’s examine the models formulated above with height, both as a categorical predictor as well as a continuous predictor.\n\n\n\n\nFit the models, only containing the pre data\nSave the residuals to your data set using resid(mod), where mod is the model object\nSimilarly, save the fitted values to your data set using fitted(mod).\nCreate a ggplot of the fitted (x axis) and residual values (y axis).\nInterpret the resulting plots.\n\n\n\nA possible solution\n# Continuous variable\nm1 &lt;- lm(VO2.max ~ height.T1, data = dat)\n\n# Categorical variable \nm2 &lt;- lm(VO2.max ~ height, data = dat)\n\n# Retreive fitted and residual values\ndat$fitted_cont &lt;- fitted(m1)\ndat$resid_cont &lt;- resid(m1)\n\ndat$fitted_cat &lt;- fitted(m2)\ndat$resid_cat &lt;- resid(m2)\n\n# Create figures\nresid_plot_cont &lt;- dat %&gt;%\n        ggplot(aes(fitted_cont, resid_cont)) + geom_point() +\n        labs(title = \"Continous predictor\")\n\n\nresid_plot_cat &lt;- dat %&gt;%\n        ggplot(aes(fitted_cat, resid_cat)) + geom_point()+\n        labs(title = \"Categoricals predictor\")\n\n# Using patchwork to combine the figures\nlibrary(patchwork)\n\nresid_plot_cont + resid_plot_cat\n\n\n\n\nIt is not always easy to tell if there is heteroscedasticity, especially with small number of observations.\nThe graphical method is convenient to use, but there are also tests to detect it. One such test is Breusch–Pagan test.\n\nIn the figure below, two scenarios are presented, one homoscedastic and one heteroscedastic scenario.\n\n\n\n\n\n\nHomoscedasticity vs. heteroscedasticity\n\n\n\n\n\nIt may be difficult to detect heteroscedasticity from a single model created with a small sample size.\n\n\n\n\n\n\nSampling data from population data with equal vs. unequal variation across x-values.\n\n\n\n\n\nIn fact, graphical analysis and formal tests of population characteristics are dependent on the sample size.\nA larger sample size will more often detect heteroscedasticity in the population data.\n\n\n\n\n\n\nSampling data with different sizes from population data with equal vs. unequal variation across x-values and comparing p-values from a test of homoscedasticity.\n\n\n\n\n\n\n\n\n\nSampling data with different sizes from population data with equal vs. unequal variation across x-values and plotting the ability to detect violoations of equal variance.\n\n\n\n\n\nIf we detect violations of the assumption of equal variation across the fitted values we can attempt transformations.\nThe log-transformation can be used when the variance increases with larger values (positive correlation between fitted values and variance).\nNote that log transformation of the dependent variable changes the interpretation of the model. Instead of additive, we evaluate the multiplicative effect. For a unit difference in x, y changes by a percentage.\n\n\n\nCode\n# create some fake, non-linear data\nset.seed(10)\na &lt;- 20\nb &lt;- 15\n\nx &lt;- seq(from = 5, to = 20, by = 0.2)\n\n# and random error to each data point\ny &lt;- a + b*x + rnorm(length(x), 0, 2*x) \n\n\n\n# Store in data frame\nfake &lt;- data.frame(x = x, y = y)\n\n# Plot the raw data, including a model\nraw_data_plot &lt;- fake %&gt;%\n        ggplot(aes(x, y)) + geom_point() + geom_smooth(method = \"lm\", se = FALSE) + \n        labs(title = \"Raw data\",\n             subtitle =  \"Best fit model y ~ x\")\n\nraw_data_plot_log &lt;- fake %&gt;%\n        ggplot(aes(x, log(y))) + geom_point() + geom_smooth(method = \"lm\", se = FALSE) + \n        labs(title = \"Raw data\",\n             subtitle =  \"Best fit model log(y) ~ x\")\n\n# Model the data with a straight line\nm &lt;- lm(y ~ x, data = fake)\nlogm &lt;- lm(log(y) ~ x, data = fake)\n\n# Test for heteroscedasticity\nbp_m &lt;- lmtest::bptest(m)\nbp_logm &lt;- lmtest::bptest(logm)\n\n\n# Extract residuals \nfake$resid &lt;- resid(m)\nfake$residlog &lt;- resid(logm)\n\n# Extract predicted/fitted valus\nfake$fitted &lt;- fitted(m)\nfake$fittedlog &lt;- fitted(logm)\n\n# Plot fitted vs. residual values\nresidual_plot &lt;- fake %&gt;%\n        ggplot(aes(fitted, resid)) + geom_point() + \n        labs(title = \"Analysis of residuals\", \n             subtitle =  \"Generated from the model y ~ x\", \n             caption = paste0(\"Breusch–Pagan test p-value: \", round(bp_m$p.value, 5))) \n\nresidual_plot_log &lt;- fake %&gt;%\n        ggplot(aes(fittedlog, residlog)) + geom_point() + \n        labs(title = \"Analysis of residuals\", \n             subtitle =  \"Generated from the model log(y) ~ x\", \n             caption = paste0(\"Breusch–Pagan test p-value: \", round(bp_logm$p.value, 3)))\n\n\n# Using patchwork to arrange plots\nlibrary(patchwork)\n\n(raw_data_plot + residual_plot) / (raw_data_plot_log + residual_plot_log) \n\n\n\n\n\nDealing with heteroscedasticity using log-transformation of the data\n\n\n\n\n\n\n\nI addition the assumption of errors being equally distributed (homoscedasticity), we also assume that they are normally distributed.\nThis is also conveniently evaluated graphically using a histogram of the residuals and a qq-plot.\n\n\n\n\n\n\nAssessing normality of residuals using graphical methods\n\n\n\n\n\nThe qq-plot indicates deviations from the normal distribution as points that fall far away from the line deviates from a theoretical distribution.\nDeviations from the assumption of normal residuals may be due to a number of “outliers”.\nLet’s add 200 to 6 observations from the previous diagnostic plots and re-fit the model.\n\n\n\n\n\n\nAssessing normality of residuals using graphical methods with 6 observations inflated by 200 units"
  },
  {
    "objectID": "ws9-linear-model-categorical.html#interpreting-the-output-from-plotmodel",
    "href": "ws9-linear-model-categorical.html#interpreting-the-output-from-plotmodel",
    "title": "The linear model: Categorical predictors and diagnostics",
    "section": "",
    "text": "R has nice built in functions for graphical assessment of regression models\nAfter fitting a model, simply use plot(model)\nThe resulting plots are similar to what we have examined so far\n\n\n# Copy and run the code on your own!\n\ndat &lt;- cyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        select(subject, height.T1, VO2.max) \n\nm &lt;- lm(VO2.max ~ height.T1, data = dat)\n\nplot(m)\n\n\nThe first plot is the ordinary Residuals vs. Fitted plot that we created above. Here we can assess the assumption regarding homoscedasticity.\nThe second plot is a Q-Q plot where we can assess the assumption regarding normality of the residuals\nThe third plot provides a slightly different version of the residual vs. fitted plot. Scale-location has the advantage of showing absolute values, this may make it easier to detect heteroscedasticity.\nThe forth plot contains residuals vs. leverage where leverage indicates influential data points.\nAn influential data point has a lot of influence on the regression model. Let’s see how.\n\n\n\nCode\ndat2 &lt;- dat %&gt;%\n        mutate(added = \"NO\") %&gt;%\n        add_row(subject = 200, height.T1 = 200, VO2.max = 7000, added = \"YES\")\n\n\nm &lt;- lm(VO2.max ~ height.T1, data = dat2) \n\n\n\n\nraw_data &lt;- dat2 %&gt;%\n        ggplot(aes(height.T1, VO2.max, color = added)) + \n        geom_point() +\n        geom_smooth(method = \"lm\", se = FALSE) +\n        geom_smooth(method = \"lm\", aes(color = NULL), se = FALSE) +\n        labs(title = \"An influential data point added\")\n\n\n### These functions are needed to plot the cooks distance\ncd_cont_pos &lt;- function(leverage, level, model) {sqrt(level*length(coef(model))*(1-leverage)/leverage)}\ncd_cont_neg &lt;- function(leverage, level, model) {-cd_cont_pos(leverage, level, model)}\n\n\n\nleverage_plot &lt;- data.frame(leverage = hatvalues(m), \n                            scaled.resid = resid(m)/ sd(resid(m)), \n                            Added = dat2$added) %&gt;%\n        ggplot(aes(leverage, scaled.resid, fill = Added)) + geom_point(shape = 21, size = 2) +\n        \n        stat_function(fun = cd_cont_pos, args = list(level = 0.5, model = m), xlim = c(0, 0.5), lty = 2, colour = \"red\") +\n        stat_function(fun = cd_cont_neg, args = list(level = 0.5, model = m), xlim = c(0, 0.5), lty = 2, colour = \"red\")  +\n        \n        stat_function(fun = cd_cont_pos, args = list(level = 1, model = m), xlim = c(0, 0.5), lty = 1, colour = \"red\") +\n        stat_function(fun = cd_cont_neg, args = list(level = 1, model = m), xlim = c(0, 0.5), lty = 1, colour = \"red\")  +\n        \n        scale_y_continuous(limits = c(-2, 2.5)) +\n        labs(x = \"Leverage\", \n             y = \"Standardized residuals\", \n             title = \"Identifying an influential data point\", \n             subtitle = \"Cook's distance indicated by red lines\")\n\n\nraw_data + leverage_plot\n\n\n\n\n\nExamining the effect of an influential data point"
  },
  {
    "objectID": "ws9-linear-model-categorical.html#summary",
    "href": "ws9-linear-model-categorical.html#summary",
    "title": "The linear model: Categorical predictors and diagnostics",
    "section": "",
    "text": "Review your understanding\n\n\n\n\n\nExplain these concepts:\n\nPredictor variable, dependent variable, independent variable\nContinuous variable, dummy variable\nFitted values, residuals, standardized residuals\nHomoscedasticity, heteroscedasticity, equal variance, normality of residuals\nPseudo-replication"
  },
  {
    "objectID": "ws9-linear-model-categorical.html#multiple-linear-regression",
    "href": "ws9-linear-model-categorical.html#multiple-linear-regression",
    "title": "The linear model: Categorical predictors and diagnostics",
    "section": "",
    "text": "The ordinary linear model is easily extended to include more predictor variables.\nIn fact, when we add a categorical variable with more than 2 levels, we effectively perform multiple regression.\nLet’s say that we want to estimate the differences in change in VO2max (ml min-1) between the three groups in the cycling study data set (see figure).\n\n\n\nCode\nlibrary(ggtext) # needed to use markdown in legends/axis texts\n\ncyclingstudy %&gt;% \n        select(subject, group, timepoint, VO2.max) %&gt;%\n        pivot_wider(names_from = timepoint, \n                    values_from = VO2.max) %&gt;%\n        mutate(change = meso3 - pre) %&gt;%\n        ggplot(aes(group, change)) + geom_boxplot() +\n        labs(y = \"VO&lt;sub&gt;2max&lt;/sub&gt; (ml min&lt;sup&gt;-1&lt;/sup&gt;) change from pre to meso3\", \n             x = \"Group\", \n             title = \"Absolute change in maximal aerobic power\") + \n        \n        theme(axis.title.y = element_markdown())\n\n\n\n\n\nChanges in VO2max from pre to meso3 in three groups in the cycling study data set\n\n\n\n\n\nWhen R dummy codes the factor variable group, DECR will be the reference level. A coefficient will be used to indicate each of the other two groups (INCR and MIX).\nThis can be written as\n\nchange = intercept + group2 + group3 + error\nor\n\\[\\hat{y_i} = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\epsilon_i\\] - If e.g., \\(x_1\\) is 0, then \\(\\beta_1\\) drops from the equation, otherwise it is added to the estimate. - Let’s fit the model (copy the code in to your own session).\n\n\nCode\ndat &lt;- cyclingstudy %&gt;% \n        select(subject, group, timepoint, VO2.max) %&gt;%\n        pivot_wider(names_from = timepoint, \n                    values_from = VO2.max) %&gt;%\n        mutate(change = meso3 - pre) %&gt;%\n        print()\n\nm &lt;- lm(change ~ group, data = dat)\n\n\n# Use summary to retrieve the results\n\nsummary(m)\n\n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\nLook at the figure, and the output from the model.\n\nExplain the estimates for (Intercept), groupINCR and groupMIX.\nThe order of the factor variable group determines the reference level. Change the reference level to MIX using mutate(group = factor(group, levels = c(\"MIX\", \"DECR\", \"INCR\")))\nFit a new model and reinterpret the results.\n\n\n\nA possible solution\ndat &lt;- cyclingstudy %&gt;% \n        select(subject, group, timepoint, VO2.max) %&gt;%\n        pivot_wider(names_from = timepoint, \n                    values_from = VO2.max) %&gt;%\n        mutate(change = meso3 - pre, \n               group = factor(group, levels = c(\"MIX\", \"DECR\", \"INCR\"))) %&gt;%\n        print()\n\nm &lt;- lm(change ~ group, data = dat)\n\n\n# Use summary to retrieve the results\n\nsummary(m)\n\n\n\n\n\n\nThe interpretation of model coefficients in multiple linear regression is the effect when the other coefficients are held constant (at zero).\n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nSelect two or more variables (continuous or categorical) from the cyclingstudy data set and fit a model.\nInterpret the results."
  },
  {
    "objectID": "ws9-linear-model-categorical.html#the-hypertrophy-data-set",
    "href": "ws9-linear-model-categorical.html#the-hypertrophy-data-set",
    "title": "The linear model: Categorical predictors and diagnostics",
    "section": "",
    "text": "Haun et al. (2019) investigated the predictors of responses to a resistance training intervention\nThe composite score of muscle hypertrophy is calculated as:\n\n\nright leg VL muscle thickness assessed via ultrasound\nupper right leg lean soft tissue assessed via DXA\nright leg mid-thigh circumference\nright leg VL mean (type I and type II) muscle fCSA.\n\n(See Haun et al. (2019)).\nTo load the data set:\n\n\nCode\nlibrary(exscidata)\n\nexscidata::hypertrophy\n\n\n\n\n\n\n\n\nGroup work\n\n\n\nUsing the hypertrophy data set:\n\nReproduce the results in figure 9B\nIdentify variables of interest (e.g. strength or muscle mass gains) and explain the using a regression model with a selected set of predictor variables."
  },
  {
    "objectID": "ws9-linear-model-categorical.html#footnotes",
    "href": "ws9-linear-model-categorical.html#footnotes",
    "title": "The linear model: Categorical predictors and diagnostics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee Common statistical tests are linear models https://lindeloev.github.io/tests-as-linear/↩︎"
  },
  {
    "objectID": "ws5-writing-reports.html#creating-an-empty-project-github-first",
    "href": "ws5-writing-reports.html#creating-an-empty-project-github-first",
    "title": "Writing version controlled reports",
    "section": "Creating an empty project (GitHub first)",
    "text": "Creating an empty project (GitHub first)\n\n\n\n\n\n\nExercise (1)\n\n\n\nIn groups of three (Student A, B and C), work together and help Student A to:\n\nGo to github.com and sign in\nCreate a new repository\nGive the repository a good name\nMake the repository public, and add a readme file\nAfter pressing “create new repository”, Add your fellow group members to be collaborators\nAll group members must accept the invitation to the repository\n\n\n\n\nYour new github project is now ready to be downloaded into an RStudio project.\n\n\n\n\n\n\n\nExercise (2)\n\n\n\nStudent A, B and C all do the same steps on their own computer\n\nOpen RStudio and create a New Project from the RStudio project menu\nChose “Version Control” and add the address to the repository\nSave your version of the project at a good location on your computer (What is a good location?)\nOpen Github Desktop and add the local repository (your local folder) to your list of repositories\n\n\n\n\nAll group members have downloaded a copy of the original repository, we will start making changes and see how we can “share” them across group members.\n\n\n\n\n\n\n\nExercise (3)\n\n\n\nStudent A performs the following steps\n\nAdd a quarto document called my-report.qmd to the root project folder.\nUsing GitHub desktop Student A commits changes (addition of the file) and push to the remote repository\n\nStudent B and C does\n\nIn Github desktop, press “fetch origin”.\nInspect what changes occurred in Files in R Studio.\n\n\n\n\nNext we want to add different content to the report. We will do this simultaneously and see how git manages this.\n\n\n\n\n\n\n\nExercise (4)\n\n\n\nStudent B:\n\nAdd a table to the report, use the table created during homework (a solution can be found here)\nThe table should be cross-referenced and displayed without the code (#| echo: false)\nCommit changes with a message and push\n\nStudent C:\n\nAdd a figure to the report, use the code from the homework as can be found here.\nThe figure should have a label and cross-referenced without the code (#| echo: false)\nCommit changes with a message and push\n\nStudent A: 1. Fetch origin and inspect changes"
  },
  {
    "objectID": "ws5-writing-reports.html#git-ignore",
    "href": "ws5-writing-reports.html#git-ignore",
    "title": "Writing version controlled reports",
    "section": "Git ignore",
    "text": "Git ignore\n\nSometimes you would like to store information in your folder locally that is not meant to be version controlled\nGit ignore can make this possible\n\n\n\n\n\n\n\nExercise (4)\n\n\n\nAll group members\n\nIn RStudio, find the .gitignore file and add a line that says my-files-studentA/ (if you play the part of student A),\nIn RStudio create a text-file and store it in a folder called my-files/\nCommit changes and push.\nFetch any changes and inspect what happens to your repo."
  },
  {
    "objectID": "ws5-writing-reports.html#using-a-branch-for-a-large-change",
    "href": "ws5-writing-reports.html#using-a-branch-for-a-large-change",
    "title": "Writing version controlled reports",
    "section": "Using a branch for a large change",
    "text": "Using a branch for a large change\n\nBranches are different from forks as they are parallel copies of the main working branch\nA branch can be used to introduce some new feature to the repository which needs more attention.\n\n\n\n\n\n\n\nExercise (5)\n\n\n\nStudent A:\n\nIn Github desktop, create a new branch, name it new-branch\nAdd a bibliography to the report (using the visual editor and a DOI of your choosing).\nCommit changes to the new branch.\nGo to GitHub.com and make a pull request.\nReview the change and merge the branches (on Github.com)"
  },
  {
    "objectID": "ws5-writing-reports.html#using-forks-for-changes",
    "href": "ws5-writing-reports.html#using-forks-for-changes",
    "title": "Writing version controlled reports",
    "section": "Using forks for changes",
    "text": "Using forks for changes\n\n\n\n\n\n\nExercise (5)\n\n\n\nStudent B and C:\n\nIn Github desktop, fork the original repository to your own user profile\nDo a change to the repository\nCommit changes and create a pull request\nStudent A reviews the pull requests."
  },
  {
    "objectID": "ws5-writing-reports.html#conflicts",
    "href": "ws5-writing-reports.html#conflicts",
    "title": "Writing version controlled reports",
    "section": "Conflicts!",
    "text": "Conflicts!\n\n\n\n\n\n\nExercise (6)\n\n\n\n\nStudent A selects a specific line in the report and edits it, commits and push\nStudent B (without first fetching), change the same line, commits and push.\nWhat does the error message say,\nResolve the conflict and commit/push the final version."
  },
  {
    "objectID": "ws4-data-wrangling-tables.html",
    "href": "ws4-data-wrangling-tables.html",
    "title": "Data wrangling and tables",
    "section": "",
    "text": "The tidyverse contains two packages with functions used to wrangle data, dplyr and tidyr. On wikipedia we can read that:\n\nData analysts typically spend the majority of their time in the process of data wrangling compared to the actual analysis of the data.\n\nIn the age of data, we need data wrangling\n\n\ndplyr contains verbs used for data manipulation, such as filtering rows, selecting variables and changing or creating variables. Verbs can be used in a pipe and read as sequential operations:\n\nTake the data then do\n\n\nfilter based on group then do\n\n\ncreate a new variable then do\n\n\nshow the data in the console.\n\n\n\n\nA pipe\n\n\nThe pipe operations are made possible by another package, magrittr. This package contains the forward pipe operator %&gt;%. The forward pipe operator (%&gt;%) can be read as “then do”. The operator takes the object on the left-hand side and puts it as the first argument in the following function.\nTranslating the “pipe” above from human language to R language using the “tidyverse dialect” looks like this:\n\ndata %&gt;%\n        filter(group == \"xx\") %&gt;%\n        mutate(new.var = old.var + old.var2) %&gt;%\n        print()\n\n\n\n\nWe will use the cyclingstudy data from exscidata in our exercises. Load the required components to your session.\n\n\nShow the code\nlibrary(tidyverse) # loads dplyr etc.\nlibrary(exscidata) # loads the data-package\n\ndata(\"cyclingstudy\")\n\n\n\n\n\nMutate can help you create new variables (or overwrite existing once). In the cycling data set there is a variable called VO2.max, this variable is expressed in absolute units (ml min-1), however, we might want to express it as relative units (ml kg-1 min-1).\n\ncyclingstudy %&gt;%\n        mutate(rel.vo2max = VO2.max / weight.T1) %&gt;%\n        print()\n\nThe mutate function creates new variables (or overwrite existing) in a flexible way. Here we simply use division. Other mathematical operators can similarly be used (+, -, *, etc.).\nNotice the print() function in the end of the pipe. This is used to display the results of any manipulations done in the pipe. Notice also that our new variable is not listed. We might need to select a sub-set of variables to get a better overview. We will do this using the select function.\n\ncyclingstudy %&gt;%\n        mutate(rel.vo2max = VO2.max / weight.T1) %&gt;%\n        select(subject, group, timepoint, age, height.T1, weight.T1, VO2.max, rel.vo2max) %&gt;%\n        print()\n\nThe select function takes variable names as “unquoted” names. We can also select a range of columns using the syntax &lt;from&gt;:&lt;to&gt; where &lt;from&gt; is the first column you would like to select and &lt;to&gt; would be the last. Subsequently the above pipe can be re-written as\n\ncyclingstudy %&gt;%\n        mutate(rel.vo2max = VO2.max / weight.T1) %&gt;%\n        select(subject:weight.T1, VO2.max, rel.vo2max) %&gt;%\n        print()\n\nselect can also be used to re-name variables. Simpler variable names for weight, height and relative VO2max could be weight, height, vo2max.kg.\n\ncyclingstudy %&gt;%\n        mutate(rel.vo2max = VO2.max / weight.T1) %&gt;%\n        select(subject:age, height = height.T1, weight = weight.T1, VO2.max,vo2max.kg = rel.vo2max) %&gt;%\n        print()\n\n\n\nWhen you have selected and mutated variables you might want to keep the new data frame in your environment. Do this by assigning the whole pipe to a new object.\n\ncyc.subset &lt;- cyclingstudy %&gt;%\n        mutate(rel.vo2max = VO2.max / weight.T1) %&gt;%\n        select(subject:age, height = height.T1, weight = weight.T1, VO2.max,vo2max.kg = rel.vo2max) %&gt;%\n        print()\n\n\n\n\n\n\n\nExercises (1)\n\n\n\n\nSelect the variables subject, group, timepoint, age, height.T1, weight.T1. Rename the variables weight.T1 and height.T1 to weight and height.\nCalculate body mass index (BMI) for all observations using the formula \\(\\text{BMI} = \\frac{\\text{weight (kg)}}{\\text{height (m)}^2}\\)\nStore the data frame as cyc.bmi in your environment.\n\n\n\n\n\n\n\nFiltering is used to select specific observations (rows) of a data set. We filter based on specific conditions such as:\n\nAll values bigger than X\nAll values less than Y\nAll observations than contain A, B or C in variable V.\n\nThe above examples must be translated to formal expressions.\n\n\nAn expression that make comparisons can be\n\nx &lt; y → x less than y\nx &gt; y → x greater than y\nx &lt;= y → x less or equal to y\nx &gt;= y → x greater or equal to y\nx == y → x exactly equal to y\nx != y → x not exactly equal to y\n\nIn the filter function these expressions give either TRUE or FALSE. If TRUE the rows are included in the filtered data frame.\nWe can see the mechanism behind filtering by creating a vector of TRUE and FALSE based on an expression. Let’s say that we want to see which rows has weight.T1 greater than 75.\n\ncyclingstudy$weight.T1 &gt; 75\n\n [1]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n[13]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE\n[25]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE  TRUE\n[37]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE    NA  TRUE  TRUE FALSE\n[49]    NA  TRUE FALSE    NA  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[61]  TRUE FALSE  TRUE  TRUE    NA  TRUE  TRUE FALSE  TRUE FALSE FALSE FALSE\n[73]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n\n\nWe can see that the first row returns a TRUE while the second row returns FALSE.\nUsing the filter function, we just add the expression as an argument in the function and all the rows that comes up TRUE will remain.\n\ncyclingstudy %&gt;%\n        filter(weight.T1 &gt; 75) %&gt;%\n        print()\n\n# A tibble: 60 × 101\n   subject group timepoint   age height.T1 weight.T1 sj.max cmj.max lac.125\n     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1       1 INCR  pre          33       183      80.3   31.0    35.0    1.5 \n 2       3 INCR  pre          39       193      98.1   26.8    28.8    1.17\n 3       4 DECR  pre          37       175      79.2   29.2    30.8    0.88\n 4       5 DECR  pre          31       176      88     31.2    25.8    1.06\n 5       6 INCR  pre          33       168      79.6   34.2    35.3    1.27\n 6       7 MIX   pre          42       180      77.6   30.1    33.0    0.85\n 7       8 MIX   pre          26       179      75.5   32.8    33.2    0.93\n 8       9 MIX   pre          41       185      82.4   22.7    22.4    1.48\n 9      10 INCR  pre          35       187      75.6   29.7    31.1    0.93\n10      13 DECR  pre          41       183      76     34.3    36      1.67\n# ℹ 50 more rows\n# ℹ 92 more variables: lac.175 &lt;dbl&gt;, lac.225 &lt;dbl&gt;, lac.250 &lt;dbl&gt;,\n#   lac.275 &lt;dbl&gt;, lac.300 &lt;dbl&gt;, lac.325 &lt;dbl&gt;, lac.350 &lt;dbl&gt;, lac.375 &lt;dbl&gt;,\n#   VO2.125 &lt;dbl&gt;, VO2.175 &lt;dbl&gt;, VO2.225 &lt;dbl&gt;, VO2.250 &lt;dbl&gt;, VO2.275 &lt;dbl&gt;,\n#   VO2.300 &lt;dbl&gt;, VO2.325 &lt;dbl&gt;, VO2.350 &lt;dbl&gt;, VO2.375 &lt;dbl&gt;, VCO2.125 &lt;dbl&gt;,\n#   VCO2.175 &lt;dbl&gt;, VCO2.225 &lt;dbl&gt;, VCO2.250 &lt;dbl&gt;, VCO2.275 &lt;dbl&gt;,\n#   VCO2.300 &lt;dbl&gt;, VCO2.325 &lt;dbl&gt;, VCO2.350 &lt;dbl&gt;, VCO2.375 &lt;dbl&gt;, …\n\n\n\n\n\n\n\n\nExercises (2)\n\n\n\nHow many rows in the cyclingstudy data set has\n\nVO2.max values greater than 6000\nVO2.max values less than 6000\nVO2.max values less or equal to than 5360\nVO2.max values greater or equal to than 5360\nthe value pre in timepoint\na value in timepoint other than pre\n\n\n\n\n\n\nLogical operators similarly creates TRUE or FALSE as the basis of filtering operations. These can be used in combination with comparisons.\n\n! x → NOT x\nx & y → x and y\nx | y → x or y\nis.na(x) → returns TRUE if x is NA\n\nWe might want to keep all rows with weight.T1 greater than 80 that are also from the group INCR. This can be solved with an AND operator (&).\n\n#| echo: true\n\ncyclingstudy %&gt;%\n        filter(weight.T1 &gt; 80 & group == \"INCR\") %&gt;%\n        print()\n\n# A tibble: 9 × 101\n  subject group timepoint   age height.T1 weight.T1 sj.max cmj.max lac.125\n    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1       1 INCR  pre          33       183      80.3   31.0    35.0    1.5 \n2       3 INCR  pre          39       193      98.1   26.8    28.8    1.17\n3      20 INCR  pre          42       180      82.3   32.5    32.2    2.26\n4       3 INCR  meso1        39        NA     101.    26.0    26.5    0.72\n5      20 INCR  meso1        42        NA      82.5   30.2    30.6    0.96\n6       3 INCR  meso2        39        NA      99.2   25.4    26.8    0.5 \n7      20 INCR  meso2        42        NA      81.1   29.6    29.8    1.53\n8       3 INCR  meso3        39        NA      99.2   27.1    27.0    0.77\n9      20 INCR  meso3        43        NA      81.5   30.0    30.9    1.77\n# ℹ 92 more variables: lac.175 &lt;dbl&gt;, lac.225 &lt;dbl&gt;, lac.250 &lt;dbl&gt;,\n#   lac.275 &lt;dbl&gt;, lac.300 &lt;dbl&gt;, lac.325 &lt;dbl&gt;, lac.350 &lt;dbl&gt;, lac.375 &lt;dbl&gt;,\n#   VO2.125 &lt;dbl&gt;, VO2.175 &lt;dbl&gt;, VO2.225 &lt;dbl&gt;, VO2.250 &lt;dbl&gt;, VO2.275 &lt;dbl&gt;,\n#   VO2.300 &lt;dbl&gt;, VO2.325 &lt;dbl&gt;, VO2.350 &lt;dbl&gt;, VO2.375 &lt;dbl&gt;, VCO2.125 &lt;dbl&gt;,\n#   VCO2.175 &lt;dbl&gt;, VCO2.225 &lt;dbl&gt;, VCO2.250 &lt;dbl&gt;, VCO2.275 &lt;dbl&gt;,\n#   VCO2.300 &lt;dbl&gt;, VCO2.325 &lt;dbl&gt;, VCO2.350 &lt;dbl&gt;, VCO2.375 &lt;dbl&gt;,\n#   VE.125 &lt;dbl&gt;, VE.175 &lt;dbl&gt;, VE.225 &lt;dbl&gt;, VE.250 &lt;dbl&gt;, VE.275 &lt;dbl&gt;, …\n\n\nWe can similarly use OR (|) to select either weight greater than 80 or group INCR.\n\n#| echo: true\n\ncyclingstudy %&gt;%\n        filter(weight.T1 &gt; 80 | group == \"INCR\") %&gt;%\n        print()\n\n# A tibble: 46 × 101\n   subject group timepoint   age height.T1 weight.T1 sj.max cmj.max lac.125\n     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1       1 INCR  pre          33       183      80.3   31.0    35.0    1.5 \n 2       3 INCR  pre          39       193      98.1   26.8    28.8    1.17\n 3       5 DECR  pre          31       176      88     31.2    25.8    1.06\n 4       6 INCR  pre          33       168      79.6   34.2    35.3    1.27\n 5       9 MIX   pre          41       185      82.4   22.7    22.4    1.48\n 6      10 INCR  pre          35       187      75.6   29.7    31.1    0.93\n 7      14 MIX   pre          35       183      81.3   27.6    30.0    1.13\n 8      15 INCR  pre          34       178      75.1   33.5    32.4    0.8 \n 9      16 INCR  pre          27       178      77.8   32.9    33.7    0.94\n10      18 DECR  pre          41       186     105.    33.5    33.7    1.75\n# ℹ 36 more rows\n# ℹ 92 more variables: lac.175 &lt;dbl&gt;, lac.225 &lt;dbl&gt;, lac.250 &lt;dbl&gt;,\n#   lac.275 &lt;dbl&gt;, lac.300 &lt;dbl&gt;, lac.325 &lt;dbl&gt;, lac.350 &lt;dbl&gt;, lac.375 &lt;dbl&gt;,\n#   VO2.125 &lt;dbl&gt;, VO2.175 &lt;dbl&gt;, VO2.225 &lt;dbl&gt;, VO2.250 &lt;dbl&gt;, VO2.275 &lt;dbl&gt;,\n#   VO2.300 &lt;dbl&gt;, VO2.325 &lt;dbl&gt;, VO2.350 &lt;dbl&gt;, VO2.375 &lt;dbl&gt;, VCO2.125 &lt;dbl&gt;,\n#   VCO2.175 &lt;dbl&gt;, VCO2.225 &lt;dbl&gt;, VCO2.250 &lt;dbl&gt;, VCO2.275 &lt;dbl&gt;,\n#   VCO2.300 &lt;dbl&gt;, VCO2.325 &lt;dbl&gt;, VCO2.350 &lt;dbl&gt;, VCO2.375 &lt;dbl&gt;, …\n\n\nNotice that there are rows containing weights less than 80 from the INCR group.\nAny logical statement can also be negated with ! indication NOT. This means we will get a vector of TRUE for any expression previously being FALSE. Notice the extra parentheses below.\n\n#| echo: true\n\ncyclingstudy %&gt;%\n        filter(!(weight.T1 &gt; 80 & group == \"INCR\")) %&gt;%\n        print()\n\n# A tibble: 71 × 101\n   subject group timepoint   age height.T1 weight.T1 sj.max cmj.max lac.125\n     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1       2 DECR  pre          32       174      71.4   31.6    33.8    1.19\n 2       4 DECR  pre          37       175      79.2   29.2    30.8    0.88\n 3       5 DECR  pre          31       176      88     31.2    25.8    1.06\n 4       6 INCR  pre          33       168      79.6   34.2    35.3    1.27\n 5       7 MIX   pre          42       180      77.6   30.1    33.0    0.85\n 6       8 MIX   pre          26       179      75.5   32.8    33.2    0.93\n 7       9 MIX   pre          41       185      82.4   22.7    22.4    1.48\n 8      10 INCR  pre          35       187      75.6   29.7    31.1    0.93\n 9      11 MIX   pre          34       168      55.8   33.2    30.8    0.62\n10      13 DECR  pre          41       183      76     34.3    36      1.67\n# ℹ 61 more rows\n# ℹ 92 more variables: lac.175 &lt;dbl&gt;, lac.225 &lt;dbl&gt;, lac.250 &lt;dbl&gt;,\n#   lac.275 &lt;dbl&gt;, lac.300 &lt;dbl&gt;, lac.325 &lt;dbl&gt;, lac.350 &lt;dbl&gt;, lac.375 &lt;dbl&gt;,\n#   VO2.125 &lt;dbl&gt;, VO2.175 &lt;dbl&gt;, VO2.225 &lt;dbl&gt;, VO2.250 &lt;dbl&gt;, VO2.275 &lt;dbl&gt;,\n#   VO2.300 &lt;dbl&gt;, VO2.325 &lt;dbl&gt;, VO2.350 &lt;dbl&gt;, VO2.375 &lt;dbl&gt;, VCO2.125 &lt;dbl&gt;,\n#   VCO2.175 &lt;dbl&gt;, VCO2.225 &lt;dbl&gt;, VCO2.250 &lt;dbl&gt;, VCO2.275 &lt;dbl&gt;,\n#   VCO2.300 &lt;dbl&gt;, VCO2.325 &lt;dbl&gt;, VCO2.350 &lt;dbl&gt;, VCO2.375 &lt;dbl&gt;, …\n\n\nThe dplyr function filter also accepts multiple arguments separated with a comma. This is equal to adding conditions with the AND operator. Example:\n\n#| echo: true\n\ncyclingstudy %&gt;%\n        filter(weight.T1 &gt; 80, \n               group == \"INCR\", \n               timepoint == \"pre\", \n               age &gt; 35) %&gt;%\n        print()\n\n# A tibble: 2 × 101\n  subject group timepoint   age height.T1 weight.T1 sj.max cmj.max lac.125\n    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1       3 INCR  pre          39       193      98.1   26.8    28.8    1.17\n2      20 INCR  pre          42       180      82.3   32.5    32.2    2.26\n# ℹ 92 more variables: lac.175 &lt;dbl&gt;, lac.225 &lt;dbl&gt;, lac.250 &lt;dbl&gt;,\n#   lac.275 &lt;dbl&gt;, lac.300 &lt;dbl&gt;, lac.325 &lt;dbl&gt;, lac.350 &lt;dbl&gt;, lac.375 &lt;dbl&gt;,\n#   VO2.125 &lt;dbl&gt;, VO2.175 &lt;dbl&gt;, VO2.225 &lt;dbl&gt;, VO2.250 &lt;dbl&gt;, VO2.275 &lt;dbl&gt;,\n#   VO2.300 &lt;dbl&gt;, VO2.325 &lt;dbl&gt;, VO2.350 &lt;dbl&gt;, VO2.375 &lt;dbl&gt;, VCO2.125 &lt;dbl&gt;,\n#   VCO2.175 &lt;dbl&gt;, VCO2.225 &lt;dbl&gt;, VCO2.250 &lt;dbl&gt;, VCO2.275 &lt;dbl&gt;,\n#   VCO2.300 &lt;dbl&gt;, VCO2.325 &lt;dbl&gt;, VCO2.350 &lt;dbl&gt;, VCO2.375 &lt;dbl&gt;,\n#   VE.125 &lt;dbl&gt;, VE.175 &lt;dbl&gt;, VE.225 &lt;dbl&gt;, VE.250 &lt;dbl&gt;, VE.275 &lt;dbl&gt;, …\n\n\nFinally, dplyr comes with two convenient functions to find values between and near.\nbetween() lets you select observations between two values:\n\n#| echo: true\n\ncyclingstudy %&gt;%\n        filter( between(age, 31, 42)) %&gt;%\n        print()\n\n# A tibble: 62 × 101\n   subject group timepoint   age height.T1 weight.T1 sj.max cmj.max lac.125\n     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1       1 INCR  pre          33       183      80.3   31.0    35.0    1.5 \n 2       2 DECR  pre          32       174      71.4   31.6    33.8    1.19\n 3       3 INCR  pre          39       193      98.1   26.8    28.8    1.17\n 4       4 DECR  pre          37       175      79.2   29.2    30.8    0.88\n 5       5 DECR  pre          31       176      88     31.2    25.8    1.06\n 6       6 INCR  pre          33       168      79.6   34.2    35.3    1.27\n 7       7 MIX   pre          42       180      77.6   30.1    33.0    0.85\n 8       9 MIX   pre          41       185      82.4   22.7    22.4    1.48\n 9      10 INCR  pre          35       187      75.6   29.7    31.1    0.93\n10      11 MIX   pre          34       168      55.8   33.2    30.8    0.62\n# ℹ 52 more rows\n# ℹ 92 more variables: lac.175 &lt;dbl&gt;, lac.225 &lt;dbl&gt;, lac.250 &lt;dbl&gt;,\n#   lac.275 &lt;dbl&gt;, lac.300 &lt;dbl&gt;, lac.325 &lt;dbl&gt;, lac.350 &lt;dbl&gt;, lac.375 &lt;dbl&gt;,\n#   VO2.125 &lt;dbl&gt;, VO2.175 &lt;dbl&gt;, VO2.225 &lt;dbl&gt;, VO2.250 &lt;dbl&gt;, VO2.275 &lt;dbl&gt;,\n#   VO2.300 &lt;dbl&gt;, VO2.325 &lt;dbl&gt;, VO2.350 &lt;dbl&gt;, VO2.375 &lt;dbl&gt;, VCO2.125 &lt;dbl&gt;,\n#   VCO2.175 &lt;dbl&gt;, VCO2.225 &lt;dbl&gt;, VCO2.250 &lt;dbl&gt;, VCO2.275 &lt;dbl&gt;,\n#   VCO2.300 &lt;dbl&gt;, VCO2.325 &lt;dbl&gt;, VCO2.350 &lt;dbl&gt;, VCO2.375 &lt;dbl&gt;, …\n\n\nnear lets you select values that are approximately equal to your specified value with some specified tolerance:\n\n#| echo: true\n\ncyclingstudy %&gt;%\n        filter( near(age, 40, tol = 2)) %&gt;%\n        print()\n\n# A tibble: 18 × 101\n   subject group timepoint   age height.T1 weight.T1 sj.max cmj.max lac.125\n     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1       3 INCR  pre          39       193      98.1   26.8    28.8    1.17\n 2       9 MIX   pre          41       185      82.4   22.7    22.4    1.48\n 3      13 DECR  pre          41       183      76     34.3    36      1.67\n 4      18 DECR  pre          41       186     105.    33.5    33.7    1.75\n 5      21 DECR  pre          40       175      82.3   35.2    32.6    0.68\n 6       3 INCR  meso1        39        NA     101.    26.0    26.5    0.72\n 7       9 MIX   meso1        41        NA      82.3   23.3    22.4    0.96\n 8      13 DECR  meso1        41        NA      76     34.4    33.8    1.25\n 9      18 DECR  meso1        41        NA     103     33.2    34.1    1.51\n10      21 DECR  meso1        40        NA      81.1   33.6    34.5    0.6 \n11       3 INCR  meso2        39        NA      99.2   25.4    26.8    0.5 \n12       9 MIX   meso2        41        NA      NA     NA      NA     NA   \n13      13 DECR  meso2        41        NA      NA     NA      NA     NA   \n14      18 DECR  meso2        41        NA     101.    32.1    33.6    0.5 \n15      21 DECR  meso2        40        NA      81.7   32.4    31.5    0.5 \n16       3 INCR  meso3        39        NA      99.2   27.1    27.0    0.77\n17       9 MIX   meso3        41        NA      78.7   25.0    25.6    1.25\n18      18 DECR  meso3        41        NA     101.    32.1    34.7    1.94\n# ℹ 92 more variables: lac.175 &lt;dbl&gt;, lac.225 &lt;dbl&gt;, lac.250 &lt;dbl&gt;,\n#   lac.275 &lt;dbl&gt;, lac.300 &lt;dbl&gt;, lac.325 &lt;dbl&gt;, lac.350 &lt;dbl&gt;, lac.375 &lt;dbl&gt;,\n#   VO2.125 &lt;dbl&gt;, VO2.175 &lt;dbl&gt;, VO2.225 &lt;dbl&gt;, VO2.250 &lt;dbl&gt;, VO2.275 &lt;dbl&gt;,\n#   VO2.300 &lt;dbl&gt;, VO2.325 &lt;dbl&gt;, VO2.350 &lt;dbl&gt;, VO2.375 &lt;dbl&gt;, VCO2.125 &lt;dbl&gt;,\n#   VCO2.175 &lt;dbl&gt;, VCO2.225 &lt;dbl&gt;, VCO2.250 &lt;dbl&gt;, VCO2.275 &lt;dbl&gt;,\n#   VCO2.300 &lt;dbl&gt;, VCO2.325 &lt;dbl&gt;, VCO2.350 &lt;dbl&gt;, VCO2.375 &lt;dbl&gt;,\n#   VE.125 &lt;dbl&gt;, VE.175 &lt;dbl&gt;, VE.225 &lt;dbl&gt;, VE.250 &lt;dbl&gt;, VE.275 &lt;dbl&gt;, …\n\n\nNotice however that the default tolerance is a very small number (1.4901161^{-8}).\n\n\n\n\n\n\nExercises (3)\n\n\n\n\nKeep rows in your data frame from the pre time-point, age greater than 31 but height less than 180.\nUse between to find rows with VO2.max values between 4800 and 5200 (see ?between)\nUse near to find weight.T1 values close to 80.26 with a tolerance of 0.75 (see ?near)\nRemove all rows that are NA in the height.T1 variable.\n\n\n\n\n\nSorting a data frame does not change the observations, it only determines how the data frame is printed or passed to other functions.\nIf we want to sort our data in cyclingstudy based on VO2.max we would\n\ncyclingstudy %&gt;%\n1        select(subject, group, VO2.max) %&gt;%\n2        arrange(VO2.max)\n\n\ncyclingstudy %&gt;%\n        select(subject, group, VO2.max) %&gt;% \n3        arrange(desc(VO2.max))\n\n\ncyclingstudy %&gt;%\n        select(subject, group, VO2.max) %&gt;% \n        arrange(desc(group) )\n\n\n1\n\nSelects a subset of variables for display\n\n2\n\nArrange based on the variable VO2.max, smallest values on top\n\n3\n\nArrange with largest value on top\n\n\n\n\nNotice the use of the desc function. In this example it is equivalent to using arrange(-VO2.max), in other situations, like with factors, desc will work as expected but not -.\n\n\n\n\n\nA super power of dplyr is its ability to group and summarize data. The group_by function creates a grouped data frame suitable for summaries per group. In the cyclingstudy data set we have three groups that we might want to describe using some summary function.\nExamples of summary functions in R are:\n\nmean() → computes arithmetic mean\nmedian() → computes the median\nsd() → computes the standard deviation from the mean\nIQR() → returns the inter-quartile range\nmin() and max() → gives the minimum and maximum values from a vector\nquantile() → sample quantiles from the smallest (probs = 0) to largest (probs = 1) values.\n\nAll the above functions comes with the optional argument of na.rm = TRUE. This can be read as remove missing values (NA). If there are missing values (NA) and na.rm = FALSE (the default), the calculations will return NA. This is inconvenient but can often work as check of your assumptions.\n\n\n\n\n\n\nExercises (4)\n\n\n\n\nWhat do we mean by check of your assumptions?\nWhat do you expect from the R code sd(c(4, 5, 7, NA, 5))\nWhat would you add to the code above to improve it?\n\n\n\nIn addition to the summaries above that are generic for base R, dplyr provides you with a number of great functions to…\n\nn() → count the number of observations in each group\nn_distinct() → return the number of unique values from a vector for each group\n\nIn practice a grouped summary may look like this:\n\ncyclingstudy %&gt;%\n        group_by(group) %&gt;%\n        summarise(mean.vo2max = mean(VO2.max, na.rm = TRUE))\n\nResults from the above code includes multiple data points from each participant. The variable describing time-points can be added to the grouping.\n\ncyclingstudy %&gt;%\n        group_by(group, timepoint) %&gt;%\n        summarise(mean.vo2max = mean(VO2.max, na.rm = TRUE))\n\nMultiple summary functions can be added to the summarise() function where each adds a new variable to the result data frame.\n\ncyclingstudy %&gt;%\n        group_by(group, timepoint) %&gt;%\n        summarise(mean.vo2max = mean(VO2.max, na.rm = TRUE), \n                  sd.vo2max = sd(VO2.max, na.rm = TRUE))\n\n\n\n\n\n\n\nExercises (5)\n\n\n\n\nUse the cyc.bmi data frame your created above, filter to only keep timepoint == \"pre\", and from the BMI variable calculate:\n\nmedian\nminimum\nmaximum\n25th percentile (q25 in the example below)\n75th percentile (q75 in the example below)\n\nSave the new data set as cyc.bmi.summary and plot the data using the code below:\nCompare the plot with figure created with geom_boxplot(), see ?geom_boxplot for the difference between plots as described under Summary statistics.\n\n\ncyc.bmi.summary %&gt;%\n        ggplot(aes(group, median)) + geom_point( size = 4) + \n        geom_errorbar(aes(ymin = q25, ymax = q75), width = 0, size = 2) +\n        geom_errorbar(aes(ymin = min, ymax = max), width = 0, size = 1)\n\n\ncyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        select(subject, group, weight = weight.T1, height = height.T1) %&gt;%\n        mutate(bmi = weight / ((height/100)^2)) %&gt;%\n        ggplot(aes(group, bmi)) + geom_boxplot()\n\n\n\n\n\n\nData can be described as being in long form and wide form. The long form data is tidy in the sense that all columns are distinct variables.\n\nThere are examples of wide data sets as part of the cycling data set. Using only the timepoint == pre values and columns corresponding to lactate values from the graded exercise test we have an example of wide data as the columns lac.125, lac.175, lac.225, etc., contains lactate values from different exercise intensities. This means that a separate variable (watt or exercise intensity) is combined in each column of lactate values.\n\ncyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        dplyr::select(subject, group, lac.125:lac.375) %&gt;%\n        print()\n\n# A tibble: 20 × 11\n   subject group lac.125 lac.175 lac.225 lac.250 lac.275 lac.300 lac.325 lac.350\n     &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1       1 INCR     1.5     1.86    2.38    3.54    6.21   NA      NA      NA   \n 2       2 DECR     1.19    1.49    2.34    3.21    5.33   NA      NA      NA   \n 3       3 INCR     1.17    1.52    1.22    1.54    2.04    3.32    4.72   NA   \n 4       4 DECR     0.88    0.99    2.13    3.25   NA       6.15   NA      NA   \n 5       5 DECR     1.06    1.41    1.9     2.04    3.04    3.59    4.73   NA   \n 6       6 INCR     1.27    1.73    3.21    4.83   NA      NA      NA      NA   \n 7       7 MIX      0.85    0.84    1.16    1.71    3.33    6.25   NA      NA   \n 8       8 MIX      0.93    1.34    1.94   NA       3.71    7.29   NA      NA   \n 9       9 MIX      1.48    1.17    1.95   NA       3.24    6.21   NA      NA   \n10      10 INCR     0.93    0.87    0.86    0.92    1.2     1.69    2.6     4.69\n11      11 MIX      0.62    1.22    2.85    5.86    9.87   NA      NA      NA   \n12      13 DECR     1.67    1.81    2.78    4.25    6.87   NA      NA      NA   \n13      14 MIX      1.13    1.33    2.74    3.97   NA      NA      NA      NA   \n14      15 INCR     0.8     1.12    1.43   NA       2.4     3.77    6.3    NA   \n15      16 INCR     0.94    1.18    1.89    2.83    5.33   NA      NA      NA   \n16      17 MIX      1.54    1.62    2.7     4.1    NA      NA      NA      NA   \n17      18 DECR     1.75    2.08    2.99    4.22   NA      NA      NA      NA   \n18      19 DECR     1.23    2.51    4.65   NA      NA      NA      NA      NA   \n19      20 INCR     2.26    2.05    3.19    5.17   NA      NA      NA      NA   \n20      21 DECR     0.68    0.89    1.98    3.18    5.57   NA      NA      NA   \n# ℹ 1 more variable: lac.375 &lt;dbl&gt;\n\n\nUsing pivot_longer we can change this data into a long format. Pivot wider needs information on the new variable names for values and names. Names are the column names that will form a variable and values are the values contained in the cells of the old variables. We also need to specify what columns to make longer, notice that I select variables &lt;from&gt;:&lt;to&gt;.\n\ncyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        dplyr::select(subject, group, lac.125:lac.375) %&gt;%\n        pivot_longer(names_to = \"watt\", \n                     values_to = \"lactate\", \n                     cols = lac.125:lac.375) %&gt;%\n        print()\n\n# A tibble: 180 × 4\n   subject group watt    lactate\n     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;\n 1       1 INCR  lac.125    1.5 \n 2       1 INCR  lac.175    1.86\n 3       1 INCR  lac.225    2.38\n 4       1 INCR  lac.250    3.54\n 5       1 INCR  lac.275    6.21\n 6       1 INCR  lac.300   NA   \n 7       1 INCR  lac.325   NA   \n 8       1 INCR  lac.350   NA   \n 9       1 INCR  lac.375   NA   \n10       2 DECR  lac.125    1.19\n# ℹ 170 more rows\n\n\npivot_wider makes it easy to remove prefix and fix the data type of the names variable. Below we specify to remove lac. from all names and convert the new variable to numeric data.\n\n# eval: false\n\n\ncyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        dplyr::select(subject, group, lac.125:lac.375) %&gt;%\n        pivot_longer(names_to = \"watt\", \n                     values_to = \"lactate\", \n                     cols = lac.125:lac.375, \n                     names_prefix = \"lac.\", \n                     names_transform = list(watt = as.numeric)) %&gt;%\n        print()\n\n# A tibble: 180 × 4\n   subject group  watt lactate\n     &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1       1 INCR    125    1.5 \n 2       1 INCR    175    1.86\n 3       1 INCR    225    2.38\n 4       1 INCR    250    3.54\n 5       1 INCR    275    6.21\n 6       1 INCR    300   NA   \n 7       1 INCR    325   NA   \n 8       1 INCR    350   NA   \n 9       1 INCR    375   NA   \n10       2 DECR    125    1.19\n# ℹ 170 more rows\n\n\nA data set can contain multiple “groups” of variables in a wide format. In the cycling data set variables such as HF.* and lac.* are variables that are collected at different intensities (watt, indicated with *). We would like to make the data set tidy, each column contains a variable and each row is an observation.\nIn the example below we create a long data set using two variables of interest, lactate (lac.*) and heart rate (HF.*) each collected over multiple intensities. Using a special name .value we can tell pivot_longer that there are additional information in the column names that should be regarded as names. We also need to a separator that separates our .value with the watt column, in this case the separator is a period/point (.). Note that R evaluates a single . as special character matching any single character. To use the period character as a separator we must “escape” it using either \\\\. or [.], this removes its special meaning.\n\n# eval: false\n\ncyclingstudy %&gt;%\n1        filter(timepoint == \"pre\") %&gt;%\n        select(subject, group, lac.125:lac.375, HF.125:HF.375) %&gt;%\n2        pivot_longer(cols = !c(subject, group),\n3                     names_to = c(\".value\", \"watt\"),\n4                     names_sep = \"[.]\") %&gt;%\n        print()\n\n\n1\n\nFilter the data set an select variables\n\n2\n\nUsing pivot_wider, first specify what columns to “make longer”.\n\n3\n\nUse the special .value to indicate that each variable name contains information that should be used in creating new variables\n\n4\n\nIndicate what character to split the variables on, in this case we need to “escape” the period using [.].\n\n\n\n\n# A tibble: 180 × 5\n   subject group watt    lac    HF\n     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1       1 INCR  125    1.5    109\n 2       1 INCR  175    1.86   124\n 3       1 INCR  225    2.38   139\n 4       1 INCR  250    3.54   152\n 5       1 INCR  275    6.21   167\n 6       1 INCR  300   NA       NA\n 7       1 INCR  325   NA       NA\n 8       1 INCR  350   NA       NA\n 9       1 INCR  375   NA       NA\n10       2 DECR  125    1.19   121\n# ℹ 170 more rows\n\n\n\n\n\n\n\n\nExercises (6)\n\n\n\n\nPivot the following data set, making separate columns/variables for each NAME\n\n\ndata.frame(id = c(\"id1\", \"id1\", \"id1\", \"id2\", \"id2\", \"id2\"), \n           NAME = c(\"a\", \"b\", \"c\", \"a\", \"b\", \"c\"), \n           NUMBER = c(4, 6, 7, 2, 3, 5)) %&gt;%\n\n        print()\n\n\nUsing the cyclingstudy data set, make a ggplot that shows\n\nVCO2 and VO2 values at sub-maximal intensities (watt 125-375) for participant 15.\nEach time-point (pre, meso1, meso2 and meso3) should be its own facet/panel\nVCO2 and VO2 should be of different colors indicated with points and lines\nLabels and units should be fixed (see figure below for details, look up the ggtext package for examples on how to format text in labels and titles).\n\n\n\n\n\n\n\n\n\n\n\n\nTable 1 in experimental or observational studies often contains descriptive data on the sample. These tables may help readers to understand to what group/population a study may be generalized to and how key characteristics are distributed among experimental groups.\n\n\nWe will prepare data from cyclingstudy to create a Table 1 with descriptive data - Select a set of key variables that you want to describe (center, spread and/or range) from baseline measurements (timepoint == \"pre\"), for example age, weight, height and BMI. - Group the data set on group and perform summary calculations\n\n\n\nTo display numbers with the correct number of decimals R provides many options. A simple function (round()) provides rounding. The problem is that you will lose trailing zero, e.g., 2.0 will be displayed as 2. To keep the trailing zero we must use the sprintf() function. Examples:\n\n\n# Rounding\nround(2.10, 2) \n\n[1] 2.1\n\n# Formatting to keep the trailing zero\nsprintf(\"%.2f\", 2.10)\n\n[1] \"2.10\"\n\n\n\nCombining vectors may be a good idea to make the table more attractive. The mean and standard deviation is commonly presented as mean (SD). Data from a column of means and a column of SD’s can be combined to create a nice display using the paste0() function. Example:\n\n\ndata.frame(m = c(46.7, 47.89, 43.5),  # A vector of means\n           s = c(4.21, 4.666, 3.1)) %&gt;% # A vector of SD's\n        mutate(stat = paste0(round(m, 1), \n                             \" (\",\n                             round(s, 1), \n                             \")\")) %&gt;%\n        print()\n\n      m     s       stat\n1 46.70 4.210 46.7 (4.2)\n2 47.89 4.666 47.9 (4.7)\n3 43.50 3.100 43.5 (3.1)\n\n\n\nA character vector of group names can be arranged and re-named using the factor function. In the cyclingstudy data set the groups (INCR, DECR and MIX) may be given more descriptive names, example:\n\n\ncyclingstudy %&gt;%\n        mutate(group = factor(group, levels = c(\"INCR\", \"DECR\", \"MIX\"), \n                              labels = c(\"Increased intensity\", \n                                         \"Decreased intensity\", \n                                         \"Mixed intensity\"))) %&gt;%\n        distinct(group)\n\n# A tibble: 3 × 1\n  group              \n  &lt;fct&gt;              \n1 Increased intensity\n2 Decreased intensity\n3 Mixed intensity    \n\n\n\n\n\n\n\nWe will use a table generator to create the table. See next part of this workshop here"
  },
  {
    "objectID": "ws4-data-wrangling-tables.html#dplyr-verbs-of-data-manipulation-and-pipes",
    "href": "ws4-data-wrangling-tables.html#dplyr-verbs-of-data-manipulation-and-pipes",
    "title": "Data wrangling and tables",
    "section": "",
    "text": "dplyr contains verbs used for data manipulation, such as filtering rows, selecting variables and changing or creating variables. Verbs can be used in a pipe and read as sequential operations:\n\nTake the data then do\n\n\nfilter based on group then do\n\n\ncreate a new variable then do\n\n\nshow the data in the console.\n\n\n\n\nA pipe\n\n\nThe pipe operations are made possible by another package, magrittr. This package contains the forward pipe operator %&gt;%. The forward pipe operator (%&gt;%) can be read as “then do”. The operator takes the object on the left-hand side and puts it as the first argument in the following function.\nTranslating the “pipe” above from human language to R language using the “tidyverse dialect” looks like this:\n\ndata %&gt;%\n        filter(group == \"xx\") %&gt;%\n        mutate(new.var = old.var + old.var2) %&gt;%\n        print()"
  },
  {
    "objectID": "ws4-data-wrangling-tables.html#getting-started-load-data-and-packages",
    "href": "ws4-data-wrangling-tables.html#getting-started-load-data-and-packages",
    "title": "Data wrangling and tables",
    "section": "",
    "text": "We will use the cyclingstudy data from exscidata in our exercises. Load the required components to your session.\n\n\nShow the code\nlibrary(tidyverse) # loads dplyr etc.\nlibrary(exscidata) # loads the data-package\n\ndata(\"cyclingstudy\")"
  },
  {
    "objectID": "ws4-data-wrangling-tables.html#adding-changing-and-selecting-variables---mutate-and-select",
    "href": "ws4-data-wrangling-tables.html#adding-changing-and-selecting-variables---mutate-and-select",
    "title": "Data wrangling and tables",
    "section": "",
    "text": "Mutate can help you create new variables (or overwrite existing once). In the cycling data set there is a variable called VO2.max, this variable is expressed in absolute units (ml min-1), however, we might want to express it as relative units (ml kg-1 min-1).\n\ncyclingstudy %&gt;%\n        mutate(rel.vo2max = VO2.max / weight.T1) %&gt;%\n        print()\n\nThe mutate function creates new variables (or overwrite existing) in a flexible way. Here we simply use division. Other mathematical operators can similarly be used (+, -, *, etc.).\nNotice the print() function in the end of the pipe. This is used to display the results of any manipulations done in the pipe. Notice also that our new variable is not listed. We might need to select a sub-set of variables to get a better overview. We will do this using the select function.\n\ncyclingstudy %&gt;%\n        mutate(rel.vo2max = VO2.max / weight.T1) %&gt;%\n        select(subject, group, timepoint, age, height.T1, weight.T1, VO2.max, rel.vo2max) %&gt;%\n        print()\n\nThe select function takes variable names as “unquoted” names. We can also select a range of columns using the syntax &lt;from&gt;:&lt;to&gt; where &lt;from&gt; is the first column you would like to select and &lt;to&gt; would be the last. Subsequently the above pipe can be re-written as\n\ncyclingstudy %&gt;%\n        mutate(rel.vo2max = VO2.max / weight.T1) %&gt;%\n        select(subject:weight.T1, VO2.max, rel.vo2max) %&gt;%\n        print()\n\nselect can also be used to re-name variables. Simpler variable names for weight, height and relative VO2max could be weight, height, vo2max.kg.\n\ncyclingstudy %&gt;%\n        mutate(rel.vo2max = VO2.max / weight.T1) %&gt;%\n        select(subject:age, height = height.T1, weight = weight.T1, VO2.max,vo2max.kg = rel.vo2max) %&gt;%\n        print()\n\n\n\nWhen you have selected and mutated variables you might want to keep the new data frame in your environment. Do this by assigning the whole pipe to a new object.\n\ncyc.subset &lt;- cyclingstudy %&gt;%\n        mutate(rel.vo2max = VO2.max / weight.T1) %&gt;%\n        select(subject:age, height = height.T1, weight = weight.T1, VO2.max,vo2max.kg = rel.vo2max) %&gt;%\n        print()\n\n\n\n\n\n\n\nExercises (1)\n\n\n\n\nSelect the variables subject, group, timepoint, age, height.T1, weight.T1. Rename the variables weight.T1 and height.T1 to weight and height.\nCalculate body mass index (BMI) for all observations using the formula \\(\\text{BMI} = \\frac{\\text{weight (kg)}}{\\text{height (m)}^2}\\)\nStore the data frame as cyc.bmi in your environment."
  },
  {
    "objectID": "ws4-data-wrangling-tables.html#select-rows-based-on-condition-and-sort-them---filter-and-arrange",
    "href": "ws4-data-wrangling-tables.html#select-rows-based-on-condition-and-sort-them---filter-and-arrange",
    "title": "Data wrangling and tables",
    "section": "",
    "text": "Filtering is used to select specific observations (rows) of a data set. We filter based on specific conditions such as:\n\nAll values bigger than X\nAll values less than Y\nAll observations than contain A, B or C in variable V.\n\nThe above examples must be translated to formal expressions.\n\n\nAn expression that make comparisons can be\n\nx &lt; y → x less than y\nx &gt; y → x greater than y\nx &lt;= y → x less or equal to y\nx &gt;= y → x greater or equal to y\nx == y → x exactly equal to y\nx != y → x not exactly equal to y\n\nIn the filter function these expressions give either TRUE or FALSE. If TRUE the rows are included in the filtered data frame.\nWe can see the mechanism behind filtering by creating a vector of TRUE and FALSE based on an expression. Let’s say that we want to see which rows has weight.T1 greater than 75.\n\ncyclingstudy$weight.T1 &gt; 75\n\n [1]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n[13]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE\n[25]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE  TRUE\n[37]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE    NA  TRUE  TRUE FALSE\n[49]    NA  TRUE FALSE    NA  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[61]  TRUE FALSE  TRUE  TRUE    NA  TRUE  TRUE FALSE  TRUE FALSE FALSE FALSE\n[73]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n\n\nWe can see that the first row returns a TRUE while the second row returns FALSE.\nUsing the filter function, we just add the expression as an argument in the function and all the rows that comes up TRUE will remain.\n\ncyclingstudy %&gt;%\n        filter(weight.T1 &gt; 75) %&gt;%\n        print()\n\n# A tibble: 60 × 101\n   subject group timepoint   age height.T1 weight.T1 sj.max cmj.max lac.125\n     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1       1 INCR  pre          33       183      80.3   31.0    35.0    1.5 \n 2       3 INCR  pre          39       193      98.1   26.8    28.8    1.17\n 3       4 DECR  pre          37       175      79.2   29.2    30.8    0.88\n 4       5 DECR  pre          31       176      88     31.2    25.8    1.06\n 5       6 INCR  pre          33       168      79.6   34.2    35.3    1.27\n 6       7 MIX   pre          42       180      77.6   30.1    33.0    0.85\n 7       8 MIX   pre          26       179      75.5   32.8    33.2    0.93\n 8       9 MIX   pre          41       185      82.4   22.7    22.4    1.48\n 9      10 INCR  pre          35       187      75.6   29.7    31.1    0.93\n10      13 DECR  pre          41       183      76     34.3    36      1.67\n# ℹ 50 more rows\n# ℹ 92 more variables: lac.175 &lt;dbl&gt;, lac.225 &lt;dbl&gt;, lac.250 &lt;dbl&gt;,\n#   lac.275 &lt;dbl&gt;, lac.300 &lt;dbl&gt;, lac.325 &lt;dbl&gt;, lac.350 &lt;dbl&gt;, lac.375 &lt;dbl&gt;,\n#   VO2.125 &lt;dbl&gt;, VO2.175 &lt;dbl&gt;, VO2.225 &lt;dbl&gt;, VO2.250 &lt;dbl&gt;, VO2.275 &lt;dbl&gt;,\n#   VO2.300 &lt;dbl&gt;, VO2.325 &lt;dbl&gt;, VO2.350 &lt;dbl&gt;, VO2.375 &lt;dbl&gt;, VCO2.125 &lt;dbl&gt;,\n#   VCO2.175 &lt;dbl&gt;, VCO2.225 &lt;dbl&gt;, VCO2.250 &lt;dbl&gt;, VCO2.275 &lt;dbl&gt;,\n#   VCO2.300 &lt;dbl&gt;, VCO2.325 &lt;dbl&gt;, VCO2.350 &lt;dbl&gt;, VCO2.375 &lt;dbl&gt;, …\n\n\n\n\n\n\n\n\nExercises (2)\n\n\n\nHow many rows in the cyclingstudy data set has\n\nVO2.max values greater than 6000\nVO2.max values less than 6000\nVO2.max values less or equal to than 5360\nVO2.max values greater or equal to than 5360\nthe value pre in timepoint\na value in timepoint other than pre\n\n\n\n\n\n\nLogical operators similarly creates TRUE or FALSE as the basis of filtering operations. These can be used in combination with comparisons.\n\n! x → NOT x\nx & y → x and y\nx | y → x or y\nis.na(x) → returns TRUE if x is NA\n\nWe might want to keep all rows with weight.T1 greater than 80 that are also from the group INCR. This can be solved with an AND operator (&).\n\n#| echo: true\n\ncyclingstudy %&gt;%\n        filter(weight.T1 &gt; 80 & group == \"INCR\") %&gt;%\n        print()\n\n# A tibble: 9 × 101\n  subject group timepoint   age height.T1 weight.T1 sj.max cmj.max lac.125\n    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1       1 INCR  pre          33       183      80.3   31.0    35.0    1.5 \n2       3 INCR  pre          39       193      98.1   26.8    28.8    1.17\n3      20 INCR  pre          42       180      82.3   32.5    32.2    2.26\n4       3 INCR  meso1        39        NA     101.    26.0    26.5    0.72\n5      20 INCR  meso1        42        NA      82.5   30.2    30.6    0.96\n6       3 INCR  meso2        39        NA      99.2   25.4    26.8    0.5 \n7      20 INCR  meso2        42        NA      81.1   29.6    29.8    1.53\n8       3 INCR  meso3        39        NA      99.2   27.1    27.0    0.77\n9      20 INCR  meso3        43        NA      81.5   30.0    30.9    1.77\n# ℹ 92 more variables: lac.175 &lt;dbl&gt;, lac.225 &lt;dbl&gt;, lac.250 &lt;dbl&gt;,\n#   lac.275 &lt;dbl&gt;, lac.300 &lt;dbl&gt;, lac.325 &lt;dbl&gt;, lac.350 &lt;dbl&gt;, lac.375 &lt;dbl&gt;,\n#   VO2.125 &lt;dbl&gt;, VO2.175 &lt;dbl&gt;, VO2.225 &lt;dbl&gt;, VO2.250 &lt;dbl&gt;, VO2.275 &lt;dbl&gt;,\n#   VO2.300 &lt;dbl&gt;, VO2.325 &lt;dbl&gt;, VO2.350 &lt;dbl&gt;, VO2.375 &lt;dbl&gt;, VCO2.125 &lt;dbl&gt;,\n#   VCO2.175 &lt;dbl&gt;, VCO2.225 &lt;dbl&gt;, VCO2.250 &lt;dbl&gt;, VCO2.275 &lt;dbl&gt;,\n#   VCO2.300 &lt;dbl&gt;, VCO2.325 &lt;dbl&gt;, VCO2.350 &lt;dbl&gt;, VCO2.375 &lt;dbl&gt;,\n#   VE.125 &lt;dbl&gt;, VE.175 &lt;dbl&gt;, VE.225 &lt;dbl&gt;, VE.250 &lt;dbl&gt;, VE.275 &lt;dbl&gt;, …\n\n\nWe can similarly use OR (|) to select either weight greater than 80 or group INCR.\n\n#| echo: true\n\ncyclingstudy %&gt;%\n        filter(weight.T1 &gt; 80 | group == \"INCR\") %&gt;%\n        print()\n\n# A tibble: 46 × 101\n   subject group timepoint   age height.T1 weight.T1 sj.max cmj.max lac.125\n     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1       1 INCR  pre          33       183      80.3   31.0    35.0    1.5 \n 2       3 INCR  pre          39       193      98.1   26.8    28.8    1.17\n 3       5 DECR  pre          31       176      88     31.2    25.8    1.06\n 4       6 INCR  pre          33       168      79.6   34.2    35.3    1.27\n 5       9 MIX   pre          41       185      82.4   22.7    22.4    1.48\n 6      10 INCR  pre          35       187      75.6   29.7    31.1    0.93\n 7      14 MIX   pre          35       183      81.3   27.6    30.0    1.13\n 8      15 INCR  pre          34       178      75.1   33.5    32.4    0.8 \n 9      16 INCR  pre          27       178      77.8   32.9    33.7    0.94\n10      18 DECR  pre          41       186     105.    33.5    33.7    1.75\n# ℹ 36 more rows\n# ℹ 92 more variables: lac.175 &lt;dbl&gt;, lac.225 &lt;dbl&gt;, lac.250 &lt;dbl&gt;,\n#   lac.275 &lt;dbl&gt;, lac.300 &lt;dbl&gt;, lac.325 &lt;dbl&gt;, lac.350 &lt;dbl&gt;, lac.375 &lt;dbl&gt;,\n#   VO2.125 &lt;dbl&gt;, VO2.175 &lt;dbl&gt;, VO2.225 &lt;dbl&gt;, VO2.250 &lt;dbl&gt;, VO2.275 &lt;dbl&gt;,\n#   VO2.300 &lt;dbl&gt;, VO2.325 &lt;dbl&gt;, VO2.350 &lt;dbl&gt;, VO2.375 &lt;dbl&gt;, VCO2.125 &lt;dbl&gt;,\n#   VCO2.175 &lt;dbl&gt;, VCO2.225 &lt;dbl&gt;, VCO2.250 &lt;dbl&gt;, VCO2.275 &lt;dbl&gt;,\n#   VCO2.300 &lt;dbl&gt;, VCO2.325 &lt;dbl&gt;, VCO2.350 &lt;dbl&gt;, VCO2.375 &lt;dbl&gt;, …\n\n\nNotice that there are rows containing weights less than 80 from the INCR group.\nAny logical statement can also be negated with ! indication NOT. This means we will get a vector of TRUE for any expression previously being FALSE. Notice the extra parentheses below.\n\n#| echo: true\n\ncyclingstudy %&gt;%\n        filter(!(weight.T1 &gt; 80 & group == \"INCR\")) %&gt;%\n        print()\n\n# A tibble: 71 × 101\n   subject group timepoint   age height.T1 weight.T1 sj.max cmj.max lac.125\n     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1       2 DECR  pre          32       174      71.4   31.6    33.8    1.19\n 2       4 DECR  pre          37       175      79.2   29.2    30.8    0.88\n 3       5 DECR  pre          31       176      88     31.2    25.8    1.06\n 4       6 INCR  pre          33       168      79.6   34.2    35.3    1.27\n 5       7 MIX   pre          42       180      77.6   30.1    33.0    0.85\n 6       8 MIX   pre          26       179      75.5   32.8    33.2    0.93\n 7       9 MIX   pre          41       185      82.4   22.7    22.4    1.48\n 8      10 INCR  pre          35       187      75.6   29.7    31.1    0.93\n 9      11 MIX   pre          34       168      55.8   33.2    30.8    0.62\n10      13 DECR  pre          41       183      76     34.3    36      1.67\n# ℹ 61 more rows\n# ℹ 92 more variables: lac.175 &lt;dbl&gt;, lac.225 &lt;dbl&gt;, lac.250 &lt;dbl&gt;,\n#   lac.275 &lt;dbl&gt;, lac.300 &lt;dbl&gt;, lac.325 &lt;dbl&gt;, lac.350 &lt;dbl&gt;, lac.375 &lt;dbl&gt;,\n#   VO2.125 &lt;dbl&gt;, VO2.175 &lt;dbl&gt;, VO2.225 &lt;dbl&gt;, VO2.250 &lt;dbl&gt;, VO2.275 &lt;dbl&gt;,\n#   VO2.300 &lt;dbl&gt;, VO2.325 &lt;dbl&gt;, VO2.350 &lt;dbl&gt;, VO2.375 &lt;dbl&gt;, VCO2.125 &lt;dbl&gt;,\n#   VCO2.175 &lt;dbl&gt;, VCO2.225 &lt;dbl&gt;, VCO2.250 &lt;dbl&gt;, VCO2.275 &lt;dbl&gt;,\n#   VCO2.300 &lt;dbl&gt;, VCO2.325 &lt;dbl&gt;, VCO2.350 &lt;dbl&gt;, VCO2.375 &lt;dbl&gt;, …\n\n\nThe dplyr function filter also accepts multiple arguments separated with a comma. This is equal to adding conditions with the AND operator. Example:\n\n#| echo: true\n\ncyclingstudy %&gt;%\n        filter(weight.T1 &gt; 80, \n               group == \"INCR\", \n               timepoint == \"pre\", \n               age &gt; 35) %&gt;%\n        print()\n\n# A tibble: 2 × 101\n  subject group timepoint   age height.T1 weight.T1 sj.max cmj.max lac.125\n    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1       3 INCR  pre          39       193      98.1   26.8    28.8    1.17\n2      20 INCR  pre          42       180      82.3   32.5    32.2    2.26\n# ℹ 92 more variables: lac.175 &lt;dbl&gt;, lac.225 &lt;dbl&gt;, lac.250 &lt;dbl&gt;,\n#   lac.275 &lt;dbl&gt;, lac.300 &lt;dbl&gt;, lac.325 &lt;dbl&gt;, lac.350 &lt;dbl&gt;, lac.375 &lt;dbl&gt;,\n#   VO2.125 &lt;dbl&gt;, VO2.175 &lt;dbl&gt;, VO2.225 &lt;dbl&gt;, VO2.250 &lt;dbl&gt;, VO2.275 &lt;dbl&gt;,\n#   VO2.300 &lt;dbl&gt;, VO2.325 &lt;dbl&gt;, VO2.350 &lt;dbl&gt;, VO2.375 &lt;dbl&gt;, VCO2.125 &lt;dbl&gt;,\n#   VCO2.175 &lt;dbl&gt;, VCO2.225 &lt;dbl&gt;, VCO2.250 &lt;dbl&gt;, VCO2.275 &lt;dbl&gt;,\n#   VCO2.300 &lt;dbl&gt;, VCO2.325 &lt;dbl&gt;, VCO2.350 &lt;dbl&gt;, VCO2.375 &lt;dbl&gt;,\n#   VE.125 &lt;dbl&gt;, VE.175 &lt;dbl&gt;, VE.225 &lt;dbl&gt;, VE.250 &lt;dbl&gt;, VE.275 &lt;dbl&gt;, …\n\n\nFinally, dplyr comes with two convenient functions to find values between and near.\nbetween() lets you select observations between two values:\n\n#| echo: true\n\ncyclingstudy %&gt;%\n        filter( between(age, 31, 42)) %&gt;%\n        print()\n\n# A tibble: 62 × 101\n   subject group timepoint   age height.T1 weight.T1 sj.max cmj.max lac.125\n     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1       1 INCR  pre          33       183      80.3   31.0    35.0    1.5 \n 2       2 DECR  pre          32       174      71.4   31.6    33.8    1.19\n 3       3 INCR  pre          39       193      98.1   26.8    28.8    1.17\n 4       4 DECR  pre          37       175      79.2   29.2    30.8    0.88\n 5       5 DECR  pre          31       176      88     31.2    25.8    1.06\n 6       6 INCR  pre          33       168      79.6   34.2    35.3    1.27\n 7       7 MIX   pre          42       180      77.6   30.1    33.0    0.85\n 8       9 MIX   pre          41       185      82.4   22.7    22.4    1.48\n 9      10 INCR  pre          35       187      75.6   29.7    31.1    0.93\n10      11 MIX   pre          34       168      55.8   33.2    30.8    0.62\n# ℹ 52 more rows\n# ℹ 92 more variables: lac.175 &lt;dbl&gt;, lac.225 &lt;dbl&gt;, lac.250 &lt;dbl&gt;,\n#   lac.275 &lt;dbl&gt;, lac.300 &lt;dbl&gt;, lac.325 &lt;dbl&gt;, lac.350 &lt;dbl&gt;, lac.375 &lt;dbl&gt;,\n#   VO2.125 &lt;dbl&gt;, VO2.175 &lt;dbl&gt;, VO2.225 &lt;dbl&gt;, VO2.250 &lt;dbl&gt;, VO2.275 &lt;dbl&gt;,\n#   VO2.300 &lt;dbl&gt;, VO2.325 &lt;dbl&gt;, VO2.350 &lt;dbl&gt;, VO2.375 &lt;dbl&gt;, VCO2.125 &lt;dbl&gt;,\n#   VCO2.175 &lt;dbl&gt;, VCO2.225 &lt;dbl&gt;, VCO2.250 &lt;dbl&gt;, VCO2.275 &lt;dbl&gt;,\n#   VCO2.300 &lt;dbl&gt;, VCO2.325 &lt;dbl&gt;, VCO2.350 &lt;dbl&gt;, VCO2.375 &lt;dbl&gt;, …\n\n\nnear lets you select values that are approximately equal to your specified value with some specified tolerance:\n\n#| echo: true\n\ncyclingstudy %&gt;%\n        filter( near(age, 40, tol = 2)) %&gt;%\n        print()\n\n# A tibble: 18 × 101\n   subject group timepoint   age height.T1 weight.T1 sj.max cmj.max lac.125\n     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1       3 INCR  pre          39       193      98.1   26.8    28.8    1.17\n 2       9 MIX   pre          41       185      82.4   22.7    22.4    1.48\n 3      13 DECR  pre          41       183      76     34.3    36      1.67\n 4      18 DECR  pre          41       186     105.    33.5    33.7    1.75\n 5      21 DECR  pre          40       175      82.3   35.2    32.6    0.68\n 6       3 INCR  meso1        39        NA     101.    26.0    26.5    0.72\n 7       9 MIX   meso1        41        NA      82.3   23.3    22.4    0.96\n 8      13 DECR  meso1        41        NA      76     34.4    33.8    1.25\n 9      18 DECR  meso1        41        NA     103     33.2    34.1    1.51\n10      21 DECR  meso1        40        NA      81.1   33.6    34.5    0.6 \n11       3 INCR  meso2        39        NA      99.2   25.4    26.8    0.5 \n12       9 MIX   meso2        41        NA      NA     NA      NA     NA   \n13      13 DECR  meso2        41        NA      NA     NA      NA     NA   \n14      18 DECR  meso2        41        NA     101.    32.1    33.6    0.5 \n15      21 DECR  meso2        40        NA      81.7   32.4    31.5    0.5 \n16       3 INCR  meso3        39        NA      99.2   27.1    27.0    0.77\n17       9 MIX   meso3        41        NA      78.7   25.0    25.6    1.25\n18      18 DECR  meso3        41        NA     101.    32.1    34.7    1.94\n# ℹ 92 more variables: lac.175 &lt;dbl&gt;, lac.225 &lt;dbl&gt;, lac.250 &lt;dbl&gt;,\n#   lac.275 &lt;dbl&gt;, lac.300 &lt;dbl&gt;, lac.325 &lt;dbl&gt;, lac.350 &lt;dbl&gt;, lac.375 &lt;dbl&gt;,\n#   VO2.125 &lt;dbl&gt;, VO2.175 &lt;dbl&gt;, VO2.225 &lt;dbl&gt;, VO2.250 &lt;dbl&gt;, VO2.275 &lt;dbl&gt;,\n#   VO2.300 &lt;dbl&gt;, VO2.325 &lt;dbl&gt;, VO2.350 &lt;dbl&gt;, VO2.375 &lt;dbl&gt;, VCO2.125 &lt;dbl&gt;,\n#   VCO2.175 &lt;dbl&gt;, VCO2.225 &lt;dbl&gt;, VCO2.250 &lt;dbl&gt;, VCO2.275 &lt;dbl&gt;,\n#   VCO2.300 &lt;dbl&gt;, VCO2.325 &lt;dbl&gt;, VCO2.350 &lt;dbl&gt;, VCO2.375 &lt;dbl&gt;,\n#   VE.125 &lt;dbl&gt;, VE.175 &lt;dbl&gt;, VE.225 &lt;dbl&gt;, VE.250 &lt;dbl&gt;, VE.275 &lt;dbl&gt;, …\n\n\nNotice however that the default tolerance is a very small number (1.4901161^{-8}).\n\n\n\n\n\n\nExercises (3)\n\n\n\n\nKeep rows in your data frame from the pre time-point, age greater than 31 but height less than 180.\nUse between to find rows with VO2.max values between 4800 and 5200 (see ?between)\nUse near to find weight.T1 values close to 80.26 with a tolerance of 0.75 (see ?near)\nRemove all rows that are NA in the height.T1 variable.\n\n\n\n\n\nSorting a data frame does not change the observations, it only determines how the data frame is printed or passed to other functions.\nIf we want to sort our data in cyclingstudy based on VO2.max we would\n\ncyclingstudy %&gt;%\n1        select(subject, group, VO2.max) %&gt;%\n2        arrange(VO2.max)\n\n\ncyclingstudy %&gt;%\n        select(subject, group, VO2.max) %&gt;% \n3        arrange(desc(VO2.max))\n\n\ncyclingstudy %&gt;%\n        select(subject, group, VO2.max) %&gt;% \n        arrange(desc(group) )\n\n\n1\n\nSelects a subset of variables for display\n\n2\n\nArrange based on the variable VO2.max, smallest values on top\n\n3\n\nArrange with largest value on top\n\n\n\n\nNotice the use of the desc function. In this example it is equivalent to using arrange(-VO2.max), in other situations, like with factors, desc will work as expected but not -."
  },
  {
    "objectID": "ws4-data-wrangling-tables.html#group-and-summarise-data---group_by-and-summarise",
    "href": "ws4-data-wrangling-tables.html#group-and-summarise-data---group_by-and-summarise",
    "title": "Data wrangling and tables",
    "section": "",
    "text": "A super power of dplyr is its ability to group and summarize data. The group_by function creates a grouped data frame suitable for summaries per group. In the cyclingstudy data set we have three groups that we might want to describe using some summary function.\nExamples of summary functions in R are:\n\nmean() → computes arithmetic mean\nmedian() → computes the median\nsd() → computes the standard deviation from the mean\nIQR() → returns the inter-quartile range\nmin() and max() → gives the minimum and maximum values from a vector\nquantile() → sample quantiles from the smallest (probs = 0) to largest (probs = 1) values.\n\nAll the above functions comes with the optional argument of na.rm = TRUE. This can be read as remove missing values (NA). If there are missing values (NA) and na.rm = FALSE (the default), the calculations will return NA. This is inconvenient but can often work as check of your assumptions.\n\n\n\n\n\n\nExercises (4)\n\n\n\n\nWhat do we mean by check of your assumptions?\nWhat do you expect from the R code sd(c(4, 5, 7, NA, 5))\nWhat would you add to the code above to improve it?\n\n\n\nIn addition to the summaries above that are generic for base R, dplyr provides you with a number of great functions to…\n\nn() → count the number of observations in each group\nn_distinct() → return the number of unique values from a vector for each group\n\nIn practice a grouped summary may look like this:\n\ncyclingstudy %&gt;%\n        group_by(group) %&gt;%\n        summarise(mean.vo2max = mean(VO2.max, na.rm = TRUE))\n\nResults from the above code includes multiple data points from each participant. The variable describing time-points can be added to the grouping.\n\ncyclingstudy %&gt;%\n        group_by(group, timepoint) %&gt;%\n        summarise(mean.vo2max = mean(VO2.max, na.rm = TRUE))\n\nMultiple summary functions can be added to the summarise() function where each adds a new variable to the result data frame.\n\ncyclingstudy %&gt;%\n        group_by(group, timepoint) %&gt;%\n        summarise(mean.vo2max = mean(VO2.max, na.rm = TRUE), \n                  sd.vo2max = sd(VO2.max, na.rm = TRUE))\n\n\n\n\n\n\n\nExercises (5)\n\n\n\n\nUse the cyc.bmi data frame your created above, filter to only keep timepoint == \"pre\", and from the BMI variable calculate:\n\nmedian\nminimum\nmaximum\n25th percentile (q25 in the example below)\n75th percentile (q75 in the example below)\n\nSave the new data set as cyc.bmi.summary and plot the data using the code below:\nCompare the plot with figure created with geom_boxplot(), see ?geom_boxplot for the difference between plots as described under Summary statistics.\n\n\ncyc.bmi.summary %&gt;%\n        ggplot(aes(group, median)) + geom_point( size = 4) + \n        geom_errorbar(aes(ymin = q25, ymax = q75), width = 0, size = 2) +\n        geom_errorbar(aes(ymin = min, ymax = max), width = 0, size = 1)\n\n\ncyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        select(subject, group, weight = weight.T1, height = height.T1) %&gt;%\n        mutate(bmi = weight / ((height/100)^2)) %&gt;%\n        ggplot(aes(group, bmi)) + geom_boxplot()"
  },
  {
    "objectID": "ws4-data-wrangling-tables.html#pivoting-using-pivot_longer-and-pivot_wider-from-the-tidyr-package",
    "href": "ws4-data-wrangling-tables.html#pivoting-using-pivot_longer-and-pivot_wider-from-the-tidyr-package",
    "title": "Data wrangling and tables",
    "section": "",
    "text": "Data can be described as being in long form and wide form. The long form data is tidy in the sense that all columns are distinct variables.\n\nThere are examples of wide data sets as part of the cycling data set. Using only the timepoint == pre values and columns corresponding to lactate values from the graded exercise test we have an example of wide data as the columns lac.125, lac.175, lac.225, etc., contains lactate values from different exercise intensities. This means that a separate variable (watt or exercise intensity) is combined in each column of lactate values.\n\ncyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        dplyr::select(subject, group, lac.125:lac.375) %&gt;%\n        print()\n\n# A tibble: 20 × 11\n   subject group lac.125 lac.175 lac.225 lac.250 lac.275 lac.300 lac.325 lac.350\n     &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1       1 INCR     1.5     1.86    2.38    3.54    6.21   NA      NA      NA   \n 2       2 DECR     1.19    1.49    2.34    3.21    5.33   NA      NA      NA   \n 3       3 INCR     1.17    1.52    1.22    1.54    2.04    3.32    4.72   NA   \n 4       4 DECR     0.88    0.99    2.13    3.25   NA       6.15   NA      NA   \n 5       5 DECR     1.06    1.41    1.9     2.04    3.04    3.59    4.73   NA   \n 6       6 INCR     1.27    1.73    3.21    4.83   NA      NA      NA      NA   \n 7       7 MIX      0.85    0.84    1.16    1.71    3.33    6.25   NA      NA   \n 8       8 MIX      0.93    1.34    1.94   NA       3.71    7.29   NA      NA   \n 9       9 MIX      1.48    1.17    1.95   NA       3.24    6.21   NA      NA   \n10      10 INCR     0.93    0.87    0.86    0.92    1.2     1.69    2.6     4.69\n11      11 MIX      0.62    1.22    2.85    5.86    9.87   NA      NA      NA   \n12      13 DECR     1.67    1.81    2.78    4.25    6.87   NA      NA      NA   \n13      14 MIX      1.13    1.33    2.74    3.97   NA      NA      NA      NA   \n14      15 INCR     0.8     1.12    1.43   NA       2.4     3.77    6.3    NA   \n15      16 INCR     0.94    1.18    1.89    2.83    5.33   NA      NA      NA   \n16      17 MIX      1.54    1.62    2.7     4.1    NA      NA      NA      NA   \n17      18 DECR     1.75    2.08    2.99    4.22   NA      NA      NA      NA   \n18      19 DECR     1.23    2.51    4.65   NA      NA      NA      NA      NA   \n19      20 INCR     2.26    2.05    3.19    5.17   NA      NA      NA      NA   \n20      21 DECR     0.68    0.89    1.98    3.18    5.57   NA      NA      NA   \n# ℹ 1 more variable: lac.375 &lt;dbl&gt;\n\n\nUsing pivot_longer we can change this data into a long format. Pivot wider needs information on the new variable names for values and names. Names are the column names that will form a variable and values are the values contained in the cells of the old variables. We also need to specify what columns to make longer, notice that I select variables &lt;from&gt;:&lt;to&gt;.\n\ncyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        dplyr::select(subject, group, lac.125:lac.375) %&gt;%\n        pivot_longer(names_to = \"watt\", \n                     values_to = \"lactate\", \n                     cols = lac.125:lac.375) %&gt;%\n        print()\n\n# A tibble: 180 × 4\n   subject group watt    lactate\n     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;\n 1       1 INCR  lac.125    1.5 \n 2       1 INCR  lac.175    1.86\n 3       1 INCR  lac.225    2.38\n 4       1 INCR  lac.250    3.54\n 5       1 INCR  lac.275    6.21\n 6       1 INCR  lac.300   NA   \n 7       1 INCR  lac.325   NA   \n 8       1 INCR  lac.350   NA   \n 9       1 INCR  lac.375   NA   \n10       2 DECR  lac.125    1.19\n# ℹ 170 more rows\n\n\npivot_wider makes it easy to remove prefix and fix the data type of the names variable. Below we specify to remove lac. from all names and convert the new variable to numeric data.\n\n# eval: false\n\n\ncyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        dplyr::select(subject, group, lac.125:lac.375) %&gt;%\n        pivot_longer(names_to = \"watt\", \n                     values_to = \"lactate\", \n                     cols = lac.125:lac.375, \n                     names_prefix = \"lac.\", \n                     names_transform = list(watt = as.numeric)) %&gt;%\n        print()\n\n# A tibble: 180 × 4\n   subject group  watt lactate\n     &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1       1 INCR    125    1.5 \n 2       1 INCR    175    1.86\n 3       1 INCR    225    2.38\n 4       1 INCR    250    3.54\n 5       1 INCR    275    6.21\n 6       1 INCR    300   NA   \n 7       1 INCR    325   NA   \n 8       1 INCR    350   NA   \n 9       1 INCR    375   NA   \n10       2 DECR    125    1.19\n# ℹ 170 more rows\n\n\nA data set can contain multiple “groups” of variables in a wide format. In the cycling data set variables such as HF.* and lac.* are variables that are collected at different intensities (watt, indicated with *). We would like to make the data set tidy, each column contains a variable and each row is an observation.\nIn the example below we create a long data set using two variables of interest, lactate (lac.*) and heart rate (HF.*) each collected over multiple intensities. Using a special name .value we can tell pivot_longer that there are additional information in the column names that should be regarded as names. We also need to a separator that separates our .value with the watt column, in this case the separator is a period/point (.). Note that R evaluates a single . as special character matching any single character. To use the period character as a separator we must “escape” it using either \\\\. or [.], this removes its special meaning.\n\n# eval: false\n\ncyclingstudy %&gt;%\n1        filter(timepoint == \"pre\") %&gt;%\n        select(subject, group, lac.125:lac.375, HF.125:HF.375) %&gt;%\n2        pivot_longer(cols = !c(subject, group),\n3                     names_to = c(\".value\", \"watt\"),\n4                     names_sep = \"[.]\") %&gt;%\n        print()\n\n\n1\n\nFilter the data set an select variables\n\n2\n\nUsing pivot_wider, first specify what columns to “make longer”.\n\n3\n\nUse the special .value to indicate that each variable name contains information that should be used in creating new variables\n\n4\n\nIndicate what character to split the variables on, in this case we need to “escape” the period using [.].\n\n\n\n\n# A tibble: 180 × 5\n   subject group watt    lac    HF\n     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1       1 INCR  125    1.5    109\n 2       1 INCR  175    1.86   124\n 3       1 INCR  225    2.38   139\n 4       1 INCR  250    3.54   152\n 5       1 INCR  275    6.21   167\n 6       1 INCR  300   NA       NA\n 7       1 INCR  325   NA       NA\n 8       1 INCR  350   NA       NA\n 9       1 INCR  375   NA       NA\n10       2 DECR  125    1.19   121\n# ℹ 170 more rows\n\n\n\n\n\n\n\n\nExercises (6)\n\n\n\n\nPivot the following data set, making separate columns/variables for each NAME\n\n\ndata.frame(id = c(\"id1\", \"id1\", \"id1\", \"id2\", \"id2\", \"id2\"), \n           NAME = c(\"a\", \"b\", \"c\", \"a\", \"b\", \"c\"), \n           NUMBER = c(4, 6, 7, 2, 3, 5)) %&gt;%\n\n        print()\n\n\nUsing the cyclingstudy data set, make a ggplot that shows\n\nVCO2 and VO2 values at sub-maximal intensities (watt 125-375) for participant 15.\nEach time-point (pre, meso1, meso2 and meso3) should be its own facet/panel\nVCO2 and VO2 should be of different colors indicated with points and lines\nLabels and units should be fixed (see figure below for details, look up the ggtext package for examples on how to format text in labels and titles).\n\n\n\n\n\n\n\n\n\n\n\n\nTable 1 in experimental or observational studies often contains descriptive data on the sample. These tables may help readers to understand to what group/population a study may be generalized to and how key characteristics are distributed among experimental groups.\n\n\nWe will prepare data from cyclingstudy to create a Table 1 with descriptive data - Select a set of key variables that you want to describe (center, spread and/or range) from baseline measurements (timepoint == \"pre\"), for example age, weight, height and BMI. - Group the data set on group and perform summary calculations\n\n\n\nTo display numbers with the correct number of decimals R provides many options. A simple function (round()) provides rounding. The problem is that you will lose trailing zero, e.g., 2.0 will be displayed as 2. To keep the trailing zero we must use the sprintf() function. Examples:\n\n\n# Rounding\nround(2.10, 2) \n\n[1] 2.1\n\n# Formatting to keep the trailing zero\nsprintf(\"%.2f\", 2.10)\n\n[1] \"2.10\"\n\n\n\nCombining vectors may be a good idea to make the table more attractive. The mean and standard deviation is commonly presented as mean (SD). Data from a column of means and a column of SD’s can be combined to create a nice display using the paste0() function. Example:\n\n\ndata.frame(m = c(46.7, 47.89, 43.5),  # A vector of means\n           s = c(4.21, 4.666, 3.1)) %&gt;% # A vector of SD's\n        mutate(stat = paste0(round(m, 1), \n                             \" (\",\n                             round(s, 1), \n                             \")\")) %&gt;%\n        print()\n\n      m     s       stat\n1 46.70 4.210 46.7 (4.2)\n2 47.89 4.666 47.9 (4.7)\n3 43.50 3.100 43.5 (3.1)\n\n\n\nA character vector of group names can be arranged and re-named using the factor function. In the cyclingstudy data set the groups (INCR, DECR and MIX) may be given more descriptive names, example:\n\n\ncyclingstudy %&gt;%\n        mutate(group = factor(group, levels = c(\"INCR\", \"DECR\", \"MIX\"), \n                              labels = c(\"Increased intensity\", \n                                         \"Decreased intensity\", \n                                         \"Mixed intensity\"))) %&gt;%\n        distinct(group)\n\n# A tibble: 3 × 1\n  group              \n  &lt;fct&gt;              \n1 Increased intensity\n2 Decreased intensity\n3 Mixed intensity"
  },
  {
    "objectID": "ws4-data-wrangling-tables.html#the-next-step-create-a-neat-report",
    "href": "ws4-data-wrangling-tables.html#the-next-step-create-a-neat-report",
    "title": "Data wrangling and tables",
    "section": "",
    "text": "We will use a table generator to create the table. See next part of this workshop here"
  },
  {
    "objectID": "ws2-installing-r.html",
    "href": "ws2-installing-r.html",
    "title": "Starting up R",
    "section": "",
    "text": "Different statistical software packages. Source: The internet!\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAs the image above suggests, the software we will use in this class is Exciting and a bit Dangerous!\n\n\n\n\nR is an open-source programming language1 that has a well developed system for user contribution. This means that it is possible to write extensions, these are also called packages. Packages contain specialized programs designed to solve different tasks.\nThe R software, together with packages can be found at CRAN, the Comprehensive R Archive Network. You can access CRAN from R.\n\n\nRStudio is an Integrated Development Environment. This software makes it easy to “talk” to R by keeping track of your files, any data stored in your computers memory and so on. RStudio contains an editor that makes it easy to edit text files that can be interpreted by R.\nWe will get to know RStudio by checking out:\n\nThe console\nEnvironment and history\nFiles, Plots, Packages, Help and Viewer\nGlobal options\n\nImportantly we will change the option concerning “Save workspace to .RData on exit” to NEVER. This will make your work dependent on what you write in your “programs” which is a good thing for reproducibility.\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nTell a friend what the words package, CRAN, and open-source means\nExplain to a friend what you do with the R console\nExplain what the environment and history contains\nWhere do you install packages?\nWhy would you change “Save workspace to .RData on exit” to never under Global options?\n\n\n\n\n\n\n\n\nA good thing about R is that you will have to script your work. This essentially creates a “program” that R will interpret. The interpretation of your program will result in some output (e.g., a graph, table, a single number or a report).\nThis separates R from point-and-click or spreadsheet software. This also makes it easier to reproduce an analysis.\nR has many flavors of scripts as some combines text with statistical programming that in the end creates a nice looking report. We will talk more about these variants later.\n\n\n\n\n\nAn object can be a number, a collection of numbers or many other representations of data that is stored in the workspace.\nStart a new script from the File menu (File &gt; New File &gt; R Script), copy the code from the code block below to your script.\nExecute the code by placing the cursor on the row containing the code and pres Ctrl+Enter (Cmd+Enter on Mac).\n\na &lt;- 12\n\nThe &lt;-operator assigns the number 12 to an object called a. This object is stored in the workspace. We can point the arrow in the other direction also.\n\n12 -&gt; b\n\nAssignment using &lt;- or -&gt; can go either way. Alternatively you could use = which will have the same function as &lt;-. Try to assign 12 to c by using the following code 12 = c. You should encounter an error message.\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\nExplain the following sentence in your own words: “The &lt;-operator assigns the number 12 to an object called a. This object is stored in the workspace.”\n\n\n\n\n\n\nObjects can be, depending on their type, combined. Numeric objects can be used in mathematical operations. What is the result of the following code?\n\na + b\n\nMore complex mathematical treatment is also possible using objects.\n\nx &lt;- 12.3 + 3 * a + -1.2 * sqrt(b)\n\nThe sqrt(b) part in the formula above is the first occurance of a R function. It takes a number (or a vector of numbers) and return the square root.\nFor every function, there is a help page. The help pages for sqrt() can be accessed by typing ?sqrt() in the console.\n\n\n\n\nVectors are a combination of data that can be manipulated. The R function c() combines data into a vector.\n\nv &lt;- c(2, 5, 7, 3, 1)\n\nThe vector v has five numbers (2, 5, 7, 3, 1), we can combine these with another value, let’s say the value stored in a\n\nv + a\n\nHowever, if we combine it with a and b, we run into problems.\n\nv + c(a, b)\n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\nExplain the following in your own words why v + c(a, b) resulted in a warning message.\n\n\n\nLet’s add another value to v and try to combine the values stored in v with a and b.\n\nv &lt;- c(v, 1)\n\nv + c(a, b)\n\nWe can examine objects by using R functions such as:\n\nlength(v)\n\nstr(v)\n\nlength(a)\n\nclass(a)\n\n\n\n\nData can be further combined into data frames. These are tabular representations of data with where a row indicate an observation and a column represents a variable. We can create a new data frame using the following code:\n\ndf &lt;- data.frame(v = v, \n                 a = a, \n                 color = c(\"red\", \"red\", \"red\", \"green\", \"green\", \"green\"))\n\nNote that I use the = operator inside the data.frame function. This is a convenient way of separating assignments to the environment/objects from isnside e.g. data abojects or functions.\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\nExplain how the object a multiplies in the data frame function. Explore, what is the class of df?\n\n\n\nData in a data frame can be accessed in multiple ways. First, to extract a specific variable, we can use the $ operator.\n\ndf$color\n\nThe code above prints the content of the variable color as a vector of characters. A character vector is a collection of text that has no further meaning to R (but might have to us!).\nWe can also extract a specific value using brackets. To extract the second row, from the third column, we would write:\n\ndf[2, 3]\n\n(We can remember this by saying out loud ROW-COLUMN).\nNew variables can be added to the data frame by assignment\n\ndf$new_variable &lt;- df$v * 3.14 \n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\nExplain in your own words what happend in the code chunk above.\n\n\n\nWe can also add variables to the data frame that are the results of “tests” of the data.\n\ndf$another_variable &lt;- df$new_variable &gt; 10 \n\nThe operator &gt; tests if observations in the variable new_variable are larger than 10. This results in a logical vector containing TRUE or FALSE. These are special vectors called logical vectors that can be either TRUE or FALSE.\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\nWe have covered numeric, logical and character vectors. Explain the differences to your friend.\n\n\n\n\n\nThe character vector color contained in the data frame df is a character vector. However, we can transform it to a factor. A factor can be ordered by your specifications. We can also add labels. Let’s reorganize the color variable.\n\ndf$color &lt;- factor(df$color) \n\ndf$color\n\nRun the code above and you will see that by converting the character vector to a factor we ge a new data type that has levels in alphabetical order. We can change the levels to what ever we want using the factor function and the argument levels.\n\ndf$color &lt;- factor(df$color, levels = c(\"red\", \"green\")) \n\ndf$color\n\nThis was the first mention of arguments. Every function usually takes user specified arguments that determines the output from the function. Possible arguments can be reviewed by accessing e.g. ?factor.\n\n\n\n\nIt is good practice to name variables or objects with words that are meaningful to what you are doing. A variable/object name cannot start with a number, it cannot contain spaces or special characters.\nA good programmer is lazy. Avoid mixing large and small letters, reduce the number of symbols in variable names without loosing meaning and be consistent.\n\n\n\nUsing the built in system for plotting, we can create a first figure of our data. This plot has a lot of potential for improvements. We will explore this potential in the next workshop!\n\nplot(df$v, df$new_variable)"
  },
  {
    "objectID": "ws2-installing-r.html#the-r-ecosystem",
    "href": "ws2-installing-r.html#the-r-ecosystem",
    "title": "Starting up R",
    "section": "",
    "text": "R is an open-source programming language1 that has a well developed system for user contribution. This means that it is possible to write extensions, these are also called packages. Packages contain specialized programs designed to solve different tasks.\nThe R software, together with packages can be found at CRAN, the Comprehensive R Archive Network. You can access CRAN from R.\n\n\nRStudio is an Integrated Development Environment. This software makes it easy to “talk” to R by keeping track of your files, any data stored in your computers memory and so on. RStudio contains an editor that makes it easy to edit text files that can be interpreted by R.\nWe will get to know RStudio by checking out:\n\nThe console\nEnvironment and history\nFiles, Plots, Packages, Help and Viewer\nGlobal options\n\nImportantly we will change the option concerning “Save workspace to .RData on exit” to NEVER. This will make your work dependent on what you write in your “programs” which is a good thing for reproducibility.\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nTell a friend what the words package, CRAN, and open-source means\nExplain to a friend what you do with the R console\nExplain what the environment and history contains\nWhere do you install packages?\nWhy would you change “Save workspace to .RData on exit” to never under Global options?"
  },
  {
    "objectID": "ws2-installing-r.html#scripts",
    "href": "ws2-installing-r.html#scripts",
    "title": "Starting up R",
    "section": "",
    "text": "A good thing about R is that you will have to script your work. This essentially creates a “program” that R will interpret. The interpretation of your program will result in some output (e.g., a graph, table, a single number or a report).\nThis separates R from point-and-click or spreadsheet software. This also makes it easier to reproduce an analysis.\nR has many flavors of scripts as some combines text with statistical programming that in the end creates a nice looking report. We will talk more about these variants later."
  },
  {
    "objectID": "ws2-installing-r.html#r-basics",
    "href": "ws2-installing-r.html#r-basics",
    "title": "Starting up R",
    "section": "",
    "text": "An object can be a number, a collection of numbers or many other representations of data that is stored in the workspace.\nStart a new script from the File menu (File &gt; New File &gt; R Script), copy the code from the code block below to your script.\nExecute the code by placing the cursor on the row containing the code and pres Ctrl+Enter (Cmd+Enter on Mac).\n\na &lt;- 12\n\nThe &lt;-operator assigns the number 12 to an object called a. This object is stored in the workspace. We can point the arrow in the other direction also.\n\n12 -&gt; b\n\nAssignment using &lt;- or -&gt; can go either way. Alternatively you could use = which will have the same function as &lt;-. Try to assign 12 to c by using the following code 12 = c. You should encounter an error message.\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\nExplain the following sentence in your own words: “The &lt;-operator assigns the number 12 to an object called a. This object is stored in the workspace.”\n\n\n\n\n\n\nObjects can be, depending on their type, combined. Numeric objects can be used in mathematical operations. What is the result of the following code?\n\na + b\n\nMore complex mathematical treatment is also possible using objects.\n\nx &lt;- 12.3 + 3 * a + -1.2 * sqrt(b)\n\nThe sqrt(b) part in the formula above is the first occurance of a R function. It takes a number (or a vector of numbers) and return the square root.\nFor every function, there is a help page. The help pages for sqrt() can be accessed by typing ?sqrt() in the console."
  },
  {
    "objectID": "ws2-installing-r.html#vectors",
    "href": "ws2-installing-r.html#vectors",
    "title": "Starting up R",
    "section": "",
    "text": "Vectors are a combination of data that can be manipulated. The R function c() combines data into a vector.\n\nv &lt;- c(2, 5, 7, 3, 1)\n\nThe vector v has five numbers (2, 5, 7, 3, 1), we can combine these with another value, let’s say the value stored in a\n\nv + a\n\nHowever, if we combine it with a and b, we run into problems.\n\nv + c(a, b)\n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\nExplain the following in your own words why v + c(a, b) resulted in a warning message.\n\n\n\nLet’s add another value to v and try to combine the values stored in v with a and b.\n\nv &lt;- c(v, 1)\n\nv + c(a, b)\n\nWe can examine objects by using R functions such as:\n\nlength(v)\n\nstr(v)\n\nlength(a)\n\nclass(a)"
  },
  {
    "objectID": "ws2-installing-r.html#data-frames",
    "href": "ws2-installing-r.html#data-frames",
    "title": "Starting up R",
    "section": "",
    "text": "Data can be further combined into data frames. These are tabular representations of data with where a row indicate an observation and a column represents a variable. We can create a new data frame using the following code:\n\ndf &lt;- data.frame(v = v, \n                 a = a, \n                 color = c(\"red\", \"red\", \"red\", \"green\", \"green\", \"green\"))\n\nNote that I use the = operator inside the data.frame function. This is a convenient way of separating assignments to the environment/objects from isnside e.g. data abojects or functions.\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\nExplain how the object a multiplies in the data frame function. Explore, what is the class of df?\n\n\n\nData in a data frame can be accessed in multiple ways. First, to extract a specific variable, we can use the $ operator.\n\ndf$color\n\nThe code above prints the content of the variable color as a vector of characters. A character vector is a collection of text that has no further meaning to R (but might have to us!).\nWe can also extract a specific value using brackets. To extract the second row, from the third column, we would write:\n\ndf[2, 3]\n\n(We can remember this by saying out loud ROW-COLUMN).\nNew variables can be added to the data frame by assignment\n\ndf$new_variable &lt;- df$v * 3.14 \n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\nExplain in your own words what happend in the code chunk above.\n\n\n\nWe can also add variables to the data frame that are the results of “tests” of the data.\n\ndf$another_variable &lt;- df$new_variable &gt; 10 \n\nThe operator &gt; tests if observations in the variable new_variable are larger than 10. This results in a logical vector containing TRUE or FALSE. These are special vectors called logical vectors that can be either TRUE or FALSE.\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\nWe have covered numeric, logical and character vectors. Explain the differences to your friend.\n\n\n\n\n\nThe character vector color contained in the data frame df is a character vector. However, we can transform it to a factor. A factor can be ordered by your specifications. We can also add labels. Let’s reorganize the color variable.\n\ndf$color &lt;- factor(df$color) \n\ndf$color\n\nRun the code above and you will see that by converting the character vector to a factor we ge a new data type that has levels in alphabetical order. We can change the levels to what ever we want using the factor function and the argument levels.\n\ndf$color &lt;- factor(df$color, levels = c(\"red\", \"green\")) \n\ndf$color\n\nThis was the first mention of arguments. Every function usually takes user specified arguments that determines the output from the function. Possible arguments can be reviewed by accessing e.g. ?factor."
  },
  {
    "objectID": "ws2-installing-r.html#naming-variables",
    "href": "ws2-installing-r.html#naming-variables",
    "title": "Starting up R",
    "section": "",
    "text": "It is good practice to name variables or objects with words that are meaningful to what you are doing. A variable/object name cannot start with a number, it cannot contain spaces or special characters.\nA good programmer is lazy. Avoid mixing large and small letters, reduce the number of symbols in variable names without loosing meaning and be consistent."
  },
  {
    "objectID": "ws2-installing-r.html#a-first-plot",
    "href": "ws2-installing-r.html#a-first-plot",
    "title": "Starting up R",
    "section": "",
    "text": "Using the built in system for plotting, we can create a first figure of our data. This plot has a lot of potential for improvements. We will explore this potential in the next workshop!\n\nplot(df$v, df$new_variable)"
  },
  {
    "objectID": "ws2-installing-r.html#footnotes",
    "href": "ws2-installing-r.html#footnotes",
    "title": "Starting up R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA more elaborate description can be found at the CRAN FAQ↩︎"
  },
  {
    "objectID": "ws14-sampling-power-effects.html",
    "href": "ws14-sampling-power-effects.html",
    "title": "Sampling, effects and power",
    "section": "",
    "text": "Statistical power is the long-run ability of a study design to detect a true effect.\nIn frequentist statistics, the true effect is fixed and we want to use a sample to estimate it.\nSince we do not know the true effect, the goal of a power analysis is to reach sufficient power for an assumed effect (Lakens 2022).\nThis assumption can be based on a hypothesis of a true effect, or\nAn effect that is considered clinically meaningful\n\n\n\nWe now will build a simulation study to investigate\n\nHow often will we find a “true” population effect in studies of different sizes?\nWhat is the relationship between effect size, power and statistical significance?\nWhat is the effect of sample size on the precision of estimates (confidence intervals)\n\nOur simulation will investigate a known population effect of 0.5. This standardized effect (\\(d\\)) is in the one-sample case \\(d = \\frac{mean}{SD}\\).\n\n\nCode\nlibrary(tidyverse)\nset.seed(1)\n\n\n# Create a population to sample from with a known effect\n# In standardized terms, the population effect is 0.5 = mean / sd.\npopulation &lt;- rnorm(10^6, 5, 10) \n\n# Calculate the population effect size \npop.es &lt;- mean(population) / sd(population)\n\n\nresults_total &lt;- list()\n\n# 0. Inside for-loop:\nfor(i in 1:1000) {\n  \n# 1. Sample from the population with sample size 10 to 100\n  \n  # 1.1 create a vector of sample sizes\n  sample.sizes &lt;- seq(from = 10, to = 100, by = 10)\n  # 1.2 create a list to store results\n  results_sub &lt;- list()\n  \n  # 1.3 Inside a nested for-loop, perform sampling with each sample size\n  for(j in 1:length(sample.sizes)) {\n    \n  samp &lt;- sample(population, sample.sizes[j], \n                 replace = FALSE)\n  \n  \n# 2. Create a model\n  m &lt;- lm(y ~ 1, data = data.frame(y = samp))\n  \n  # 2.1 Store results from each model as a data frame in a list:\n results_sub[[j]] &lt;- data.frame(mean = coef(summary(m))[1, 1],  # Estimated mean\n             se = coef(summary(m))[1, 2],  # Standard error\n             pval = coef(summary(m))[1, 4],  # p-value\n             ci.lwr = confint(m)[1], # confidence interval \n             ci.upr = confint(m)[2], # confidence interval\n             effect.size = mean(samp) / sd(samp),\n             sample.size = sample.sizes[j]) # Sample size\n  \n    \n    \n  }\n  # 2.2 Combine all data frames from each sample size as a data frame in a list\n  results_total[[i]] &lt;- bind_rows(results_sub)\n\n  \n  # 3. Repeat 1 and 2 1000 times, performing 1000 studies each with \n  # sample sizes 10 to 100.\n  \n}\n\n# Combine all results\nresults_total &lt;- bind_rows(results_total)\n\n\n\n\n\n\nAs the sample size increases, we are more likely to have a study that detects a true effect\nWe simply count the proportion of “studies” that declear a statistically significant effect at \\(p&lt;0.05\\)\n\n\n\nCode\n# Count numbers of studies with p &lt; 0.05\nresults_total %&gt;%\n  filter(pval &lt; 0.05) %&gt;%\n  group_by(sample.size) %&gt;%\n  summarise(n = n(), \n            prop = n / 1000) %&gt;%\n  ggplot(aes(sample.size, 100 * prop)) + geom_line() + geom_point(shape = 21, fill = \"lightblue\", size = 3) + \n  labs(x = \"Sample size\", \n       y = \"Percentage of studies rejecting the null-hypothesis\", \n       title = \"Statistical power and sample size\", \n       subtitle = \"Statistical significance set to 0.05, standardized effect-size 0.5\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\nThe relationship between power and sample size can be used to make cost-benefit analyses of future studies.\nThe “cost” of a study can be regarded as e.g. economical or ethical.\nIn most cases the power calculation can be done without simulations, using e.g. the pwr package.\n\n\n\nCode\nlibrary(pwr)\n\nsample.sizes &lt;- seq(from = 10, to = 100, by = 10)\nresults.pwr &lt;- list()\n\nfor(i in 1:length(sample.sizes)) {\n  \n  pwr_analysis &lt;- pwr.t.test(type = \"one.sample\", d = 5/10, sig.level = 0.05, n = sample.sizes[i])\n  \n\n  results.pwr[[i]] &lt;- data.frame(sample.size = sample.sizes[i], \n                                 prop = pwr_analysis$power)\n  \n  \n  }\n\nresults.pwr &lt;- bind_rows(results.pwr)\n\n# Count numbers of studies with p &lt; 0.05\nresults_total %&gt;%\n  filter(pval &lt; 0.05) %&gt;%\n  group_by(sample.size) %&gt;%\n  summarise(n = n(), \n            prop = n / 1000) %&gt;%\n  ggplot(aes(sample.size, 100 * prop)) + \n  geom_line() + \n  geom_point(shape = 21, fill = \"lightblue\", size = 3) + \n  \n  geom_line(data = results.pwr, color = \"red\") +\n  \n  labs(x = \"Sample size\", \n       y = \"Percentage of studies rejecting the null-hypothesis\", \n       title = \"Statistical power and sample size\", \n       subtitle = \"Statistical significance set to 0.05, standardized effect-size 0.5.\") +\n  theme_bw()\n\n\n\n\n\nPower analysis using simulations (blue circles) and analytically (red line)\n\n\n\n\n\n\n\n\nA standardized (observed) effect size may be calculated from each study. What is the relationship between effect-sizes, sample sizes and p-values?\nThe p-value is directly related to the observed effect-size. The sample size determines the p-value at a specific effect-size.\n\n\n\nCode\nlibrary(cowplot)\n\n\nplotA &lt;- results_total %&gt;%\n  ggplot(aes(effect.size, pval, \n             color = as.factor(sample.size))) + geom_point() +\n  labs(color = \"Sample size\") +\n  \n    geom_hline(yintercept = 0.05, color = \"red\") + \n  annotate(\"text\", x = 1.5, y = 0.05 + 0.1, \n           color = \"red\",\n           label = \"P-value = 0.05\", \n           size = 2.5) +\n  \n  geom_hline(yintercept = 0.001, color = \"blue\") + \n  annotate(\"text\", x = 1.5, y = 0.001 + 0.1, \n           color = \"blue\",\n           label = \"P-value = 0.01\", \n           size = 2.5) +\n  \n  labs(title = \"Effect size vs. p-values\", \n       x = \"Effect size\", \n       y = \"P-value\") + \n\n  theme_bw() + \n  theme(legend.position = \"none\")\n\n\n\nplotB &lt;- results_total %&gt;%\n  ggplot(aes(effect.size, -log(pval), \n             color = as.factor(sample.size))) + geom_point() +\n  labs(color = \"Sample size\", \n       title = \" \", \n       x = \"Effect size\", \n       y = \"-log(p-value)\") +\n  \n  geom_hline(yintercept = -log(0.05), color = \"red\") + \n  annotate(\"text\", x = -0.2, y = -log(0.05) + 1, \n           color = \"red\",\n           label = \"P-value = 0.05\", \n           size = 2.5) +\n  \n  geom_hline(yintercept = -log(0.001), color = \"blue\") + \n  annotate(\"text\", x = -0.2, y = -log(0.001) + 1, \n           color = \"blue\",\n           label = \"P-value = 0.01\", \n           size = 2.5) +\n  scale_x_continuous(breaks = c(0, 0.5, 1, 2)) +\n  \n  \n  theme_bw()\n\nlibrary(cowplot)\nplot_grid(plotA, plotB, ncol = 2, rel_widths = c(0.8, 1))\n\n\n\n\n\n\n\n\n\n\nThe observed effect size is an estimation of the population effect size. In our case, the population effect-size is \\(\\frac{mean}{SD} = \\frac{5}{10} = 0.5\\).\nHow well do we estimate effect sizes?\n\n\n\nCode\nlibrary(ggtext)\n\nresults_total %&gt;%\n  ggplot(aes(sample.size, effect.size, color = pval)) + \n  \n  geom_point(position = position_jitter(), alpha = 0.5) +\n    geom_hline(yintercept = pop.es, lty = 1, size = 2, color = \"red\") +\n  labs(x = \"Sample size\", \n       y = \"Observed standardized effect-size\", \n       color = \"P-value\", \n           title = \"Observed effect-sizes per sample size\") +\n  annotate(\"richtext\", x = 150, y = pop.es, \n           color = \"red\", \n           hjust = 1,\n           label = \"Population effect-size\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrecision could be regarded as the ability to estimate an effect with some degree of certainty.\nThe confidence interval (CI) is constructed to find the true population effect at a given rate (e.g. 95% of studies).\nCI depends on the sample size:\n\n\\[95\\%~CI:~Estimate \\pm t_{\\text{critical}} \\times SE\\] For n = 10:\n\\[95\\%~CI:~Estimate \\pm 2.26 \\times SE\\]\n\n\nCode\nresults_total %&gt;%\n  ggplot(aes(sample.size, ci.upr-ci.lwr, fill = as.factor(sample.size))) +\n  geom_point(position = position_jitter(width = 0.8),\n             shape = 21, alpha = 0.4) +\n  labs(x = \"Sample size\", \n       y = \"Range of confidence intervals (Upper - Lower)\", \n       title = \"Confidence interval width and sample size\") + \n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\nTo decrease the width of the confidence interval by half, we need to increase the sample size about 3-4 times.\n\n\n\nCode\nlibrary(knitr)\n\nresults_total %&gt;% \n  mutate(ci_width = ci.upr-ci.lwr) %&gt;%\n  group_by(sample.size) %&gt;%\n  summarise(ci_width = mean(ci_width)) %&gt;%\n  kable(col.names = c(\"Sample size\", \"95% CI width\"), \n        digits = c(1, 1))\n\n\n\n\n\nSample size\n95% CI width\n\n\n\n\n10\n14.0\n\n\n20\n9.3\n\n\n30\n7.4\n\n\n40\n6.4\n\n\n50\n5.7\n\n\n60\n5.1\n\n\n70\n4.7\n\n\n80\n4.4\n\n\n90\n4.2\n\n\n100\n4.0\n\n\n\n\n\n\nThe confidence interval covers the true population parameter in 95% of repeated studies. 1000 studies is not enough to reach exactly 95%.\n\n\n\nCode\n#| echo: true\n#| message: false\n#| warning: false\n\n\n\nresults_total %&gt;%\n  mutate(sig = if_else(ci.lwr &lt;= 5 & ci.upr &gt;= 5, \"sig\", \"ns\"), \n         interval = paste0(rep(seq(1:1000), each = 10))) %&gt;%\n  filter(sig == \"sig\") %&gt;%\n  group_by(sample.size) %&gt;%\n  summarise(prop = 100 * (n() / 1000)) %&gt;%\n    kable(col.names = c(\"Sample size\", \"Proportion of 95% CI finding the true effect\"), \n        digits = c(1, 1))\n\n\n\n\n\nSample size\nProportion of 95% CI finding the true effect\n\n\n\n\n10\n95.1\n\n\n20\n95.1\n\n\n30\n95.1\n\n\n40\n95.1\n\n\n50\n95.2\n\n\n60\n94.1\n\n\n70\n94.5\n\n\n80\n94.8\n\n\n90\n94.0\n\n\n100\n94.6\n\n\n\n\n\n\nWe will be wrong at the same rate with a given coverage of the confidence interval independent of the sample size.\nHowever, we could increase the coverage of the interval and maintain precision with a larger sample size\n\n\n\nCode\n#| echo: true\n#| message: false\n#| warning: false\n\n\nresults_total %&gt;%\n  mutate(sig = if_else(ci.lwr &lt;= 5 & ci.upr &gt;= 5, \"sig\", \"ns\"), \n         interval = paste0(rep(seq(1:1000), each = 10))) %&gt;%\n  filter(interval %in% seq(1:100)) %&gt;%\n  \n  ggplot(aes(mean, interval, color = sig, alpha = sig)) + \n  geom_errorbarh(aes(xmin = ci.lwr, \n                    xmax = ci.upr)) + \n  \n  scale_alpha_manual(values = c(1, 0.3)) +\n  \n  geom_vline(xintercept = 5, color = \"red\") +\n  \n  facet_wrap(~ sample.size) + \n  \n  labs(title = \"Confidence intervals from 100 studies\", \n       subtitle = \"Red intervals misses the population average\") +\n  theme_bw() +\n  \n  theme(axis.text.y = element_blank(), \n        axis.ticks.y = element_blank(), \n        legend.position = \"none\", \n        axis.title = element_blank())"
  },
  {
    "objectID": "ws14-sampling-power-effects.html#revisiting-power",
    "href": "ws14-sampling-power-effects.html#revisiting-power",
    "title": "Sampling, effects and power",
    "section": "",
    "text": "Statistical power is the long-run ability of a study design to detect a true effect.\nIn frequentist statistics, the true effect is fixed and we want to use a sample to estimate it.\nSince we do not know the true effect, the goal of a power analysis is to reach sufficient power for an assumed effect (Lakens 2022).\nThis assumption can be based on a hypothesis of a true effect, or\nAn effect that is considered clinically meaningful\n\n\n\nWe now will build a simulation study to investigate\n\nHow often will we find a “true” population effect in studies of different sizes?\nWhat is the relationship between effect size, power and statistical significance?\nWhat is the effect of sample size on the precision of estimates (confidence intervals)\n\nOur simulation will investigate a known population effect of 0.5. This standardized effect (\\(d\\)) is in the one-sample case \\(d = \\frac{mean}{SD}\\).\n\n\nCode\nlibrary(tidyverse)\nset.seed(1)\n\n\n# Create a population to sample from with a known effect\n# In standardized terms, the population effect is 0.5 = mean / sd.\npopulation &lt;- rnorm(10^6, 5, 10) \n\n# Calculate the population effect size \npop.es &lt;- mean(population) / sd(population)\n\n\nresults_total &lt;- list()\n\n# 0. Inside for-loop:\nfor(i in 1:1000) {\n  \n# 1. Sample from the population with sample size 10 to 100\n  \n  # 1.1 create a vector of sample sizes\n  sample.sizes &lt;- seq(from = 10, to = 100, by = 10)\n  # 1.2 create a list to store results\n  results_sub &lt;- list()\n  \n  # 1.3 Inside a nested for-loop, perform sampling with each sample size\n  for(j in 1:length(sample.sizes)) {\n    \n  samp &lt;- sample(population, sample.sizes[j], \n                 replace = FALSE)\n  \n  \n# 2. Create a model\n  m &lt;- lm(y ~ 1, data = data.frame(y = samp))\n  \n  # 2.1 Store results from each model as a data frame in a list:\n results_sub[[j]] &lt;- data.frame(mean = coef(summary(m))[1, 1],  # Estimated mean\n             se = coef(summary(m))[1, 2],  # Standard error\n             pval = coef(summary(m))[1, 4],  # p-value\n             ci.lwr = confint(m)[1], # confidence interval \n             ci.upr = confint(m)[2], # confidence interval\n             effect.size = mean(samp) / sd(samp),\n             sample.size = sample.sizes[j]) # Sample size\n  \n    \n    \n  }\n  # 2.2 Combine all data frames from each sample size as a data frame in a list\n  results_total[[i]] &lt;- bind_rows(results_sub)\n\n  \n  # 3. Repeat 1 and 2 1000 times, performing 1000 studies each with \n  # sample sizes 10 to 100.\n  \n}\n\n# Combine all results\nresults_total &lt;- bind_rows(results_total)\n\n\n\n\n\n\nAs the sample size increases, we are more likely to have a study that detects a true effect\nWe simply count the proportion of “studies” that declear a statistically significant effect at \\(p&lt;0.05\\)\n\n\n\nCode\n# Count numbers of studies with p &lt; 0.05\nresults_total %&gt;%\n  filter(pval &lt; 0.05) %&gt;%\n  group_by(sample.size) %&gt;%\n  summarise(n = n(), \n            prop = n / 1000) %&gt;%\n  ggplot(aes(sample.size, 100 * prop)) + geom_line() + geom_point(shape = 21, fill = \"lightblue\", size = 3) + \n  labs(x = \"Sample size\", \n       y = \"Percentage of studies rejecting the null-hypothesis\", \n       title = \"Statistical power and sample size\", \n       subtitle = \"Statistical significance set to 0.05, standardized effect-size 0.5\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\nThe relationship between power and sample size can be used to make cost-benefit analyses of future studies.\nThe “cost” of a study can be regarded as e.g. economical or ethical.\nIn most cases the power calculation can be done without simulations, using e.g. the pwr package.\n\n\n\nCode\nlibrary(pwr)\n\nsample.sizes &lt;- seq(from = 10, to = 100, by = 10)\nresults.pwr &lt;- list()\n\nfor(i in 1:length(sample.sizes)) {\n  \n  pwr_analysis &lt;- pwr.t.test(type = \"one.sample\", d = 5/10, sig.level = 0.05, n = sample.sizes[i])\n  \n\n  results.pwr[[i]] &lt;- data.frame(sample.size = sample.sizes[i], \n                                 prop = pwr_analysis$power)\n  \n  \n  }\n\nresults.pwr &lt;- bind_rows(results.pwr)\n\n# Count numbers of studies with p &lt; 0.05\nresults_total %&gt;%\n  filter(pval &lt; 0.05) %&gt;%\n  group_by(sample.size) %&gt;%\n  summarise(n = n(), \n            prop = n / 1000) %&gt;%\n  ggplot(aes(sample.size, 100 * prop)) + \n  geom_line() + \n  geom_point(shape = 21, fill = \"lightblue\", size = 3) + \n  \n  geom_line(data = results.pwr, color = \"red\") +\n  \n  labs(x = \"Sample size\", \n       y = \"Percentage of studies rejecting the null-hypothesis\", \n       title = \"Statistical power and sample size\", \n       subtitle = \"Statistical significance set to 0.05, standardized effect-size 0.5.\") +\n  theme_bw()\n\n\n\n\n\nPower analysis using simulations (blue circles) and analytically (red line)\n\n\n\n\n\n\n\n\nA standardized (observed) effect size may be calculated from each study. What is the relationship between effect-sizes, sample sizes and p-values?\nThe p-value is directly related to the observed effect-size. The sample size determines the p-value at a specific effect-size.\n\n\n\nCode\nlibrary(cowplot)\n\n\nplotA &lt;- results_total %&gt;%\n  ggplot(aes(effect.size, pval, \n             color = as.factor(sample.size))) + geom_point() +\n  labs(color = \"Sample size\") +\n  \n    geom_hline(yintercept = 0.05, color = \"red\") + \n  annotate(\"text\", x = 1.5, y = 0.05 + 0.1, \n           color = \"red\",\n           label = \"P-value = 0.05\", \n           size = 2.5) +\n  \n  geom_hline(yintercept = 0.001, color = \"blue\") + \n  annotate(\"text\", x = 1.5, y = 0.001 + 0.1, \n           color = \"blue\",\n           label = \"P-value = 0.01\", \n           size = 2.5) +\n  \n  labs(title = \"Effect size vs. p-values\", \n       x = \"Effect size\", \n       y = \"P-value\") + \n\n  theme_bw() + \n  theme(legend.position = \"none\")\n\n\n\nplotB &lt;- results_total %&gt;%\n  ggplot(aes(effect.size, -log(pval), \n             color = as.factor(sample.size))) + geom_point() +\n  labs(color = \"Sample size\", \n       title = \" \", \n       x = \"Effect size\", \n       y = \"-log(p-value)\") +\n  \n  geom_hline(yintercept = -log(0.05), color = \"red\") + \n  annotate(\"text\", x = -0.2, y = -log(0.05) + 1, \n           color = \"red\",\n           label = \"P-value = 0.05\", \n           size = 2.5) +\n  \n  geom_hline(yintercept = -log(0.001), color = \"blue\") + \n  annotate(\"text\", x = -0.2, y = -log(0.001) + 1, \n           color = \"blue\",\n           label = \"P-value = 0.01\", \n           size = 2.5) +\n  scale_x_continuous(breaks = c(0, 0.5, 1, 2)) +\n  \n  \n  theme_bw()\n\nlibrary(cowplot)\nplot_grid(plotA, plotB, ncol = 2, rel_widths = c(0.8, 1))\n\n\n\n\n\n\n\n\n\n\nThe observed effect size is an estimation of the population effect size. In our case, the population effect-size is \\(\\frac{mean}{SD} = \\frac{5}{10} = 0.5\\).\nHow well do we estimate effect sizes?\n\n\n\nCode\nlibrary(ggtext)\n\nresults_total %&gt;%\n  ggplot(aes(sample.size, effect.size, color = pval)) + \n  \n  geom_point(position = position_jitter(), alpha = 0.5) +\n    geom_hline(yintercept = pop.es, lty = 1, size = 2, color = \"red\") +\n  labs(x = \"Sample size\", \n       y = \"Observed standardized effect-size\", \n       color = \"P-value\", \n           title = \"Observed effect-sizes per sample size\") +\n  annotate(\"richtext\", x = 150, y = pop.es, \n           color = \"red\", \n           hjust = 1,\n           label = \"Population effect-size\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrecision could be regarded as the ability to estimate an effect with some degree of certainty.\nThe confidence interval (CI) is constructed to find the true population effect at a given rate (e.g. 95% of studies).\nCI depends on the sample size:\n\n\\[95\\%~CI:~Estimate \\pm t_{\\text{critical}} \\times SE\\] For n = 10:\n\\[95\\%~CI:~Estimate \\pm 2.26 \\times SE\\]\n\n\nCode\nresults_total %&gt;%\n  ggplot(aes(sample.size, ci.upr-ci.lwr, fill = as.factor(sample.size))) +\n  geom_point(position = position_jitter(width = 0.8),\n             shape = 21, alpha = 0.4) +\n  labs(x = \"Sample size\", \n       y = \"Range of confidence intervals (Upper - Lower)\", \n       title = \"Confidence interval width and sample size\") + \n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\nTo decrease the width of the confidence interval by half, we need to increase the sample size about 3-4 times.\n\n\n\nCode\nlibrary(knitr)\n\nresults_total %&gt;% \n  mutate(ci_width = ci.upr-ci.lwr) %&gt;%\n  group_by(sample.size) %&gt;%\n  summarise(ci_width = mean(ci_width)) %&gt;%\n  kable(col.names = c(\"Sample size\", \"95% CI width\"), \n        digits = c(1, 1))\n\n\n\n\n\nSample size\n95% CI width\n\n\n\n\n10\n14.0\n\n\n20\n9.3\n\n\n30\n7.4\n\n\n40\n6.4\n\n\n50\n5.7\n\n\n60\n5.1\n\n\n70\n4.7\n\n\n80\n4.4\n\n\n90\n4.2\n\n\n100\n4.0\n\n\n\n\n\n\nThe confidence interval covers the true population parameter in 95% of repeated studies. 1000 studies is not enough to reach exactly 95%.\n\n\n\nCode\n#| echo: true\n#| message: false\n#| warning: false\n\n\n\nresults_total %&gt;%\n  mutate(sig = if_else(ci.lwr &lt;= 5 & ci.upr &gt;= 5, \"sig\", \"ns\"), \n         interval = paste0(rep(seq(1:1000), each = 10))) %&gt;%\n  filter(sig == \"sig\") %&gt;%\n  group_by(sample.size) %&gt;%\n  summarise(prop = 100 * (n() / 1000)) %&gt;%\n    kable(col.names = c(\"Sample size\", \"Proportion of 95% CI finding the true effect\"), \n        digits = c(1, 1))\n\n\n\n\n\nSample size\nProportion of 95% CI finding the true effect\n\n\n\n\n10\n95.1\n\n\n20\n95.1\n\n\n30\n95.1\n\n\n40\n95.1\n\n\n50\n95.2\n\n\n60\n94.1\n\n\n70\n94.5\n\n\n80\n94.8\n\n\n90\n94.0\n\n\n100\n94.6\n\n\n\n\n\n\nWe will be wrong at the same rate with a given coverage of the confidence interval independent of the sample size.\nHowever, we could increase the coverage of the interval and maintain precision with a larger sample size\n\n\n\nCode\n#| echo: true\n#| message: false\n#| warning: false\n\n\nresults_total %&gt;%\n  mutate(sig = if_else(ci.lwr &lt;= 5 & ci.upr &gt;= 5, \"sig\", \"ns\"), \n         interval = paste0(rep(seq(1:1000), each = 10))) %&gt;%\n  filter(interval %in% seq(1:100)) %&gt;%\n  \n  ggplot(aes(mean, interval, color = sig, alpha = sig)) + \n  geom_errorbarh(aes(xmin = ci.lwr, \n                    xmax = ci.upr)) + \n  \n  scale_alpha_manual(values = c(1, 0.3)) +\n  \n  geom_vline(xintercept = 5, color = \"red\") +\n  \n  facet_wrap(~ sample.size) + \n  \n  labs(title = \"Confidence intervals from 100 studies\", \n       subtitle = \"Red intervals misses the population average\") +\n  theme_bw() +\n  \n  theme(axis.text.y = element_blank(), \n        axis.ticks.y = element_blank(), \n        legend.position = \"none\", \n        axis.title = element_blank())"
  },
  {
    "objectID": "ws14-sampling-power-effects.html#what-if-there-is-no-effect",
    "href": "ws14-sampling-power-effects.html#what-if-there-is-no-effect",
    "title": "Sampling, effects and power",
    "section": "What if there is no effect?",
    "text": "What if there is no effect?\n\nStudies that examine a population effect that is close to zero will be wrong at rate of 5%, if the \\(\\alpha\\) level is set to 0.05.\nThe “power” of such studies is 5% regardless of the sample size.\n\n\n\nCode\n#| echo: true\n#| message: false\n#| warning: false\n\n\n\nlibrary(tidyverse)\nset.seed(1)\n\n\n# Create a population to sample from with a known effect\n# In standardized terms, the population effect is 0 = mean / sd.\npopulation &lt;- rnorm(10^6, 0, 10) \n\n# Calculate the population effect size \npop.es &lt;- mean(population) / sd(population)\n\n\nresults_total_null &lt;- list()\n\n\n# 0. Inside for-loop:\nfor(i in 1:1000) {\n  \n# 1. Sample from the population with sample size 10 to 100\n  \n  # 1.1 create a vector of sample sizes\n  sample.sizes &lt;- seq(from = 10, to = 100, by = 10)\n  # 1.2 create a list to store results\n  results_sub &lt;- list()\n  \n  # 1.3 Inside a nested for-loop, perform sampling with each sample size\n  for(j in 1:length(sample.sizes)) {\n    \n  samp &lt;- sample(population, sample.sizes[j], \n                 replace = FALSE)\n  \n  \n# 2. Create a model\n  \n  m &lt;- lm(y ~ 1, data = data.frame(y = samp))\n  \n  # 2.1 Store results from each model as a data frame in a list:\n  \n results_sub[[j]] &lt;- data.frame(mean = coef(summary(m))[1, 1],  # Estimated mean\n             se = coef(summary(m))[1, 2],  # Standard error\n             pval = coef(summary(m))[1, 4],  # p-value\n             ci.lwr = confint(m)[1], # confidence interval \n             ci.upr = confint(m)[2], # confidence interval\n             effect.size = mean(samp) / sd(samp),\n             sample.size = sample.sizes[j]) # Sample size\n  \n    \n    \n  }\n  # 2.2 Combine all data frames from each sample size as a data frame in a list\n\n  results_total_null[[i]] &lt;- bind_rows(results_sub)\n\n  \n  # 3. Repeat 1 and 2 1000 times, performing 1000 studies each with \n  # sample sizes 10 to 100.\n  \n  # If we use a \"progress bar\"\n  # setTxtProgressBar(pb, i)\n  \n}\n\n# Close the progress bar\n# close(pb)\n\n# Combine all results\nresults_total_null &lt;- bind_rows(results_total_null)\n\n\n\n\nCode\n# Count numbers of studies with p &lt; 0.05\nresults_total_null %&gt;%\n  filter(pval &lt; 0.05) %&gt;%\n  group_by(sample.size) %&gt;%\n  summarise(n = n(), \n            prop = n / 1000) %&gt;%\n  ggplot(aes(sample.size, 100 * prop)) + \n  geom_hline(yintercept = 5, lty = 2, color = \"grey50\") +\n  geom_line() + \n  geom_point(shape = 21, fill = \"lightblue\", size = 3) + \n  labs(x = \"Sample size\", \n       y = \"Percentage of studies rejecting the null-hypothesis\", \n       title = \"Statistical power and sample size\", \n       subtitle = \"Statistical significance set to 0.05, standardized effect-size 0\") +\n  scale_y_continuous(limits = c(0, 100)) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\nP-values in studies examining population zero-effects will be uniformly distributed\n\n\n\nCode\nresults_total_null %&gt;%\n  ggplot(aes(pval)) + \n  geom_histogram(aes(y=100 * ..count../1000), \n                 binwidth = 0.05, boundary = 0, color = \"gray30\", fill = \"white\") + \n  labs(x = \"P-value\", \n       y = \"Percentage of studies\", \n       title = \"Distribution of p-values from studies of a population zero-effect\") +\n  scale_y_continuous(limits = c(0, 10), expand = c(0, 0)) +\n  facet_wrap(~ sample.size) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\nBe aware of p = 0.047, this p-value can be more probable when sampling from the zero-effect population in certain cases, an example:\n\n\n\nCode\nresults_total %&gt;%\n  filter(sample.size == 50, \n         pval &lt;= 0.05) %&gt;%\n    ggplot(aes(pval)) + \n  geom_histogram(aes(y=100 * ..count../1000), \n                 binwidth = 0.005, boundary = 0, color = \"gray30\", fill = \"darkolivegreen4\") +\n  \n  \n    geom_histogram(data = results_total_null %&gt;%\n                          filter(sample.size == 50, \n                                  pval &lt;= 0.05), \n  aes(y=100 * ..count../1000), \n                 binwidth = 0.005, boundary = 0, color = \"gray30\", fill = \"darkblue\") +\n  \n    labs(x = \"P-value\", \n       y = \"Percentage of studies\", \n       title = \"Distribution of p-values from studies of true zero-effect and true d = 0.5\", \n       subtitle = \"Sample size: n = 50\") +\n  annotate(\"text\", x = 0.03, y = 5, label = \"Population effect: 0\", color = \"darkblue\") +\n  annotate(\"text\", x = 0.03, y = 6, label = \"Population effect: 0.5\", color = \"darkolivegreen4\") +\n  coord_cartesian(ylim=c(0,10)) +\n  theme_bw()"
  },
  {
    "objectID": "ws12-writing-functions.html",
    "href": "ws12-writing-functions.html",
    "title": "Writing R functions and calculate a lactate threshold",
    "section": "",
    "text": "For a comprehensive overview of writing functions in R, Chapter 19 in (Wickham and Grolemund 2017) contains an excellent introduction."
  },
  {
    "objectID": "ws12-writing-functions.html#why-write-functions",
    "href": "ws12-writing-functions.html#why-write-functions",
    "title": "Writing R functions and calculate a lactate threshold",
    "section": "Why write functions",
    "text": "Why write functions\n\nYou should invest in writing functions, if you plan to re-use code by copy paste more than once (Wickham and Grolemund 2017).\nFunctions can make your code more readable, reusable and make sure you minimize errors.\nThe DRY principle applies → Do not repeat yourself (Wickham and Grolemund 2017)."
  },
  {
    "objectID": "ws12-writing-functions.html#what-is-a-function",
    "href": "ws12-writing-functions.html#what-is-a-function",
    "title": "Writing R functions and calculate a lactate threshold",
    "section": "What is a function?",
    "text": "What is a function?\n\nR is easily “extended” through the creation of function.\nFunctions can live inside package that you or someone else has written.\nFunctions can also live in your environment after being defined in your script.\nmean(), sd() mutate()and ggplot() are examples of functions that comes with the basic installation of R or packages.\nFor a function to live in your environment, it must have a name\nThe function can come with a set of arguments\nThe actual mechanics of the function is defined in the body\n\n\n# An example function\nmy_fun  &lt;- function(arg1, arg2) { # Name and arguments of the function\n        \n        # This is the body\n        sum &lt;- arg1 + arg2\n        \n        return(sum)\n        \n}\n\nmy_fun(2, 4)\n\n\nNaming the function\n\nThe same rules apply to naming functions as other R objects.\n(Wickham and Grolemund 2017) recommend longer and informative/descriptive names written in “snake_case” (as opposed to “camelCase”).\nBe consistent!\nDo not use names of other function!\n\n\n\nArguments\n\nA function may be defined with named arguments, this is the “input” of the function.\nAn argument may be a data frame, a single value or vector that the function will use.\nThe function can be used with the arguments used in their place or named out of place.\n\n\n# An example function\nmy_fun  &lt;- function(value1, value2) { # Name and arguments of the function\n        \n        # This is the body\n        diff &lt;- value1 - value2\n        \n        return(diff)\n        \n}\n\n# these are different\nmy_fun(value2 = 1, value1 = 4)\n\nmy_fun(1, 4)\n\n\n\nBody\n\nThe function body defines the output of the function\nIt usually makes use of the data/values/variables defined in the arguments and returns a given output\nOutput can be any R object.\nThe return() function is used to explicitly make some part of the body to the output of the function\n\n\n# An example function\nmy_fun  &lt;- function(value1, value2) { # Name and arguments of the function\n        \n        # This is the body\n        diff &lt;- value1 - value2\n        \n        sum &lt;- value1 + value2\n        \n        results &lt;- list()\n        \n        results$diff &lt;- diff\n        results$sum &lt;- sum\n        \n        return(results)\n        \n}\n\n# these are different\nmy_fun(value2 = 1, value1 = 4)\n\nmy_fun(1, 4)\n\n\nEnvironment\n\nIf a variable is not defined in the function, R will try to look for it in the environment.\nIt is considered good practice to not rely on variables not defined in the function\n\n\n# An example function\n\n# A variable defined outside the function\nvalue1 &lt;- 3\n\n\nmy_fun  &lt;- function(value2) { # Name and arguments of the function\n        \n        # This is the body\n        diff &lt;- value1 - value2\n        \n        sum &lt;- value1 + value2\n        \n        results &lt;- list()\n        \n        results$diff &lt;- diff\n        results$sum &lt;- sum\n        \n        return(results)\n        \n}\n\nmy_fun(value2 = 1)\n\nmy_fun(5)\n\n\n\n\nConditions, errors and messages\n\nFunctions can include conditional sections, using if makes it possible to make the function flexible depending on input variables\n\n\nmy_fun &lt;- function(value1, value2, calculate.sum = FALSE) {\n        \n        if (calculate.sum == TRUE) {\n                \n                sum &lt;- value1 + value2\n                \n                return(sum)\n                \n        } else {\n                \n                print(\"No sum calculated\")\n                \n        }\n        \n}\n\n# Calculates the sum \nmy_fun(2, 4, TRUE)\n\n# Does not calculate the sum\nmy_fun(2, 4, FALSE)\n\n\nA function can stop if the input variable is of the wrong type,\n\n\nmy_fun &lt;- function(value1, value2) {\n        \n        if (!any(is.numeric(c(value1, value2)))) {\n                \n                stop(\"One or more values are not numeric\")\n                \n        } else {\n                \n                sum &lt;- value1 + value2\n                \n                return(sum)\n                \n        }\n        \n}\n\n# Calculates the sum \nmy_fun(value1 = \"error?\", value2 = 4)\n\n# Does not calculate the sum\nmy_fun(value1 = 2, value2 = 4)\n\n\nThere are several additional ways to create conditional operations (see (Wickham and Grolemund 2017)).\n\n\n\n\n\n\n\nGroup work\n\n\n\n\nWrite a function that is reusable using this code:\n\n\nz &lt;- df$var1 - mean(df$var1, na.rm = TRUE) / sd(df$var1, na.rm = TRUE)\n\n\nWrite a function that stops if the input is not a character vector\nWrite a function that calculates the standard deviation\n\n\\[s=\\sqrt{\\frac{\\sum(x_i-\\bar{x})^2}{n-1}}\\]\n\n# An example vector\nx &lt;- c(2, 5, 7, 8)\n\n# Sum of squares\nss &lt;- sum((x - mean(x))^2)\n\n## Calculates standard deviation\ns &lt;- sqrt(ss / (length(x)-1))"
  },
  {
    "objectID": "ws12-writing-functions.html#a-function-to-calculate-lactate-thresholds",
    "href": "ws12-writing-functions.html#a-function-to-calculate-lactate-thresholds",
    "title": "Writing R functions and calculate a lactate threshold",
    "section": "A function to calculate lactate thresholds",
    "text": "A function to calculate lactate thresholds\n\nLactate threshold (LT) tests are commonly performed in the laboratory\nCalculations of the LT differs between labs and there are many methods\nA possible method is to calculate the power at a fixed lactate value (e.g. 4 mmoL L-1)\n\n\nlibrary(tidyverse)\nlibrary(exscidata)\n\ndata(\"cyclingstudy\")\n\n\ncyclingstudy %&gt;%\n  # Select columns needed for analysis\n  select(subject, group, timepoint, lac.225:lac.375) %&gt;%\n  # Only one participant and time-point\n  filter(timepoint == \"pre\", subject == 10) %&gt;%\n  # Pivot to long format data using the lactate columns\n  pivot_longer(names_to = \"watt\", \n               values_to = \"lactate\", \n               names_prefix = \"lac.\",\n               names_transform = list(watt = as.numeric),\n               cols = lac.225:lac.375) %&gt;%\n  # Plot the data, group = subject needed to connect the points\n  ggplot(aes(watt, lactate, group = subject))  + \n  geom_line(lty = 2) +\n  geom_point(shape = 21, fill = \"lightblue\", size = 2.5) \n\n\n\n\nA workload and lactate relationship\n\n\n\n\n\nManually, we could estimate the watt at a specific lactate using ocular inspection\n\n\ncyclingstudy %&gt;%\n  # Select columns needed for analysis\n  select(subject, group, timepoint, lac.225:lac.375) %&gt;%\n  # Only one participant and time-point\n  filter(timepoint == \"pre\", subject == 10) %&gt;%\n  # Pivot to long format data using the lactate columns\n  pivot_longer(names_to = \"watt\", \n               values_to = \"lactate\", \n               names_prefix = \"lac.\",\n               names_transform = list(watt = as.numeric),\n               cols = lac.225:lac.375) %&gt;%\n  # Plot the data, group = subject needed to connect the points\n  ggplot(aes(watt, lactate, group = subject))  + \n  geom_line(lty = 2) +\n  geom_point(shape = 21, fill = \"lightblue\", size = 2.5) +\n  # Adding straight lines at specific values\n  geom_hline(yintercept = 4, color = \"red\") +\n  geom_vline(xintercept = 341.5, color = \"blue\")\n\n\n\n\nA workload and lactate relationship, manual identification of the lactate threshold\n\n\n\n\n\nA better approximation can be derived from the curve linear model\n\n\ncyclingstudy %&gt;%\n  # Select columns needed for analysis\n  select(subject, group, timepoint, lac.225:lac.375) %&gt;%\n  # Only one participant and time-point\n  filter(timepoint == \"pre\", subject == 10) %&gt;%\n  # Pivot to long format data using the lactate columns\n  pivot_longer(names_to = \"watt\", \n               values_to = \"lactate\", \n               names_prefix = \"lac.\",\n               names_transform = list(watt = as.numeric),\n               cols = lac.225:lac.375) %&gt;%\n  # Plot the data, group = subject needed to connect the points\n  ggplot(aes(watt, lactate, group = subject))  + \n  geom_line(lty = 2) +\n  geom_point(shape = 21, fill = \"lightblue\", size = 2.5) +\n  geom_hline(yintercept = 4, color = \"red\") +\n  geom_vline(xintercept = 341.5, color = \"blue\") +\n  # Adding a straight line from a linear model\n  geom_smooth(method = \"lm\", se = FALSE, formula = y ~ x, color = \"#e41a1c\") +\n  \n  # Adding a polynomial linear model to the plot\n  \n  # poly(x, 2) add a second degree polynomial model.\n  geom_smooth(method = \"lm\", se = FALSE, formula = y ~ poly(x, 2), color = \"#377eb8\") +\n  # poly(x, 3) add a third degree polynomial model.\n  geom_smooth(method = \"lm\", se = FALSE, formula = y ~ poly(x, 3), color = \"#4daf4a\") +\n  # poly(x, 4) add a forth degree polynomial model.\n  geom_smooth(method = \"lm\", se = FALSE, formula = y ~ poly(x, 4), color = \"#ff7f00\") \n\n\n\n\nA workload and lactate relationship, adding curve linear models\n\n\n\n\n\nThese models are all “wrong but some are useful”1\n\n\nlactate &lt;- cyclingstudy %&gt;%\n  # Select columns needed for analysis\n  select(subject, group, timepoint, lac.225:lac.375) %&gt;%\n  # Only one participant and time-point\n  filter(timepoint == \"pre\", subject == 10) %&gt;%\n  # Pivot to long format data using the lactate columns\n  pivot_longer(names_to = \"watt\", \n               values_to = \"lactate\", \n               names_prefix = \"lac.\",\n               names_transform = list(watt = as.numeric),\n               cols = lac.225:lac.375) %&gt;%\n  # Remove NA (missing) values to avoid warning/error messages.\n  filter(!is.na(lactate))\n\n# fit \"straight line\" model\nm1 &lt;- lm(lactate ~ watt, data = lactate)\n\n# fit second degree polynomial\nm2 &lt;- lm(lactate ~ poly(watt, 2, raw = TRUE), data = lactate)\n\n# fit third degree polynomial\nm3 &lt;- lm(lactate ~ poly(watt, 3, raw = TRUE), data = lactate)\n\n# fit forth degree polynomial\nm4 &lt;- lm(lactate ~ poly(watt, 4, raw = TRUE), data = lactate)\n\n# Store all residuals as new variables\nlactate$resid.m1 &lt;- resid(m1)\nlactate$resid.m2 &lt;- resid(m2)\nlactate$resid.m3 &lt;- resid(m3)\nlactate$resid.m4 &lt;- resid(m4)\n\nlactate %&gt;%\n  # gather all the data from the models\n  pivot_longer(names_to = \"model\", \n               values_to = \"residual\", \n               names_prefix = \"resid.\", \n               names_transform = list(residual = as.numeric), \n               cols = resid.m1:resid.m4) %&gt;%\n  # Plot values with the observed watt on x axis and residual values at the y\n  ggplot(aes(watt, residual, fill = model)) + geom_point(shape = 21, size = 3) +\n  \n  # To set the same colors/fills as above we use scale fill manual\n  scale_fill_manual(values = c(\"#e41a1c\", \"#377eb8\", \"#4daf4a\", \"#ff7f00\"))\n\n\n\n\nAssessing the fit of different linear models on a exercise intensity to lactate accumulation relationship\n\n\n\n\n\nUsing the predict() function we can predict lactate values at a specific power output.\nWe are modelling the effect of watt on lactate, so we are unable to input a specific lactate value, instead we could approximate with “inverse prediction\n\n\nndf &lt;- data.frame(watt = seq(from = 225, to = 350, by = 0.1)) # high resolution, we can find the nearest10:th a watt\n\nndf$predictions &lt;- predict(m3, newdata = ndf)\n\n# Which value of the predictions comes closest to our value of 4 mmol L-1?\n# abs finds the absolute value, makes all values positive, \n# predictions - 4 givs an exact prediction of 4 mmol the value zero\n# filter the row which has the prediction - 4 equal to the minimal absolut difference between prediction and 4 mmol\nlactate_threshold &lt;- ndf %&gt;%\n  filter(abs(predictions - 4) == min(abs(predictions - 4)))\n\n\n\n\n\n\n\nGroup work\n\n\n\n\nWrite a function that calculates the lactate threshold using a fixed lactate value.\nThe function should have arguments that defines the data and lactate value\nThe output should be a value of the approximate watt at the fixed lactate"
  },
  {
    "objectID": "ws12-writing-functions.html#footnotes",
    "href": "ws12-writing-functions.html#footnotes",
    "title": "Writing R functions and calculate a lactate threshold",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee https://en.wikipedia.org/wiki/All_models_are_wrong↩︎"
  },
  {
    "objectID": "ws1-data-science-intro.html",
    "href": "ws1-data-science-intro.html",
    "title": "Introduksjon til datavitenskap og data i praksis",
    "section": "",
    "text": "Diskuter i grupper (3-4 pers, 15 min):\n\nHvilke verktøy for reproduserbar dataanalyse er du kjent med i dag? På hvilken måte kan disse verktøyene brukes for reproduserbar dataanalyse?\nDiskuter noen scenarioer hvor du, i forskjellige yrkesroller (f.eks. helse-omsorg, idrett, undervisning, forskning) ville hatt bruk av PPDAC (eller lignende datadrevet grunnlag for beslutninger). Vær realistisk! Tenk på noen spesifikke arbeidsoppgaver.\nHvilke er de største hindringene for lære «datavitenskap» som beskrevet i pensum/forelesning?\n\n\n\n\nBruk Broman og Woo (Broman and Woo 2018) for å analysere et datasett. Oppgaven er å finne problemer og foreslå løsninger. Gjennomfør øvelsen i gruppe (3-4 pers, 20 min):\n\nAlle leser punkt 1 (Introduction)\nGruppe 1 analysere datasett ved hjelp av punktene 2-5\nGruppe 2 analysere ved hjelp av 6-8\nGruppe 3 bruker punktene 10-13\n\nNår det er mulig, forandre filen til det bedre. Noter når det er ikke er mulig eller gir uønskede konsekvenser."
  },
  {
    "objectID": "ws1-data-science-intro.html#hvordan-forstå-å-bruke-datavitenskap",
    "href": "ws1-data-science-intro.html#hvordan-forstå-å-bruke-datavitenskap",
    "title": "Introduksjon til datavitenskap og data i praksis",
    "section": "",
    "text": "Diskuter i grupper (3-4 pers, 15 min):\n\nHvilke verktøy for reproduserbar dataanalyse er du kjent med i dag? På hvilken måte kan disse verktøyene brukes for reproduserbar dataanalyse?\nDiskuter noen scenarioer hvor du, i forskjellige yrkesroller (f.eks. helse-omsorg, idrett, undervisning, forskning) ville hatt bruk av PPDAC (eller lignende datadrevet grunnlag for beslutninger). Vær realistisk! Tenk på noen spesifikke arbeidsoppgaver.\nHvilke er de største hindringene for lære «datavitenskap» som beskrevet i pensum/forelesning?"
  },
  {
    "objectID": "ws1-data-science-intro.html#data-in-the-wild",
    "href": "ws1-data-science-intro.html#data-in-the-wild",
    "title": "Introduksjon til datavitenskap og data i praksis",
    "section": "",
    "text": "Bruk Broman og Woo (Broman and Woo 2018) for å analysere et datasett. Oppgaven er å finne problemer og foreslå løsninger. Gjennomfør øvelsen i gruppe (3-4 pers, 20 min):\n\nAlle leser punkt 1 (Introduction)\nGruppe 1 analysere datasett ved hjelp av punktene 2-5\nGruppe 2 analysere ved hjelp av 6-8\nGruppe 3 bruker punktene 10-13\n\nNår det er mulig, forandre filen til det bedre. Noter når det er ikke er mulig eller gir uønskede konsekvenser."
  },
  {
    "objectID": "lec1-intr-to-data-sci.html#why-data-science-and-statistics",
    "href": "lec1-intr-to-data-sci.html#why-data-science-and-statistics",
    "title": "Lecture 1: Introduction to data science",
    "section": "Why data science and statistics?",
    "text": "Why data science and statistics?\n\n\n\n\n\n\n\nFrom (Spiegelhalter 2019, 24)\n\n\n\n\n\n\n\n\n\n\n\nFrom (Spiegelhalter 2019, 24)\n\n\n\n\n\n\n\n\n\n\n\nFrom (Spiegelhalter 2019, 24)\n\n\n\n\n\n\n\n\n\n\n\nFrom (Spiegelhalter 2019, 24)\n\n\n\n\n\n\n\n\n\n\n\nFrom (Spiegelhalter 2019, 24)\n\n\n\n\n\n\n\n\n\n\n\nFrom (Spiegelhalter 2019, 24)\n\n\n\n\n\n\n\n\nWe will start by talking about the research process. One way to think about this process is to imagine it as a cycle.\nA definition of a problem comes from first finding something else.\nAfter defining the problem we could move on to plan how to resolve the problem (…).\nThe next step includes collecting, managing and cleaning data (…)\nThe analysis includes steps like summarising and visualizing data, creating models and examine hypotheses…\nIn the “conclusion” step we interpret and communicate what we have found, and the cycle moves on.."
  },
  {
    "objectID": "lec1-intr-to-data-sci.html#why-data-science-and-statistics-1",
    "href": "lec1-intr-to-data-sci.html#why-data-science-and-statistics-1",
    "title": "Lecture 1: Introduction to data science",
    "section": "Why data science and statistics?",
    "text": "Why data science and statistics?\nData literacy: “the ability to understand the principles behind learning from data, carry out basic data analyses, and critique the quality of claims made on the basis of data” (Spiegelhalter 2019)\n\n\nAll steps in the research cycle helps you develop data literacy.\nHow can we LEARN from data, how can we do analyses, and how may analyses fool us (…)"
  },
  {
    "objectID": "lec1-intr-to-data-sci.html#transferable-skills",
    "href": "lec1-intr-to-data-sci.html#transferable-skills",
    "title": "Lecture 1: Introduction to data science",
    "section": "Transferable skills",
    "text": "Transferable skills\n\nNew York Post\n\nData literacy is a transferable skill, we can apply what we learn here in many different contexts."
  },
  {
    "objectID": "lec1-intr-to-data-sci.html#replication",
    "href": "lec1-intr-to-data-sci.html#replication",
    "title": "Lecture 1: Introduction to data science",
    "section": "Replication",
    "text": "Replication\n\nA lot of the scientific process is about confirming claims and refining what we think we know\nReplication is a key component in scientific research → can a phenomena be replicated in an independent setting?"
  },
  {
    "objectID": "lec1-intr-to-data-sci.html#replication-and-reproducibility",
    "href": "lec1-intr-to-data-sci.html#replication-and-reproducibility",
    "title": "Lecture 1: Introduction to data science",
    "section": "Replication and Reproducibility",
    "text": "Replication and Reproducibility\n\nFrom (Peng 2011)."
  },
  {
    "objectID": "lec1-intr-to-data-sci.html#replication-and-reproducibility-1",
    "href": "lec1-intr-to-data-sci.html#replication-and-reproducibility-1",
    "title": "Lecture 1: Introduction to data science",
    "section": "Replication and Reproducibility",
    "text": "Replication and Reproducibility\n\nReplication: Confirming scientific claims with independent data.\n\nEssential for verifying research findings.\nChallenges with replication due to study size, cost, and urgency.\n\nReproducibility: Minimum standard in scientific research.\n\nIndependent researchers can replicate the results using the same data."
  },
  {
    "objectID": "lec1-intr-to-data-sci.html#reproducibility",
    "href": "lec1-intr-to-data-sci.html#reproducibility",
    "title": "Lecture 1: Introduction to data science",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\n\n\n\nUnfortunately, reproducibility is one of those things that are not a top priority in the scientific community…"
  },
  {
    "objectID": "lec1-intr-to-data-sci.html#reproducibility-1",
    "href": "lec1-intr-to-data-sci.html#reproducibility-1",
    "title": "Lecture 1: Introduction to data science",
    "section": "Reproducibility",
    "text": "Reproducibility\n\nAvailable data\nComputer code\nDocumentation\nData and code sharing methods\n\n(Peng 2011)"
  },
  {
    "objectID": "lec1-intr-to-data-sci.html#how-to-enable-reproducibility",
    "href": "lec1-intr-to-data-sci.html#how-to-enable-reproducibility",
    "title": "Lecture 1: Introduction to data science",
    "section": "How to enable reproducibility",
    "text": "How to enable reproducibility"
  },
  {
    "objectID": "lec1-intr-to-data-sci.html#what-is-a-computer-program",
    "href": "lec1-intr-to-data-sci.html#what-is-a-computer-program",
    "title": "Lecture 1: Introduction to data science",
    "section": "What is a computer program?",
    "text": "What is a computer program?\n\n\n\n\n\n(By Wikimedia Foundation, CC BY-SA 3.0, Link)\n\n\n\n“A computer program is a sequence or set of instructions in a programming language for a computer to execute. It is one component of software, which also includes documentation and other intangible components.”"
  },
  {
    "objectID": "lec1-intr-to-data-sci.html#a-sequence-of-instructions",
    "href": "lec1-intr-to-data-sci.html#a-sequence-of-instructions",
    "title": "Lecture 1: Introduction to data science",
    "section": "A sequence of instructions",
    "text": "A sequence of instructions\n\n\n\n\n\n\n\n\nCoko, a programmable crocodile"
  },
  {
    "objectID": "lec1-intr-to-data-sci.html#data-analysis-as-a-computer-program",
    "href": "lec1-intr-to-data-sci.html#data-analysis-as-a-computer-program",
    "title": "Lecture 1: Introduction to data science",
    "section": "Data analysis as a computer program",
    "text": "Data analysis as a computer program\n\n(Wickham and Grolemund 2017)"
  },
  {
    "objectID": "lec1-intr-to-data-sci.html#tools-in-data-science",
    "href": "lec1-intr-to-data-sci.html#tools-in-data-science",
    "title": "Lecture 1: Introduction to data science",
    "section": "Tools in Data Science",
    "text": "Tools in Data Science\n\n\nMicrosoft Excel: Widely used, versatile tool.\nSPSS, Stata, Jamovi: Specialized software for statistical analysis.\nR: Preferred for reproducible data analysis:\n\nFree, open-source, and script-based.\nSteep learning curve."
  },
  {
    "objectID": "lec1-intr-to-data-sci.html#research-software",
    "href": "lec1-intr-to-data-sci.html#research-software",
    "title": "Lecture 1: Introduction to data science",
    "section": "Research software",
    "text": "Research software\n\n\n\n\nhttps://xkcd.com/2347/"
  },
  {
    "objectID": "lec1-intr-to-data-sci.html#why-r",
    "href": "lec1-intr-to-data-sci.html#why-r",
    "title": "Lecture 1: Introduction to data science",
    "section": "Why R?",
    "text": "Why R?\n\n\n\nR is a very powerful language (software ecosystem), we want to use it because it makes doing reproducible science easier."
  },
  {
    "objectID": "lec1-intr-to-data-sci.html#why-r-1",
    "href": "lec1-intr-to-data-sci.html#why-r-1",
    "title": "Lecture 1: Introduction to data science",
    "section": "Why R?",
    "text": "Why R?\n\n\n\nThere are other languages that offer similar capabilities, but R is developed with Data analysis in mind."
  },
  {
    "objectID": "lec1-intr-to-data-sci.html#how-to-learn",
    "href": "lec1-intr-to-data-sci.html#how-to-learn",
    "title": "Lecture 1: Introduction to data science",
    "section": "How to learn?",
    "text": "How to learn?\n\n\nPractice by imitation\nUnderstand that there are multiple solutions to problems\nUse online resources like Stack Overflow and Google\nStay motivated by using what you learn on problems that interest you\nHave patience: “the capacity to accept or tolerate delay, problems, or suffering without becoming annoyed or anxious.”"
  },
  {
    "objectID": "feedback-assignments.html",
    "href": "feedback-assignments.html",
    "title": "Sjekkliste for arbeidskrav og eksamen",
    "section": "",
    "text": "Følgende punkter er basert på tidligere arbeidskrav som har blitt levert i emnet. Disse kan brukes for å forbedre dine arbeidskrav."
  },
  {
    "objectID": "feedback-assignments.html#å-skrive-rapporter",
    "href": "feedback-assignments.html#å-skrive-rapporter",
    "title": "Sjekkliste for arbeidskrav og eksamen",
    "section": "Å skrive rapporter",
    "text": "Å skrive rapporter\n\nUnngå å forfatte rapporter i punkter/lister. Denne formen av tekst kan hjelpe leseren men kan også bryte opp og skape unødvendig forvirring. En punktliste kan også få spørsmålet «hvorfor» å forsvinne. Leseren kan være interessert i å vite hvorfor man velger en tilnærming eller metode.\nSkriv for en lesere som er kjent med området men savner informasjon om hvorfor man skal lese rapporten. Du må fylle i «blanks» hos leseren!\nRapporter kan med fordel struktureres som introduksjon, metode, resultat og diskusjon. Man kan velge å ikke ha med disse overskriftene men til tross organisere paragrafene på denne måten. Introduksjonen forteller leseren hva teksten skal belyse, metoden gir et innblikk i hva og hvordan du gjennomført arbeidet, resultatene beskriver og diskusjon tolker.\nVær sikker på at begreper og metoder er definerte! «Typical error», «Coefficient of variation», «laktatterskel» osv. kan være mange ulike ting. Man kan bruke en referanse eller en formel for å definere beregninger og konsepter osv.\nHusk å definer feilstapler (error bars), farger, punkter enheter osv. I tabeller, figurer og tekst."
  },
  {
    "objectID": "feedback-assignments.html#å-bruke-r",
    "href": "feedback-assignments.html#å-bruke-r",
    "title": "Sjekkliste for arbeidskrav og eksamen",
    "section": "Å bruke R",
    "text": "Å bruke R\n\nR bruker punktum som kommaseparerare. For en helhetlig rapport kan det være lurt å velge en måte å rapportere resultater på.\nHensikten/styrken med reproduserbar rapportering er at tall, tabeller, figurer osv. er direkte koblet til dataene. Prøv å unngå å skrive inn tall direkte. Bruk isteden variabler (eks. var1) som du skaper i code chunks och settes inn i teksten. R vill i dette fallet lete etter en variable i environment som heter var1. Se her https://rmarkdown.rstudio.com/lesson-4.html for mer info.\nÅ definere innstillinger for code chunks vil gjøre rapportene lettere å lese. F.eks. ved å inkludere koden som beskrives under vil ta vekk meddelende, «warnings» og koden fra din rapport. Hus at innstillinger må settes først i code chunken.\n\n#| echo: false\n#| message: false\n#| warning: false\n\n\n\nUnngå å bruke print() i en rapport da dette vil resultere i uformatert tekst. Bruk istedenfor tabellverktøy (som gt eller knitr::kable()) for tabeller.\nSe her Quarto - Citations & Footnotes for hvordan du kan sette en egen overskrift for referanser og inkludere referanser hvor du ønsker.\n\nFor å bytte ut engelsk Table/Figure mot norsk Tabell/Figur, sett inn\ncrossref:\n  fig-title: \"Figur\"\n  tbl-title: \"Tabell\"\ni YAML feltet."
  },
  {
    "objectID": "feedback-assignments.html#spesifikt-innhold",
    "href": "feedback-assignments.html#spesifikt-innhold",
    "title": "Sjekkliste for arbeidskrav og eksamen",
    "section": "Spesifikt innhold",
    "text": "Spesifikt innhold\n\nArbeidskrav 1\n\nSe til å organiser github-mapper på en overskuelig måte. En metode for å underlette for leseren er å bruke en README.md. Denne vil bli konvertert til HTML på github og kan dermed gi et overblikk over mappen (repository). Hvis du trenger å spare gamle filer så kan disse flyttes til en mappe («archive» eller lignende).\nDet er viktig å vise at du bruker kilder som er angitt i oppgaven. Bruk disse for å vise til definisjoner og gi bakgrunn til hvorfor du ønsker å vise dine resultater til leseren. Bruk funksjonen for bibliography i quarto.\nHva betyr CV, typical error osv. som du beregnet? Her holder det ikke bare med definisjoner, prøv deg også på en tolkning. En måte kan være å bruke annen litteratur som beregnet CV, eller sammenligne med et annet test som du gjort.\nNoen detaljer i en metode kan med fordel holdes kort og kanskje til og med utelates da det tilhører eks. «good laboratory practice». Her kan man spørre seg hvilken informasjon som kreves for å repetere forsøkene gitt at man praktiserer gode rutiner i laboratoriet?\n\n\n\nArbeidskrav 2\n\nAlle gruppene/rapportene er meget sparsomt skrevne. Bruk rapportens deler til å skrive noe om de grafer og analysene som dere presenter. Hvordan kan man tolke laktatterskel? hvorfor predikerer man størrelse på DNA fragmenter? Etc!\nStandard error er spredning i en estimert fordeling av «teoretiske» utvalg. Den forteller ikke hvor bra modellen er, men snarere med hvilken usikkerhet vi bør tolke estimatet. Hvordan skiller denne tolkningen fra deres tolkning?\nT-verdien i en regresjonstabell er en ratio mellom estimat og SE, denne brukes for å beregne p-verdien. P-verdien i sin tur forteller oss om hvor usannsynlig vår data, eller enda mer ekstrem data er hvis nullhypotesen er sann. Hvordan kan dere justere beskrivelsen av regresjonsoutput basert på denne beskrivelsen?\n\n\n\nArbeidskrav 3\nSpørsmål 1-3:\n\nForskjell i SE mellom modellene henger sammen med utvalgsstørrelsen. P-verdien forteller ikke sannsynligheten for at null-hypotesen er sann. Hvordan henger utvalgsstørrelse sammen p-verdien?\nDen skyggete delen av t-fordeling er resultater like eller mer ekstreme enn et observert resultat. T-fordelingen representerer vårt beste estimat av mulige resultater under null-hypotesen.\n\nSpørsmål 4:\n\nHer er del lurt å igjen definere SE! Den gjennomsnittlige standardfeilen er et estimat på spredningen i utvalgsfordelingen! Utvalgsfordelingen er alle beregnede gjennomsnittene!\n\nSpørsmål 5-7:\n\nHvordan kan vi bruke dette for å definere statistisk styrke? Statistisk styrke kan defineres ved hjelp av tankeeksperimentet at man gjennomfører 1000 studier på samme populasjon. Hvor mange vill finne en definert effekt når man setter grensen for p-verdien og utvalgsstørrelsen til et spesifikt nivå?\n\nSpørsmål 8:\n\nNår vi ikke har noen effekt så vil vi finne en effekt i et antall studier basert på den grense vi setter på \\(\\alpha\\). Dette betyr at \\(\\alpha\\) definerer våre antall falske positive fynd og at fordeling av p-verdier ved ingen effekt er uniform, hva betyr dette?\n\n\n\nArbeidskrav 4:\n\nHusk primær målsetting med denne oppgaven: «fokus på design av studiene og valg av statistiske metoder/test for å besvare studienes problemstilling». For å løfte flere av deres tekster kan man tenke at man fokuserer på å sammenligne studiene på disse punktene.\nJeg savner noe informasjon i mange arbeidskrav om studiedesigner i stort, hvordan forholder seg de studier dere har analysert til andre studiedesigner. Her kan tenkes at man gir en generell innføring i studiedesigner i en paragraf for å vise for leseren hvordan man kan forstå de studiene som du har analysert. Det er da også mulig å bruke forslag på pensum!:)\nBruk gjerne også noe plass på å beskrive de statistiske metodene som blir brukt, dette kan med fordel gjøres med henvisning til pensum (eller annen passende litteratur). Har du kjennskap til testene som blir brukt på den nivå at du vet hvilke kommandoen som skal brukes i R for å gjenskape analysene?\nTil tross for at jeg foreslå at man kan bruke QALMRI så sa jeg også at rapporten ikke skulle inneholde QALMRI-tabellen. (Jeg vil ikke underkjenne oppgaver som har QALMRI som bærende struktur).\nTenk på om din tekst kan bli mer strukturert ved å bruke tydeligere temaer per paragraf. Når en paragraf er veldig lang, og inneholder flere ulike temaer blir det vanskelig for leseren å følge. En lettlest paragraf har kanskje 100-200 ord!\nI oppgaven er det lov på kommer med noen meninger om studiedesignene, eller analysene er «gode», hva bør gjøres annerledes for å lage bedre vitenskap. Et eksempel som flere er inne på er statistiske tester innad eksperimentelle grupper, hva sier disse oss?\nEn tabell eller figur er et veldig godt innslag i en rapport. Men den må følges opp i løpende tekst.\n\nArbeidskrav 5\n\nBruk tabeller for det mest sentrale. En kortere liste på eksempelvis øvelser, eksklusjonskriterier osv. kan skrives i løpende tekst. Når man har en mer kompleks fremstilling kan tabellen hjelpe.\nHusk at %-vis endring passer mindre bra for statistiske analyser. Det finnes noen unnatak, når vi log-transformerer så blir tilbake-transformerte data % endring (pga loven om logaritmer, log(a) – log(b) = log(a/b)). Istedenfor % endring i statistiske analyser kan man bruke absolutte tall og eventuelt konvertere til % endring for fremvisning av resultater.\nVed å bruke ordet signifikant så gjør man av tradisjon et utsagn om et statistikk test, vær nøyaktig med hvilken sammenligning som er «signifikant».\nI metodeavsnitt så bør rapporten bare beskrive de deler som rapporten omhandler. Dersom man f.eks. ikke presenterer muskelbiopsidata bør disse ikke presenteres. Tanken her er å lage en rapport som etterligner men ikke er en kopi av originalrapporten!\nStudiedesignen kan stille till problemer da vi måler styrke og muskelvekst innad deltakere. Vi kan bruke mixed-effects models, men også forenkle. Under følger eksempler på modeller for muskelmassedataene (den samme tilnærmingen kan brukes for styrke).\n\n\nlibrary(tidyverse); library(exscidata); library(lme4)\n\n# Load data\ndat &lt;- dxadata %&gt;%\n  select(participant:include, lean.left_leg, lean.right_leg) %&gt;%\n        \n        # Extract leg specific data\n  pivot_longer(names_to = \"leg\", \n               values_to = \"lean.mass\", \n               cols = lean.left_leg:lean.right_leg) %&gt;%\n  mutate(leg = if_else(leg == \"lean.left_leg\", \"L\", \"R\"), \n         sets = if_else(multiple == leg, \"multiple\", \"single\")) %&gt;%\n  select(participant, time, sex, include, sets, leg, lean.mass) %&gt;%\n        # Filter only included participants\n        filter(include == \"incl\") %&gt;%\n        # Make data set wider by time \n        \n        pivot_wider(names_from = \"time\", \n                    values_from = \"lean.mass\") %&gt;%\n        # Calculate change score\n        \n        mutate(change = post - pre) %&gt;%\n        \n        # Keep change score and make it wider by sets\n        select(participant:sets, change) %&gt;%\n        pivot_wider(names_from = sets, values_from = change) %&gt;%\n        \n  print()\n\n# A tibble: 34 × 5\n   participant sex    include multiple single\n   &lt;chr&gt;       &lt;chr&gt;  &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n 1 FP28        female incl         214    123\n 2 FP40        female incl         -69      2\n 3 FP21        male   incl         619    189\n 4 FP34        female incl         396    312\n 5 FP23        male   incl        -205    445\n 6 FP36        female incl         587    386\n 7 FP38        female incl         -85    225\n 8 FP25        male   incl         373    -47\n 9 FP19        male   incl         302    127\n10 FP13        male   incl         734    915\n# ℹ 24 more rows\n\n### Use simple t-test on change score\n\nt.test(dat$multiple, dat$single, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  dat$multiple and dat$single\nt = 2.1875, df = 33, p-value = 0.0359\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n   8.586109 237.002126\nsample estimates:\nmean difference \n       122.7941 \n\n\n\nDenne enkle måten å analysere dataene på er helt OK. Et annet alternativ er en “mixed-effects” modell.\n\n\n# Load data\ndat &lt;- dxadata %&gt;%\n  select(participant:include, lean.left_leg, lean.right_leg) %&gt;%\n        \n        # Extract leg specific data\n  pivot_longer(names_to = \"leg\", \n               values_to = \"lean.mass\", \n               cols = lean.left_leg:lean.right_leg) %&gt;%\n  mutate(leg = if_else(leg == \"lean.left_leg\", \"L\", \"R\"), \n         sets = if_else(multiple == leg, \"multiple\", \"single\")) %&gt;%\n  select(participant, time, sex, include, sets, leg, lean.mass) %&gt;%\n        # Filter only included participants\n        filter(include == \"incl\") %&gt;%\n        # Fix time factor \n        mutate(time = factor(time, levels = c(\"pre\", \"post\"))) %&gt;%\n        \n  print()\n\n# A tibble: 136 × 7\n   participant time  sex    include sets     leg   lean.mass\n   &lt;chr&gt;       &lt;fct&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;     &lt;dbl&gt;\n 1 FP28        pre   female incl    multiple L          7059\n 2 FP28        pre   female incl    single   R          7104\n 3 FP40        pre   female incl    single   L          7190\n 4 FP40        pre   female incl    multiple R          7506\n 5 FP21        pre   male   incl    single   L         10281\n 6 FP21        pre   male   incl    multiple R         10200\n 7 FP34        pre   female incl    single   L          6014\n 8 FP34        pre   female incl    multiple R          6009\n 9 FP23        pre   male   incl    single   L          8242\n10 FP23        pre   male   incl    multiple R          8685\n# ℹ 126 more rows\n\n### Use a mixed model to determine effects of time and condition\n\nm &lt;- lmer(lean.mass ~ time + time:sets + (1|participant), \n          data = dat)\n\n\nsummary(m)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: lean.mass ~ time + time:sets + (1 | participant)\n   Data: dat\n\nREML criterion at convergence: 2022.2\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.72697 -0.60997 -0.03598  0.46355  2.78820 \n\nRandom effects:\n Groups      Name        Variance Std.Dev.\n participant (Intercept) 4181094  2044.8  \n Residual                  57202   239.2  \nNumber of obs: 136, groups:  participant, 34\n\nFixed effects:\n                    Estimate Std. Error t value\n(Intercept)          8603.53     353.07  24.368\ntimepost              289.06      58.01   4.983\ntimepre:setssingle    -14.53      58.01  -0.250\ntimepost:setssingle  -137.32      58.01  -2.367\n\nCorrelation of Fixed Effects:\n            (Intr) timpst tmpr:s\ntimepost    -0.082              \ntmpr:stssng -0.082  0.500       \ntmpst:stssn  0.000 -0.500  0.000\n\nconfint(m)\n\n                        2.5 %     97.5 %\n.sig01              1614.7219 2609.47559\n.sigma               206.6486  272.06561\n(Intercept)         7902.4449 9304.61392\ntimepost             175.9888  402.12889\ntimepre:setssingle  -127.5995   98.54066\ntimepost:setssingle -250.3936  -24.25346\n\n\n\nI modellen over gis ingen korreksjon for “baseline” i sammenligningen post. Dette er en legitim måte å analysere dataene på når vi kan anta at forskjeller ved baseline bare er målefeil.\nTil sist kan vi prøve en ANCOVA, denne må og ta hensyn til at dataene kommer fra samme individ to ganger\n\n\n# Load data\ndat &lt;- dxadata %&gt;%\n  select(participant:include, lean.left_leg, lean.right_leg) %&gt;%\n        \n        # Extract leg specific data\n  pivot_longer(names_to = \"leg\", \n               values_to = \"lean.mass\", \n               cols = lean.left_leg:lean.right_leg) %&gt;%\n  mutate(leg = if_else(leg == \"lean.left_leg\", \"L\", \"R\"), \n         sets = if_else(multiple == leg, \"multiple\", \"single\")) %&gt;%\n  select(participant, time, sex, include, sets, leg, lean.mass) %&gt;%\n        # Filter only included participants\n        filter(include == \"incl\") %&gt;%\n        # Fix time factor \n        mutate(time = factor(time, levels = c(\"pre\", \"post\"))) %&gt;%\n        # Pivot wider by time\n        pivot_wider(names_from = time, \n                    values_from = lean.mass) %&gt;%\n        \n  print()\n\n# A tibble: 68 × 7\n   participant sex    include sets     leg     pre  post\n   &lt;chr&gt;       &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 FP28        female incl    multiple L      7059  7273\n 2 FP28        female incl    single   R      7104  7227\n 3 FP40        female incl    single   L      7190  7192\n 4 FP40        female incl    multiple R      7506  7437\n 5 FP21        male   incl    single   L     10281 10470\n 6 FP21        male   incl    multiple R     10200 10819\n 7 FP34        female incl    single   L      6014  6326\n 8 FP34        female incl    multiple R      6009  6405\n 9 FP23        male   incl    single   L      8242  8687\n10 FP23        male   incl    multiple R      8685  8480\n# ℹ 58 more rows\n\n### Use a mixed model to determine effects of time and condition\n\nm &lt;- lmer(post ~ pre + sets + (1|participant), \n          data = dat)\n\n\nsummary(m)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: post ~ pre + sets + (1 | participant)\n   Data: dat\n\nREML criterion at convergence: 966.1\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.2720 -0.5705  0.0531  0.4515  1.5658 \n\nRandom effects:\n Groups      Name        Variance Std.Dev.\n participant (Intercept) 90183    300.3   \n Residual                53840    232.0   \nNumber of obs: 68, groups:  participant, 34\n\nFixed effects:\n              Estimate Std. Error t value\n(Intercept)  230.12571  260.08567   0.885\npre            1.00685    0.02927  34.401\nsetssingle  -122.69459   56.27822  -2.180\n\nCorrelation of Fixed Effects:\n           (Intr) pre   \npre        -0.968       \nsetssingle -0.116  0.008\nfit warnings:\nSome predictor variables are on very different scales: consider rescaling\n\nconfint(m)\n\n                   2.5 %     97.5 %\n.sig01       203.3519123 399.741610\n.sigma       183.2295834 296.089498\n(Intercept) -274.8628148 749.191873\npre            0.9483379   1.063678\nsetssingle  -234.5001167 -10.850906\n\n\n\nlme4 gir oss ikke p-verdier. Men vi kan bruke confint() for å beregne konfidensintervaller. Disse kan brukes får inferens.\nMan kan være intressert i å få ut estimate fra modellene, her kan pakken emmeans hjelpe. La si at du ønsker å estimere post-skår fra modellen m.\n\n\nlibrary(emmeans)\n\n# store estimated marginal means in new object\nem &lt;- emmeans(m, specs = ~ sets)\n\n# For plotting you can make the estimates to a data frame\ndata.frame(em)\n\n      sets   emmean       SE       df lower.CL upper.CL\n1 multiple 8885.274 65.08461 45.88236 8754.256 9016.291\n2   single 8762.579 65.08461 45.88236 8631.562 8893.597\n\n\n\n\nReproduserbare rapporter på Github\n\nMed en reproduserbar rapport mener jeg at tekst, kode og data blir brukt til å skape et «output» som en pdf-, html- eller word-fil. Fordelen med github (og lignende løsninger) er at man har mulighet å samle alle delene i et versjonskontrollsystem. Dette har fordeler for deg som forfatter og for vitenskapelig arbeid i stort (transparens, muligheter å reprodusere osv.). Hensikten med å bruke Github i emnet er å øve på denne måten å lage rapporter.\n\nMed det sagt vil jeg ikke underkjenne rapporter som ikke er reproduserbar i denne betydningen. - Vær konsekvent med filstruktur, filnavn osv på github. Filer som ikke blir brukt bør f.eks. flyttes til en annen mappe.\n\nKontroller at din mappe går å laste ned fra github, at den seneste versjonen av prosjektet ligger oppe osv!"
  },
  {
    "objectID": "feedback-assignments.html#innlevering-av-eksamen",
    "href": "feedback-assignments.html#innlevering-av-eksamen",
    "title": "Sjekkliste for arbeidskrav og eksamen",
    "section": "Innlevering av eksamen",
    "text": "Innlevering av eksamen\n\nEksamen skal leveres som en pdf på inspera.\nI pdf:en skal det stå beskrevet hvor man kan finne data og kod (beskriv dette under preface i mallen).\nJeg har skapt en mal for innlevering av eksamen, for å bruke denne:\n- Lag en `fork` av `https://github.com/dhammarstrom/innlevering-idr4000-qmd` til din egen github bruker.\n- Last ned din `fork` til R Studio og oppdatere qmd-filene med dine tekster og repository med dine data.\n- For å lage pdf:en, tryck på render. Det kan kreves flere render for å løse problemer som oppstår. Se til å levere den endlige versjonen!\n- Den pdf som skapes finner du i mappen `_book`\n- Legg til endringer (gi add -A) og commit (git commit -m ‘a message’) og push (git push). Din versjon av innleveringsmappen er nå oppdatert på din github profil\nFor å bruke denne løsningen kreves at du har quarto installert (https://quarto.org/docs/get-started/).\nDet kreves også en installasjon av TeX, quarto sier at TinyTeX er et godt alternativ (se https://quarto.org/docs/output-formats/pdf-engine.html)\nDu kan være interessert av å forandre automatisk generert tekst i rapporten, eks. figurtekster, overskrifter. Se i .yml-filen for å endre disse innstillingene."
  },
  {
    "objectID": "assignment-5.html",
    "href": "assignment-5.html",
    "title": "Assignment 5: Analyzing repeated measures experiments",
    "section": "",
    "text": "In this assignment you will analyze and report on trial investigating the effect of resistance training volume on lean mass and muscle strength. The data are part of the exscidata package and can be accessed as data(\"strengthvolume\") and data(\"dxadata\"). Read the instructions carefully!\n\n\nYour report should consist of the sections Introduction, Methods, Results and Discussion. Each part of the report should be written as a reproducible document and a link or reference to the repository containing the source document(s) should be included in the report. Below follows detailed descriptions and requirements for each section.\n\n\nThis section should consist of a description of the field, resistance-training volume and muscle strength and mass. Use at least five to ten references to introduce your audience and explain why you are doing the analysis/study. A tip is to use the QALMRI method, introduced in Assignment 4 to structure the reading of background information. It is up to you how you motivate the study and how you phrase the purpose of the study. It could be a hypothesis based on previous studies, it could also be question to fill a knowledge gap that you have identified in your literature review.\nStructure the introduction in paragraphs. A first paragraph could contain a general introduction to the field, why is it of interest to investigate resistance-training? A second paragraph could specifically describe the specific field of resistance-training volume, why is important to know more about how we are likely to respond to different training volumes. The second paragraph should incorporate definitions important for your report, e.g., training volume, muscle mass and strength. Try to incorporate these definition as a fluid part of the text.\nA third (or last) paragraph of the introduction should contain a statement regarding the purpose of the study. The purpose could be descriptive, hypothesis-driven or guided by a question. Although it could be considered a bit backward, you should explore the data sets before you select your question/hypothesis/purpose for it to be possible to answer.\n\n\n\nThe method should give a thorough overview of the study and specific details regarding data collection. You can read about the details of this specific study in (Hammarström et al. 2020). Use your own words to describe the study based on this description. A nice way to structure the methods section is to include subheadings:\n\nParticipants and study overview: Describe the participants and give an overview of all tests/measurements. Participants should be described in the first table of the report (Table 1). The overview of the tests/measurements should be done without double presentation as details should be presented in subsequent sections.\nSpecific descriptions (e.g. strength tests): Describe in detail how tests/measurements that you mentioned in the overview where conducted.\nData analysis and statistics: Describe how you treated the data prior to statistical tests or procedures and what tests/procedures were used to draw inference (or more generally, to answer your purpose). Describe how you present data (e.g. descriptive data with mean (SD), inference with confidence intervals etc.).\n\n\n\n\nDescribe the results of your analysis. This description should make use of table and figures as well as a text that guides and structures the content to the reader. Think about it this way, the text should describe when and how to read the figures and tables. This means that all aspects of the results should be covered in the text. The figures and tables should also be “self explanatory”, this means that you have to include descriptive figure captions and descriptions of tables (see below for tips).\nAs the main purpose of the analysis should concern the effect of training volume on muscle mass and strength, it is natural that the comparison of training outcomes between volume conditions is the main analysis in the results. You may also have questions regarding the relationship between muscle strength and mass gains, if there are differences between men and women etc. Selection of statistical/analysis techniques should reflect the study question/purpose.\n\n\n\nStructure the discussion with a first paragraph describing the main results of the analysis, this could be the answer to your question or a statement regarding the study hypothesis. In the following paragraphs discuss all results that you have presented in the light of previous studies. It is your job to give the reader plausible interpretations and explanations of your results. This is how single scientific results are incorporated in our collective understanding. These interpretations can later be challenged, however if you give the reader good arguments and clear descriptions, your insights will be valuable to collective reasoning even if they turn out to be wrong in light of new data.\nEnd the discussion with a summary or conclusion. Some journals request that you state your conclusions under a specific heading in the end of the report/article.\n\n\n\n\n\n\nThe data is already structured in the exscidata package. To access the data, use the following code:\n\nlibrary(exscidata)\ndata(\"dxadata\"); data(\"strengthvolume\")\n\nTo get and overview of the variables in each data set use ?strengthvolume and ?dxadata. In the dxadata the variables of interest are organized in a more convenient way using the code below:\n\nlibrary(tidyverse)\n\ndxadata %&gt;%\n  select(participant:include, lean.left_leg, lean.right_leg) %&gt;%\n  pivot_longer(names_to = \"leg\", \n               values_to = \"lean.mass\", \n               cols = lean.left_leg:lean.right_leg) %&gt;%\n  mutate(leg = if_else(leg == \"lean.left_leg\", \"L\", \"R\"), \n         sets = if_else(multiple == leg, \"multiple\", \"single\")) %&gt;%\n  select(participant, time, sex, include, sets, leg, lean.mass) %&gt;%\n  print()\n\n# A tibble: 160 × 7\n   participant time  sex    include sets     leg   lean.mass\n   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;     &lt;dbl&gt;\n 1 FP28        pre   female incl    multiple L          7059\n 2 FP28        pre   female incl    single   R          7104\n 3 FP40        pre   female incl    single   L          7190\n 4 FP40        pre   female incl    multiple R          7506\n 5 FP21        pre   male   incl    single   L         10281\n 6 FP21        pre   male   incl    multiple R         10200\n 7 FP34        pre   female incl    single   L          6014\n 8 FP34        pre   female incl    multiple R          6009\n 9 FP23        pre   male   incl    single   L          8242\n10 FP23        pre   male   incl    multiple R          8685\n# ℹ 150 more rows"
  },
  {
    "objectID": "assignment-5.html#organizing-the-report",
    "href": "assignment-5.html#organizing-the-report",
    "title": "Assignment 5: Analyzing repeated measures experiments",
    "section": "",
    "text": "Your report should consist of the sections Introduction, Methods, Results and Discussion. Each part of the report should be written as a reproducible document and a link or reference to the repository containing the source document(s) should be included in the report. Below follows detailed descriptions and requirements for each section.\n\n\nThis section should consist of a description of the field, resistance-training volume and muscle strength and mass. Use at least five to ten references to introduce your audience and explain why you are doing the analysis/study. A tip is to use the QALMRI method, introduced in Assignment 4 to structure the reading of background information. It is up to you how you motivate the study and how you phrase the purpose of the study. It could be a hypothesis based on previous studies, it could also be question to fill a knowledge gap that you have identified in your literature review.\nStructure the introduction in paragraphs. A first paragraph could contain a general introduction to the field, why is it of interest to investigate resistance-training? A second paragraph could specifically describe the specific field of resistance-training volume, why is important to know more about how we are likely to respond to different training volumes. The second paragraph should incorporate definitions important for your report, e.g., training volume, muscle mass and strength. Try to incorporate these definition as a fluid part of the text.\nA third (or last) paragraph of the introduction should contain a statement regarding the purpose of the study. The purpose could be descriptive, hypothesis-driven or guided by a question. Although it could be considered a bit backward, you should explore the data sets before you select your question/hypothesis/purpose for it to be possible to answer.\n\n\n\nThe method should give a thorough overview of the study and specific details regarding data collection. You can read about the details of this specific study in (Hammarström et al. 2020). Use your own words to describe the study based on this description. A nice way to structure the methods section is to include subheadings:\n\nParticipants and study overview: Describe the participants and give an overview of all tests/measurements. Participants should be described in the first table of the report (Table 1). The overview of the tests/measurements should be done without double presentation as details should be presented in subsequent sections.\nSpecific descriptions (e.g. strength tests): Describe in detail how tests/measurements that you mentioned in the overview where conducted.\nData analysis and statistics: Describe how you treated the data prior to statistical tests or procedures and what tests/procedures were used to draw inference (or more generally, to answer your purpose). Describe how you present data (e.g. descriptive data with mean (SD), inference with confidence intervals etc.).\n\n\n\n\nDescribe the results of your analysis. This description should make use of table and figures as well as a text that guides and structures the content to the reader. Think about it this way, the text should describe when and how to read the figures and tables. This means that all aspects of the results should be covered in the text. The figures and tables should also be “self explanatory”, this means that you have to include descriptive figure captions and descriptions of tables (see below for tips).\nAs the main purpose of the analysis should concern the effect of training volume on muscle mass and strength, it is natural that the comparison of training outcomes between volume conditions is the main analysis in the results. You may also have questions regarding the relationship between muscle strength and mass gains, if there are differences between men and women etc. Selection of statistical/analysis techniques should reflect the study question/purpose.\n\n\n\nStructure the discussion with a first paragraph describing the main results of the analysis, this could be the answer to your question or a statement regarding the study hypothesis. In the following paragraphs discuss all results that you have presented in the light of previous studies. It is your job to give the reader plausible interpretations and explanations of your results. This is how single scientific results are incorporated in our collective understanding. These interpretations can later be challenged, however if you give the reader good arguments and clear descriptions, your insights will be valuable to collective reasoning even if they turn out to be wrong in light of new data.\nEnd the discussion with a summary or conclusion. Some journals request that you state your conclusions under a specific heading in the end of the report/article."
  },
  {
    "objectID": "assignment-5.html#organizing-the-data-analysis",
    "href": "assignment-5.html#organizing-the-data-analysis",
    "title": "Assignment 5: Analyzing repeated measures experiments",
    "section": "",
    "text": "The data is already structured in the exscidata package. To access the data, use the following code:\n\nlibrary(exscidata)\ndata(\"dxadata\"); data(\"strengthvolume\")\n\nTo get and overview of the variables in each data set use ?strengthvolume and ?dxadata. In the dxadata the variables of interest are organized in a more convenient way using the code below:\n\nlibrary(tidyverse)\n\ndxadata %&gt;%\n  select(participant:include, lean.left_leg, lean.right_leg) %&gt;%\n  pivot_longer(names_to = \"leg\", \n               values_to = \"lean.mass\", \n               cols = lean.left_leg:lean.right_leg) %&gt;%\n  mutate(leg = if_else(leg == \"lean.left_leg\", \"L\", \"R\"), \n         sets = if_else(multiple == leg, \"multiple\", \"single\")) %&gt;%\n  select(participant, time, sex, include, sets, leg, lean.mass) %&gt;%\n  print()\n\n# A tibble: 160 × 7\n   participant time  sex    include sets     leg   lean.mass\n   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;     &lt;dbl&gt;\n 1 FP28        pre   female incl    multiple L          7059\n 2 FP28        pre   female incl    single   R          7104\n 3 FP40        pre   female incl    single   L          7190\n 4 FP40        pre   female incl    multiple R          7506\n 5 FP21        pre   male   incl    single   L         10281\n 6 FP21        pre   male   incl    multiple R         10200\n 7 FP34        pre   female incl    single   L          6014\n 8 FP34        pre   female incl    multiple R          6009\n 9 FP23        pre   male   incl    single   L          8242\n10 FP23        pre   male   incl    multiple R          8685\n# ℹ 150 more rows"
  },
  {
    "objectID": "assignment-3.html",
    "href": "assignment-3.html",
    "title": "Assignment 3: Drawing inference from statistical models, and statistical power",
    "section": "",
    "text": "This assignment is set up as a statistical laboratory, we will perform simulations and your assignment is to interpret and explain the results. Create a report based on the code used in the lab and make sure you answer the specified questions (1-8). You can be as creative as you want and explore the results further.\nThe report should be handed in on canvas as a link to github repository containing a reproducible .Rmd (or qmd) file.\n\n\nIn this assignment we will simulate a population of possible values, from this population we will draw random samples, calculate statistics and interpret them. The population of values can be regarded as the possible differences between two treatments in a cross-over study where participants have performed both treatments. The values in the population are calculate as \\(Treatment - Control\\).\nWe will simulate a population of one million numbers with a mean of 1.5 and a standard deviation of 3. We will make two different set of studies, one set with a sample size of 8 and one set with a sample size of 40. In order to be sure you replicate your results, include and run set.seed() before simulations in your final script.\nWe will use the lm function to estimate the average value of the population. We do this in an “intercept-only” model. This model can be written as\n\\[Y_i = \\beta_0 + \\epsilon_i\\]\nwhere \\(\\beta_0\\) is the intercept and can be interpreted as the average value of \\(Y\\), our dependent variable. \\(\\epsilon\\) is the error term, each observation (\\(i\\)) deviates from the intercept to some degree. If the intercept term is positive or negative we can interpret it as a difference between the two treatments (described above). This model is equivalent to a one-sample t-test. Let’s get started!\nIn the code chunk below, we will simulate the population of differences between treatments. We will then draw two random samples corresponding sample sizes of 8 and 40 and save these data in data frames with the dependent variable named y. We fit the very simple model y ~ 1 as a linear model and save the model object as m1 and m2.\n\nlibrary(tidyverse)\n\nset.seed(1)\npopulation &lt;- rnorm(1000000, mean = 1.5, sd = 3)\n\n\nsamp1 &lt;- data.frame(y = sample(population, 8, replace = FALSE))\n\nsamp2 &lt;- data.frame(y = sample(population, 40, replace = FALSE))\n\n\nm1 &lt;- lm(y ~ 1, data = samp1)\nm2 &lt;- lm(y ~ 1, data = samp2)\n\nsummary(m1)\n\n\nCall:\nlm(formula = y ~ 1, data = samp1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.5322 -1.2523 -0.0883  1.3540  4.8692 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)    1.840      1.251    1.47    0.185\n\nResidual standard error: 3.539 on 7 degrees of freedom\n\n\nThe results from a simple model can be calculated by hand. The Estimate corresponds to the average of all values in the sample, from the smaller sample, samp1 we can do mean(samp1$y). This average should correspond to coef(m1) which should be 1.84. The variation of the data is most often described with the standard deviation (SD). The SD of y in the smaller sample is sd(samp1$y) (corresponding to 3.539). However, the regression table (summary(m1)) show you the standard error (SE). This statistic is an attempt to estimate the variation in a hypothetical distribution of means. The standard error is (in this simple case) \\(SE_y = \\frac{SD_y}{\\sqrt{n}}\\). Calculating by hand using the data in samp1 we would do sd(samp1$y)/sqrt(8). Amazingly this corresponds to 1.251!\nBy using the estimate 1.84 and the corresponding SE (1.251) we can calculate the t-value as the ratio \\(\\frac{Estimate}{SE}\\). The t-value may in turn be used to determine the are under the curve of a t-distribution. The t-value from the above calculation is 1.4702611. Using our single \\(n=8\\) study, we estimate that values of t, as extreme or even more extreme as our observed value both above and below 0, would occur in 18.5% of studies if the null-hypothesis was true. This corresponds to a p-value of 0.185. The figure below shows a graphical representation of a t-value distribution under the assumption that the null-hypothesis is true.\n\n\n\n\n\nA t-distribution estimated from model m1 with the shaded area corresponding to the observed p-value.\n\n\n\n\n\nIn light of what you know now about the process of conducting a study with a random sample, use your own words and…\n\nExplain the estimate, SE, t-value, and p-value from the regression models that we created previously (m1 and m2).\nDiscuss what contributes to the different results in the two studies (m1 and m2).\nWhy do we use the shaded area in the lower and upper tail of the t-distribution (See Figure @ref(fig:t-dist-fig)).\n\n\n\n\n\nBelow we will perform 1000 studies and save the results from each study. This will make it possible for us to get an actual sampling distribution. Copy the code to your own document to run the experiment.\n\n# Create data frames to store the model estimates\nresults_8 &lt;- data.frame(estimate = rep(NA, 1000), \n                      se = rep(NA, 1000), \n                      pval = rep(NA, 1000), \n                      n = 8)  \n\nresults_40 &lt;- data.frame(estimate = rep(NA, 1000), \n                      se = rep(NA, 1000), \n                      pval = rep(NA, 1000), \n                      n = 40)\n\n# A for loop used to sample 1000 studies, each iteration (i) will draw a new sample\n# from the population. \n\nfor(i in 1:1000) {\n  \n  # Draw a sample \n  samp1 &lt;- data.frame(y = sample(population, 8, replace = FALSE))\n  samp2 &lt;- data.frame(y = sample(population, 40, replace = FALSE))\n\n  # Model the data\n  m1 &lt;- lm(y ~ 1, data = samp1)\n  m2 &lt;- lm(y ~ 1, data = samp2)\n  \n  # Extract values from the models\n  results_8[i, 1] &lt;- coef(summary(m1))[1, 1]\n  results_8[i, 2] &lt;- coef(summary(m1))[1, 2]\n  results_8[i, 3] &lt;- coef(summary(m1))[1, 4]\n\n  results_40[i, 1] &lt;- coef(summary(m2))[1, 1]\n  results_40[i, 2] &lt;- coef(summary(m2))[1, 2]\n  results_40[i, 3] &lt;- coef(summary(m2))[1, 4]\n  \n  \n}\n\n\n# Save the results in a combined data frame\n\nresults &lt;- bind_rows(results_8, results_40)\n\n\nUsing the results data frame…\n\nCalculate the standard deviation of the estimate variable, and the average of the se variable for each of the study sample sizes (8 and 40). Explain why these numbers are very similar. How can you define the Standard Error (SE) in light of these calculations?\nCreate a histogram (see example code below) of the p-values from each study sample-size. How do you interpret these histograms, what do they tell you about the effect of sample size on statistical power?\nCalculate the number of studies from each sample size that declare a statistical significant effect (specify a threshold for \\(\\alpha\\), your significance level).\nUsing the pwr package, calculate the power of a one-sample t-test, with a effect size of 1.5/3, your specified significance level and sample sizes 8 and 40. Explain the results in the light of your simulations.\n\n\n\n# Example code for copy and paste\n\n# A two facets histogram can be created with ggplot2\nresults %&gt;%\n  ggplot(aes(pval)) + \n  geom_histogram() +\n  facet_wrap(~ n)\n\n\n# Count the proportion of tests below a certain p-value for each \nresults %&gt;%\n  filter(pval &lt; 0.05) %&gt;%\n  group_by(n) %&gt;%\n  summarise(sig_results = n()/1000)\n\n# Using the pwr package\nlibrary(pwr)\n\npwr.t.test(n = 40, sig.level = 0.05, d = 1.5/3, type = \"one.sample\")\n\n\n\n\nWe will now simulate a population without differences between treatment and control. The code below is very similar to the one we use above, except that we use an average effect of 0 in the population.\n\npopulation &lt;- rnorm(1000000, mean = 0, sd = 3)\n\n\n# Create data frames to store the model estimates\nresults_8 &lt;- data.frame(estimate = rep(NA, 1000), \n                      se = rep(NA, 1000), \n                      pval = rep(NA, 1000), \n                      n = 8)  \n\nresults_40 &lt;- data.frame(estimate = rep(NA, 1000), \n                      se = rep(NA, 1000), \n                      pval = rep(NA, 1000), \n                      n = 40)\n\n# A for loop used to sample 1000 studies, each iteration (i) will draw a new sample\n# from the population. \n\nfor(i in 1:1000) {\n  \n  # Draw a sample \n  samp1 &lt;- data.frame(y = sample(population, 8, replace = FALSE))\n  samp2 &lt;- data.frame(y = sample(population, 40, replace = FALSE))\n\n  # Model the data\n  m1 &lt;- lm(y ~ 1, data = samp1)\n  m2 &lt;- lm(y ~ 1, data = samp2)\n  \n  # Extract values from the models\n  results_8[i, 1] &lt;- coef(summary(m1))[1, 1]\n  results_8[i, 2] &lt;- coef(summary(m1))[1, 2]\n  results_8[i, 3] &lt;- coef(summary(m1))[1, 4]\n\n  results_40[i, 1] &lt;- coef(summary(m2))[1, 1]\n  results_40[i, 2] &lt;- coef(summary(m2))[1, 2]\n  results_40[i, 3] &lt;- coef(summary(m2))[1, 4]\n  \n  \n}\n\n\n# Save the results in a combined data frame\n\nresults_null &lt;- bind_rows(results_8, results_40)\n\n\nUsing the new data frame with results from studies of a population with an average effect of zero, create new histograms.\n\nWith a significance level of 5%, how many studies would give you a “false positive” result if you did many repeated studies?"
  },
  {
    "objectID": "assignment-3.html#setting-up-a-simulation",
    "href": "assignment-3.html#setting-up-a-simulation",
    "title": "Assignment 3: Drawing inference from statistical models, and statistical power",
    "section": "",
    "text": "In this assignment we will simulate a population of possible values, from this population we will draw random samples, calculate statistics and interpret them. The population of values can be regarded as the possible differences between two treatments in a cross-over study where participants have performed both treatments. The values in the population are calculate as \\(Treatment - Control\\).\nWe will simulate a population of one million numbers with a mean of 1.5 and a standard deviation of 3. We will make two different set of studies, one set with a sample size of 8 and one set with a sample size of 40. In order to be sure you replicate your results, include and run set.seed() before simulations in your final script.\nWe will use the lm function to estimate the average value of the population. We do this in an “intercept-only” model. This model can be written as\n\\[Y_i = \\beta_0 + \\epsilon_i\\]\nwhere \\(\\beta_0\\) is the intercept and can be interpreted as the average value of \\(Y\\), our dependent variable. \\(\\epsilon\\) is the error term, each observation (\\(i\\)) deviates from the intercept to some degree. If the intercept term is positive or negative we can interpret it as a difference between the two treatments (described above). This model is equivalent to a one-sample t-test. Let’s get started!\nIn the code chunk below, we will simulate the population of differences between treatments. We will then draw two random samples corresponding sample sizes of 8 and 40 and save these data in data frames with the dependent variable named y. We fit the very simple model y ~ 1 as a linear model and save the model object as m1 and m2.\n\nlibrary(tidyverse)\n\nset.seed(1)\npopulation &lt;- rnorm(1000000, mean = 1.5, sd = 3)\n\n\nsamp1 &lt;- data.frame(y = sample(population, 8, replace = FALSE))\n\nsamp2 &lt;- data.frame(y = sample(population, 40, replace = FALSE))\n\n\nm1 &lt;- lm(y ~ 1, data = samp1)\nm2 &lt;- lm(y ~ 1, data = samp2)\n\nsummary(m1)\n\n\nCall:\nlm(formula = y ~ 1, data = samp1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.5322 -1.2523 -0.0883  1.3540  4.8692 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)    1.840      1.251    1.47    0.185\n\nResidual standard error: 3.539 on 7 degrees of freedom\n\n\nThe results from a simple model can be calculated by hand. The Estimate corresponds to the average of all values in the sample, from the smaller sample, samp1 we can do mean(samp1$y). This average should correspond to coef(m1) which should be 1.84. The variation of the data is most often described with the standard deviation (SD). The SD of y in the smaller sample is sd(samp1$y) (corresponding to 3.539). However, the regression table (summary(m1)) show you the standard error (SE). This statistic is an attempt to estimate the variation in a hypothetical distribution of means. The standard error is (in this simple case) \\(SE_y = \\frac{SD_y}{\\sqrt{n}}\\). Calculating by hand using the data in samp1 we would do sd(samp1$y)/sqrt(8). Amazingly this corresponds to 1.251!\nBy using the estimate 1.84 and the corresponding SE (1.251) we can calculate the t-value as the ratio \\(\\frac{Estimate}{SE}\\). The t-value may in turn be used to determine the are under the curve of a t-distribution. The t-value from the above calculation is 1.4702611. Using our single \\(n=8\\) study, we estimate that values of t, as extreme or even more extreme as our observed value both above and below 0, would occur in 18.5% of studies if the null-hypothesis was true. This corresponds to a p-value of 0.185. The figure below shows a graphical representation of a t-value distribution under the assumption that the null-hypothesis is true.\n\n\n\n\n\nA t-distribution estimated from model m1 with the shaded area corresponding to the observed p-value.\n\n\n\n\n\nIn light of what you know now about the process of conducting a study with a random sample, use your own words and…\n\nExplain the estimate, SE, t-value, and p-value from the regression models that we created previously (m1 and m2).\nDiscuss what contributes to the different results in the two studies (m1 and m2).\nWhy do we use the shaded area in the lower and upper tail of the t-distribution (See Figure @ref(fig:t-dist-fig))."
  },
  {
    "objectID": "assignment-3.html#many-studies",
    "href": "assignment-3.html#many-studies",
    "title": "Assignment 3: Drawing inference from statistical models, and statistical power",
    "section": "",
    "text": "Below we will perform 1000 studies and save the results from each study. This will make it possible for us to get an actual sampling distribution. Copy the code to your own document to run the experiment.\n\n# Create data frames to store the model estimates\nresults_8 &lt;- data.frame(estimate = rep(NA, 1000), \n                      se = rep(NA, 1000), \n                      pval = rep(NA, 1000), \n                      n = 8)  \n\nresults_40 &lt;- data.frame(estimate = rep(NA, 1000), \n                      se = rep(NA, 1000), \n                      pval = rep(NA, 1000), \n                      n = 40)\n\n# A for loop used to sample 1000 studies, each iteration (i) will draw a new sample\n# from the population. \n\nfor(i in 1:1000) {\n  \n  # Draw a sample \n  samp1 &lt;- data.frame(y = sample(population, 8, replace = FALSE))\n  samp2 &lt;- data.frame(y = sample(population, 40, replace = FALSE))\n\n  # Model the data\n  m1 &lt;- lm(y ~ 1, data = samp1)\n  m2 &lt;- lm(y ~ 1, data = samp2)\n  \n  # Extract values from the models\n  results_8[i, 1] &lt;- coef(summary(m1))[1, 1]\n  results_8[i, 2] &lt;- coef(summary(m1))[1, 2]\n  results_8[i, 3] &lt;- coef(summary(m1))[1, 4]\n\n  results_40[i, 1] &lt;- coef(summary(m2))[1, 1]\n  results_40[i, 2] &lt;- coef(summary(m2))[1, 2]\n  results_40[i, 3] &lt;- coef(summary(m2))[1, 4]\n  \n  \n}\n\n\n# Save the results in a combined data frame\n\nresults &lt;- bind_rows(results_8, results_40)\n\n\nUsing the results data frame…\n\nCalculate the standard deviation of the estimate variable, and the average of the se variable for each of the study sample sizes (8 and 40). Explain why these numbers are very similar. How can you define the Standard Error (SE) in light of these calculations?\nCreate a histogram (see example code below) of the p-values from each study sample-size. How do you interpret these histograms, what do they tell you about the effect of sample size on statistical power?\nCalculate the number of studies from each sample size that declare a statistical significant effect (specify a threshold for \\(\\alpha\\), your significance level).\nUsing the pwr package, calculate the power of a one-sample t-test, with a effect size of 1.5/3, your specified significance level and sample sizes 8 and 40. Explain the results in the light of your simulations.\n\n\n\n# Example code for copy and paste\n\n# A two facets histogram can be created with ggplot2\nresults %&gt;%\n  ggplot(aes(pval)) + \n  geom_histogram() +\n  facet_wrap(~ n)\n\n\n# Count the proportion of tests below a certain p-value for each \nresults %&gt;%\n  filter(pval &lt; 0.05) %&gt;%\n  group_by(n) %&gt;%\n  summarise(sig_results = n()/1000)\n\n# Using the pwr package\nlibrary(pwr)\n\npwr.t.test(n = 40, sig.level = 0.05, d = 1.5/3, type = \"one.sample\")"
  },
  {
    "objectID": "assignment-3.html#many-studies-without-population-effect",
    "href": "assignment-3.html#many-studies-without-population-effect",
    "title": "Assignment 3: Drawing inference from statistical models, and statistical power",
    "section": "",
    "text": "We will now simulate a population without differences between treatment and control. The code below is very similar to the one we use above, except that we use an average effect of 0 in the population.\n\npopulation &lt;- rnorm(1000000, mean = 0, sd = 3)\n\n\n# Create data frames to store the model estimates\nresults_8 &lt;- data.frame(estimate = rep(NA, 1000), \n                      se = rep(NA, 1000), \n                      pval = rep(NA, 1000), \n                      n = 8)  \n\nresults_40 &lt;- data.frame(estimate = rep(NA, 1000), \n                      se = rep(NA, 1000), \n                      pval = rep(NA, 1000), \n                      n = 40)\n\n# A for loop used to sample 1000 studies, each iteration (i) will draw a new sample\n# from the population. \n\nfor(i in 1:1000) {\n  \n  # Draw a sample \n  samp1 &lt;- data.frame(y = sample(population, 8, replace = FALSE))\n  samp2 &lt;- data.frame(y = sample(population, 40, replace = FALSE))\n\n  # Model the data\n  m1 &lt;- lm(y ~ 1, data = samp1)\n  m2 &lt;- lm(y ~ 1, data = samp2)\n  \n  # Extract values from the models\n  results_8[i, 1] &lt;- coef(summary(m1))[1, 1]\n  results_8[i, 2] &lt;- coef(summary(m1))[1, 2]\n  results_8[i, 3] &lt;- coef(summary(m1))[1, 4]\n\n  results_40[i, 1] &lt;- coef(summary(m2))[1, 1]\n  results_40[i, 2] &lt;- coef(summary(m2))[1, 2]\n  results_40[i, 3] &lt;- coef(summary(m2))[1, 4]\n  \n  \n}\n\n\n# Save the results in a combined data frame\n\nresults_null &lt;- bind_rows(results_8, results_40)\n\n\nUsing the new data frame with results from studies of a population with an average effect of zero, create new histograms.\n\nWith a significance level of 5%, how many studies would give you a “false positive” result if you did many repeated studies?"
  },
  {
    "objectID": "assignment-1.html",
    "href": "assignment-1.html",
    "title": "Assignment 1: Reliability and tools for reproducible data science",
    "section": "",
    "text": "The purpose of this assignment is to present estimates of reliability of measures collected in the physiology lab. A second purpose is to use tools for reproducible data science. The report that you are expected to hand in therefore has some strict requirements in its format (see below). The assignment is a group assignment and at least three students are expected to contribute to each report.\n\n\nThe report should describe one test that you have performed in the physiology-lab. Select the test that is most interesting to you. The test should be described with a detailed protocol, including preparations of the participant (that is being tested), standardization of the test, and post-test data preparation. Post-test data preparation refers to steps needed to get data from e.g. equipment used during the test. This section should take into account and reference (Halperin, Pyne, and Martin 2015; Tanner and Gore 2012)\nThe next section should contain descriptive data from the test. This could be measures of central tendency and variability in the measures you have collected. If possible, try to find similar estimates in the scientific literature.\nFinally, we are interested in reliability. Here you need to calculate an estimate of reliability of the test. Use (and reference) (Hopkins 2000). Try to be clear with what measure of reliability you are using and what it is telling you.\nImportantly, the report should contain:\n\nAt least one table (created from your data)\nAt least one figure (created from your data), and\ndata presented in the text.\nThe report should use a bibliography file to manage references.\n\n\n\n\nThe report should be uploaded to github with both a source file (.Rmd or .qmd) and report file as output (html, pdf or docx-file). The github folder should also contain the dataset being used in the calculations. Work on your assignment as a R project. Contributions from members of the group can be made directly to the github repository.\n\n\n\nSee Chapter 8 in the course notes for a step by step approach to getting started with git.\nAn even more comprehensive guide to using github can be found here (Happy git with R)\nSee Chapters 5-7 for graphs, tables and report writing in R in the course notes\n\n\n\n\n\nCreate a github account. All members of the group should have their own account.\nOnce signed in to github.com, create a new repository with an informative name. Make sure that the repository is public. One member of the group can create this repository in their own account.\nBe sure to download and install git on your computer.\nStart up RStudio, start a new project, select the Version Control option and copy the address to the repository.\nAdd files to your project and upload to github. You need to add files, commit them and push using git. Use the command line in your terminal:\n\n\ngit add -A\n\ngit commit -m \"A short message to describe your changes\"\n\ngit push\n\n\nWhen you want to download the latest version of your project, type in the terminal:\n\n\ngit pull\n\n\nAlternatively, use GitHub desktop to commit/push/pull your changes.\nYou may encounter conflicts if pushing changes that overwrites changes made by other group members. These may be tricky but can be resolved… Have patience!\n\nIf you are contributing to the project owned by another person in your group it is recommended that you create a pull request with your changes.\n\n\n\n\nIf you want more data, you may “borrow” data from previous students:\nVO2max tests (Group 1) VO2max tests (Group 2) Lactate threshold tests\n\n\n\nCopy the link to your github folder into canvas."
  },
  {
    "objectID": "assignment-1.html#elements-of-the-report",
    "href": "assignment-1.html#elements-of-the-report",
    "title": "Assignment 1: Reliability and tools for reproducible data science",
    "section": "",
    "text": "The report should describe one test that you have performed in the physiology-lab. Select the test that is most interesting to you. The test should be described with a detailed protocol, including preparations of the participant (that is being tested), standardization of the test, and post-test data preparation. Post-test data preparation refers to steps needed to get data from e.g. equipment used during the test. This section should take into account and reference (Halperin, Pyne, and Martin 2015; Tanner and Gore 2012)\nThe next section should contain descriptive data from the test. This could be measures of central tendency and variability in the measures you have collected. If possible, try to find similar estimates in the scientific literature.\nFinally, we are interested in reliability. Here you need to calculate an estimate of reliability of the test. Use (and reference) (Hopkins 2000). Try to be clear with what measure of reliability you are using and what it is telling you.\nImportantly, the report should contain:\n\nAt least one table (created from your data)\nAt least one figure (created from your data), and\ndata presented in the text.\nThe report should use a bibliography file to manage references."
  },
  {
    "objectID": "assignment-1.html#the-format-of-the-report",
    "href": "assignment-1.html#the-format-of-the-report",
    "title": "Assignment 1: Reliability and tools for reproducible data science",
    "section": "",
    "text": "The report should be uploaded to github with both a source file (.Rmd or .qmd) and report file as output (html, pdf or docx-file). The github folder should also contain the dataset being used in the calculations. Work on your assignment as a R project. Contributions from members of the group can be made directly to the github repository.\n\n\n\nSee Chapter 8 in the course notes for a step by step approach to getting started with git.\nAn even more comprehensive guide to using github can be found here (Happy git with R)\nSee Chapters 5-7 for graphs, tables and report writing in R in the course notes\n\n\n\n\n\nCreate a github account. All members of the group should have their own account.\nOnce signed in to github.com, create a new repository with an informative name. Make sure that the repository is public. One member of the group can create this repository in their own account.\nBe sure to download and install git on your computer.\nStart up RStudio, start a new project, select the Version Control option and copy the address to the repository.\nAdd files to your project and upload to github. You need to add files, commit them and push using git. Use the command line in your terminal:\n\n\ngit add -A\n\ngit commit -m \"A short message to describe your changes\"\n\ngit push\n\n\nWhen you want to download the latest version of your project, type in the terminal:\n\n\ngit pull\n\n\nAlternatively, use GitHub desktop to commit/push/pull your changes.\nYou may encounter conflicts if pushing changes that overwrites changes made by other group members. These may be tricky but can be resolved… Have patience!\n\nIf you are contributing to the project owned by another person in your group it is recommended that you create a pull request with your changes."
  },
  {
    "objectID": "assignment-1.html#data",
    "href": "assignment-1.html#data",
    "title": "Assignment 1: Reliability and tools for reproducible data science",
    "section": "",
    "text": "If you want more data, you may “borrow” data from previous students:\nVO2max tests (Group 1) VO2max tests (Group 2) Lactate threshold tests"
  },
  {
    "objectID": "assignment-1.html#how-to-hand-in-the-report.",
    "href": "assignment-1.html#how-to-hand-in-the-report.",
    "title": "Assignment 1: Reliability and tools for reproducible data science",
    "section": "",
    "text": "Copy the link to your github folder into canvas."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "assignment-2.html",
    "href": "assignment-2.html",
    "title": "Assignment 2: Regression models, predicting from data",
    "section": "",
    "text": "There are several suggestions on how to best capture the physiological “essence” of the lactate threshold test (See Tanner and Gore 2012, chap. 6). A simple, and very common way to analyze the relationship between exercise intensity and blood lactate is to determine exercise intensity at fixed blood lactate values. This can be done by fitting a regression model that captures the relationship and then “inverse predict” the exercise intensity value. Machado [-Machado_2012] provides a more elaborate method for calculating the lactate threshold. An example of such calculation can be found in the lecture notes, Linear and curve-linear relationships, and predictions.\nYour report should use data from the reliability project in the lab. Calculate at least two lactate thresholds (e.g. exercise intensity at 2 and 4 mmol L-1) and compare the reliability (typical error as a percentage of the mean) between the two thresholds. If you want to complicate things further you may want to implement other lactate threshold concepts [described in Tanner and Gore (2012); Newell et al. (2007); Machado_2012] and the course notes.\nIf you lack data, you could use data from the exscidata package or from a previous year (see this file Lactate threshold tests).\n\n\n\nIn the molecular laboratory you have been tasked to extract and analyze DNA and RNA in two different assignments. In this process we have to determine the size of resulting PCR (polymerase chain reaction) amplified DNA fragments or Ct values in qPCR reactions.\nIn this assignment you can either analyze DNA fragments or qPCR results.\nA tutorial using Image J and R to analyze DNA fragments can be found here. In your report you should show how you arrived to your predicted sizes by including the code chunk in your report.\nThe slope of a calibration curve can inform on the efficiency of qPCR reactions. Use the data that you collected to calculate reaction efficiency.\n\n\n\nUsing the hypertrophy data set, state a question that concerns a linear relationship between two variables in the data set. These variables might be related to muscle size and strength, or two molecular markers or any other variables you are interested in. Include a regression table from your analysis in the report and interpret its components in plain language (e.g. for an unit difference in the independent variable the dependent variable differs by y units). The interpretation should also include a description and explanation of the standard error, the t-value and the p-value. Valuable guidance on how to interpret the table may be found in for example and in (Frigessi and Aalen 2018), (Campbell, Walters, and Machin 2020) and (Spiegelhalter 2019, chap. 5).\nSpecial attention should be made concerning the p-value. How do you define and interpret the p-value in your regression table. What does it mean?.\n\n\n\nThe report is a group assignment, it is not to be included in the portfolio (mappeeksamen). However, it is required in order to pass the course (arbeidskrav).\nCreate a new project on github and collaborate with your group there. The repository with all data and coded needed to create the report, and the report itself (in html, docx or pdf format) should be reported on canvas as a link to the repository. Each member of the group hand in the link in canvas. The repository should be the same for all group members."
  },
  {
    "objectID": "assignment-2.html#part-1-lactate-thresholds",
    "href": "assignment-2.html#part-1-lactate-thresholds",
    "title": "Assignment 2: Regression models, predicting from data",
    "section": "",
    "text": "There are several suggestions on how to best capture the physiological “essence” of the lactate threshold test (See Tanner and Gore 2012, chap. 6). A simple, and very common way to analyze the relationship between exercise intensity and blood lactate is to determine exercise intensity at fixed blood lactate values. This can be done by fitting a regression model that captures the relationship and then “inverse predict” the exercise intensity value. Machado [-Machado_2012] provides a more elaborate method for calculating the lactate threshold. An example of such calculation can be found in the lecture notes, Linear and curve-linear relationships, and predictions.\nYour report should use data from the reliability project in the lab. Calculate at least two lactate thresholds (e.g. exercise intensity at 2 and 4 mmol L-1) and compare the reliability (typical error as a percentage of the mean) between the two thresholds. If you want to complicate things further you may want to implement other lactate threshold concepts [described in Tanner and Gore (2012); Newell et al. (2007); Machado_2012] and the course notes.\nIf you lack data, you could use data from the exscidata package or from a previous year (see this file Lactate threshold tests)."
  },
  {
    "objectID": "assignment-2.html#part-2-predicting-sizes-of-dna-fragments-or-slopes-of-a-qpcr-calibration-curve",
    "href": "assignment-2.html#part-2-predicting-sizes-of-dna-fragments-or-slopes-of-a-qpcr-calibration-curve",
    "title": "Assignment 2: Regression models, predicting from data",
    "section": "",
    "text": "In the molecular laboratory you have been tasked to extract and analyze DNA and RNA in two different assignments. In this process we have to determine the size of resulting PCR (polymerase chain reaction) amplified DNA fragments or Ct values in qPCR reactions.\nIn this assignment you can either analyze DNA fragments or qPCR results.\nA tutorial using Image J and R to analyze DNA fragments can be found here. In your report you should show how you arrived to your predicted sizes by including the code chunk in your report.\nThe slope of a calibration curve can inform on the efficiency of qPCR reactions. Use the data that you collected to calculate reaction efficiency."
  },
  {
    "objectID": "assignment-2.html#part-3-intepreting-a-regression-table",
    "href": "assignment-2.html#part-3-intepreting-a-regression-table",
    "title": "Assignment 2: Regression models, predicting from data",
    "section": "",
    "text": "Using the hypertrophy data set, state a question that concerns a linear relationship between two variables in the data set. These variables might be related to muscle size and strength, or two molecular markers or any other variables you are interested in. Include a regression table from your analysis in the report and interpret its components in plain language (e.g. for an unit difference in the independent variable the dependent variable differs by y units). The interpretation should also include a description and explanation of the standard error, the t-value and the p-value. Valuable guidance on how to interpret the table may be found in for example and in (Frigessi and Aalen 2018), (Campbell, Walters, and Machin 2020) and (Spiegelhalter 2019, chap. 5).\nSpecial attention should be made concerning the p-value. How do you define and interpret the p-value in your regression table. What does it mean?."
  },
  {
    "objectID": "assignment-2.html#how-to-hand-in-the-report",
    "href": "assignment-2.html#how-to-hand-in-the-report",
    "title": "Assignment 2: Regression models, predicting from data",
    "section": "",
    "text": "The report is a group assignment, it is not to be included in the portfolio (mappeeksamen). However, it is required in order to pass the course (arbeidskrav).\nCreate a new project on github and collaborate with your group there. The repository with all data and coded needed to create the report, and the report itself (in html, docx or pdf format) should be reported on canvas as a link to the repository. Each member of the group hand in the link in canvas. The repository should be the same for all group members."
  },
  {
    "objectID": "assignment-4.html",
    "href": "assignment-4.html",
    "title": "Assignment 4: Study designs",
    "section": "",
    "text": "Choose an area of interest (e.g. protein supplementation for muscle hypertrophy or the effect of block periodization on VO2max). Find at least five original research studies1 in your selected area and describe strength and weakness of these studies (see below). The report should focus on the design of the studies and selection of statistical tests to answer study aims. Conclude your report with a recommendation, how should future studies in your area be designed to best answer similar questions?\nThe report should be handed in on canvas as a link to a github folder containing a reproducible report. This is an individual assignment.\n\n\n\nWhen analyzing your studies you can use the QALMRI method2.\n\nNote that the report should not contain your QALMRI table but should instead be focused on describing differences and similarities in all studies together (see also below)!\n\n\n\n\nWhat was the broader problem the authors are trying to resolve in the study?\nWhat are the specific questions the authors are trying to answer?\n\n\nThe first point should be similar in all your studies, this could be e.g. the effect of age on physical functioning, effect of certain training protocols on VO2max, etc. The second point is potentially different between your studies.\n\n\n\n\n\nIs the specific question framed as an hypothesis or a question?\nIf the authors have formulated a hypothesis, what alternative explanations can you think of that could potentially explain the data that the authors hypothesize?\n\n\n\n\n\nWhat is the logic of the hypothesis or the question. Try to create a “line of logic” between the introduction and the question/hypothesis.\n\n\n\n\n\nDescribe the study design. Use Hulley, (2013), Chapters 7-133 in your analysis.\nDescribe the sample and if the study defines the population.\nDescribe the method of recruiting participants to the study. Did the authors justify their sample size (i.e. did they do a power calculation)?\nDescribe how the study was conducted (what tests was performed when etc.)\nDescribe the variables in the study, what variables relate to the question/hypothesis?\nWhat methods did the authors use to make claims (what statistical tests were used)\n\n\n\n\n\nWhat were the main results of the study, did the authors answer their question/address their hypothesis?\n\n\n\n\n\nWhat could the authors conclude from the study?\nWhat did the authors conclude about the study population?\n\n\n\n\n\nYour report should not contain a detailed summary of all studies for all these questions, instead you should summarize your results. Highlight differences and similarities between studies. As the main point of this review is study designs and statistical analyses, this should be your main focus. When doing your literature review, it is however a good idea to structure it in a table with the above mentioned headings:\n\nQuestion\nAlternative\nLogic\nMethods\nResults\nInference"
  },
  {
    "objectID": "assignment-4.html#overview",
    "href": "assignment-4.html#overview",
    "title": "Assignment 4: Study designs",
    "section": "",
    "text": "Choose an area of interest (e.g. protein supplementation for muscle hypertrophy or the effect of block periodization on VO2max). Find at least five original research studies1 in your selected area and describe strength and weakness of these studies (see below). The report should focus on the design of the studies and selection of statistical tests to answer study aims. Conclude your report with a recommendation, how should future studies in your area be designed to best answer similar questions?\nThe report should be handed in on canvas as a link to a github folder containing a reproducible report. This is an individual assignment."
  },
  {
    "objectID": "assignment-4.html#details",
    "href": "assignment-4.html#details",
    "title": "Assignment 4: Study designs",
    "section": "",
    "text": "When analyzing your studies you can use the QALMRI method2.\n\nNote that the report should not contain your QALMRI table but should instead be focused on describing differences and similarities in all studies together (see also below)!\n\n\n\n\nWhat was the broader problem the authors are trying to resolve in the study?\nWhat are the specific questions the authors are trying to answer?\n\n\nThe first point should be similar in all your studies, this could be e.g. the effect of age on physical functioning, effect of certain training protocols on VO2max, etc. The second point is potentially different between your studies.\n\n\n\n\n\nIs the specific question framed as an hypothesis or a question?\nIf the authors have formulated a hypothesis, what alternative explanations can you think of that could potentially explain the data that the authors hypothesize?\n\n\n\n\n\nWhat is the logic of the hypothesis or the question. Try to create a “line of logic” between the introduction and the question/hypothesis.\n\n\n\n\n\nDescribe the study design. Use Hulley, (2013), Chapters 7-133 in your analysis.\nDescribe the sample and if the study defines the population.\nDescribe the method of recruiting participants to the study. Did the authors justify their sample size (i.e. did they do a power calculation)?\nDescribe how the study was conducted (what tests was performed when etc.)\nDescribe the variables in the study, what variables relate to the question/hypothesis?\nWhat methods did the authors use to make claims (what statistical tests were used)\n\n\n\n\n\nWhat were the main results of the study, did the authors answer their question/address their hypothesis?\n\n\n\n\n\nWhat could the authors conclude from the study?\nWhat did the authors conclude about the study population?"
  },
  {
    "objectID": "assignment-4.html#performing-your-literature-review-and-writing-the-report",
    "href": "assignment-4.html#performing-your-literature-review-and-writing-the-report",
    "title": "Assignment 4: Study designs",
    "section": "",
    "text": "Your report should not contain a detailed summary of all studies for all these questions, instead you should summarize your results. Highlight differences and similarities between studies. As the main point of this review is study designs and statistical analyses, this should be your main focus. When doing your literature review, it is however a good idea to structure it in a table with the above mentioned headings:\n\nQuestion\nAlternative\nLogic\nMethods\nResults\nInference"
  },
  {
    "objectID": "assignment-4.html#footnotes",
    "href": "assignment-4.html#footnotes",
    "title": "Assignment 4: Study designs",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAvoid using review articles or meta-analyses↩︎\nSee Teaching undergraduate students to read empirical articles: An evaluation and revision of the QALMRI method, this advice was also heavily influenced by this website↩︎\nHulley, S. B. (2013). Designing clinical research. Philadelphia, Wolters Kluwer/Lippincott Williams & Wilkins.↩︎"
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "Her finner dere feedback på arbeidskrav i emnet.\n\n\n\nAssignment\nDue date\nIncluded in portfolio\nGroup assignment\n\n\n\n\nDescriptive statistics, reliability and validity, tools for reproducible data science\n2023-10-15\nYes\nYes\n\n\nRegression models and prediction from data\n2023-10-29\nNo\nYes\n\n\nExtraction and analysis of DNA\n\nOptionala\nYes\n\n\nExtraction of RNA and analysis of qPCR experiments\n\nOptionala\nYes\n\n\nExtraction and analysis of Protein\n\nOptionala\nYes\n\n\nPhilosophy of scienceb (See canvas)\n\nYes\nNo\n\n\nDrawing inference from statistical models and statistical power\n2023-10-29\nNo\nYes\n\n\nStudy designs\n2023-11-05\nYes\nNo\n\n\nAnalyzing repeated measures experiments\n2023-11-10\nYes\nNo\n\n\n\na Select one laboratory assignments for your portfolio exam. All groups presents one selected method in class. b This assignment is presented in connection with lectures."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quantitative methods and statistics (In Sport and Exercise Science)",
    "section": "",
    "text": "Welcome to the course Quantitative methods and Statistics (IDR4000). The course aims to give students an overview of methodological aspects within the field of sport and exercise-physiology. Specifically, planning, conducting and analyzing research projects with human participants will be covered. The lecture notes for the course can be found here. This website will contain tutorials, workshops, assignments and lectures that we will work on in the course."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Quantitative methods and statistics (In Sport and Exercise Science)",
    "section": "",
    "text": "Welcome to the course Quantitative methods and Statistics (IDR4000). The course aims to give students an overview of methodological aspects within the field of sport and exercise-physiology. Specifically, planning, conducting and analyzing research projects with human participants will be covered. The lecture notes for the course can be found here. This website will contain tutorials, workshops, assignments and lectures that we will work on in the course."
  },
  {
    "objectID": "index.html#practical-information",
    "href": "index.html#practical-information",
    "title": "Quantitative methods and statistics (In Sport and Exercise Science)",
    "section": "Practical information",
    "text": "Practical information\nThese notes were updated on 2024-08-14 and cover the course held during 2024 autumn semester. Contact Daniel Hammarström if you have any questions regarding this content.\n\nLearning objectives\nLearning objectives can be read in Norwegian here.\n\n\nLearning strategies\nThe course will include lectures, laboratory exercises, computer exercises and workshops, seminars and student presentations. Workshops will be held in-person.\nComputer exercises will eventually require that you have special computer software installed on your computer. The software is free (see specific chapters in lecture notes)."
  },
  {
    "objectID": "index.html#assignments-and-portfolio-exam",
    "href": "index.html#assignments-and-portfolio-exam",
    "title": "Quantitative methods and statistics (In Sport and Exercise Science)",
    "section": "Assignments and Portfolio exam",
    "text": "Assignments and Portfolio exam\nThe course is based on several assignments. Some of these assignments are to be handed in as part of a portfolio exam upon which your grade is based.\nAssignments that are due during the course (arbeidskrav) are expected to be further improved after feedback from fellow students and teachers before inclusion in your portfolio.\nSee this GitHub repository for an overview"
  },
  {
    "objectID": "index.html#the-whole-course-in-one-table",
    "href": "index.html#the-whole-course-in-one-table",
    "title": "Quantitative methods and statistics (In Sport and Exercise Science)",
    "section": "The whole course in one table",
    "text": "The whole course in one table\n\n\n\nWeek\nWorkshops\nSuggested reading\nLectures\nAssignments\n\n\n\n\n34\nIntroduction to data science, Starting up R, Creating your first graph, The quarto publishing system\nCourse notes ch. 1-5, [@RN2902, Ch. 1-2]\nIntro to data science (video)(slides)\n\n\n\n35\n\n\n\n\n\n\n36\n\n\n\n\n\n\n37\n\n\n\n\n\n\n38\n\n\n\n\n\n\n39\n\n\n\n\n\n\n40\n\n\n\n\n\n\n41\n\n\n\n\n\n\n42\n\n\n\n\n\n\n43\n\n\n\n\n\n\n44\n\n\n\n\n\n\n45\n\n\n\n\n\n\n46\n\n\n\n\n\n\n47\n\n\n\n\n\n\n48"
  },
  {
    "objectID": "notes1-linear-models.html",
    "href": "notes1-linear-models.html",
    "title": "Notes: Univariate regression models by hand",
    "section": "",
    "text": "The univariate regression model has one dependent and one independent variable. The goal is to determine the relationship between the two variables.\nHere we will explore the mathematics of this model and replicate results from R “by hand”.\nFirst, let’s simulate some data.\nlibrary(tidyverse)\n\nset.seed(100)\n\nx &lt;- runif(10, 5, 10)\ny &lt;- 10 + 2 * x + rnorm(10, 0, 1)\n\n\nggplot(data = data.frame(x, y), \n       aes(x, y)) + geom_point()"
  },
  {
    "objectID": "notes1-linear-models.html#a-categorical-predictor-variable",
    "href": "notes1-linear-models.html#a-categorical-predictor-variable",
    "title": "Notes: Univariate regression models by hand",
    "section": "A categorical predictor variable",
    "text": "A categorical predictor variable\n\nxcat &lt;- ifelse(x &lt; 7, 0, 1)\n\n\n## Calculate the correlation \nzx &lt;- (xcat - mean(xcat)) / sd(xcat)\nzy &lt;- (y - mean(y)) / sd(y)\n\n\nrxy &lt;- sum(zx * zy) / (length(xcat) - 1)\n\n\n## Calculate the slope \n\nb1 &lt;- rxy * (sd(y)/sd(xcat))\n\nb0 &lt;- mean(y) - b1 * mean(xcat)\n\n\nb1\n\n[1] 3.891868\n\nb0\n\n[1] 22.09147\n\n## Calculate the errors\n\ne &lt;- y - (b0 + b1*xcat)\n\n## \n\nvarx &lt;- (sum((xcat-mean(xcat))^2))/length(xcat)\n\nseb0 &lt;- se/sqrt(length(xcat)) * sqrt(1 + ((mean(xcat)^2) / varx))\n\nsx &lt;- sqrt(sum((xcat - mean(xcat))^2) / length(xcat))\n\nseb1 &lt;- se/sqrt(length(y)) * 1 /  sx\n\n\n## Calculate"
  },
  {
    "objectID": "workshops.html",
    "href": "workshops.html",
    "title": "Workshops",
    "section": "",
    "text": "Workshop\nAdditional material\n\n\n\n\nIntroduction to data science (Norwegian)\n\n\n\nInstalling and starting R\n\n\n\nCreating your first graph\n\n\n\nData wrangling\n\n\n\nCreating tables\n\n\n\nWriting reproducible, version controlled reports\n\n\n\nThe linear model\n\n\n\nCategorical predictors\n\n\n\nCurve-linear model\n\n\n\nWriting functions\n\n\n\nP-values and simulations\n\n\n\nSampling, power and effects\n\n\n\nStudy designs"
  },
  {
    "objectID": "ws13-p-values-simulation.html",
    "href": "ws13-p-values-simulation.html",
    "title": "Populations, samples and statistical inference",
    "section": "",
    "text": "We have a two-group design and want to know if Condition A is different from Condition B in any meaningful way.\nTo accomplish this we can test if an observed difference is very different to a reference, where any difference is up to chance\nUse the code chunk below to simulate data\n\n\nset.seed(1)\n# Population \nA &lt;- rnorm(1000, mean = 100, 10)\n\nB &lt;- rnorm(1000, mean = 92, 10)\n\n# Sample\na &lt;- sample(A, 15, replace = FALSE)\nb &lt;- sample(B, 15, replace = FALSE)\n\n\n\n\n\n\n\nGroup work\n\n\n\n\nUsing the t.test function, test against the null-hypothesis of no difference between groups\nConstruct a permutation test where a reference distribution of possible outcomes is created in a loop(!). Try to explain what the code below does.\nCalculate how many cases led to a more extreme result than the observed in your experiment.\n\n\nlibrary(tidyverse)\n\ndifferences &lt;- vector()\n\nfor(i in 1:1000) {\n        \n        \n     samp &lt;- sample(c(a, b), 30, replace = FALSE)\n        \n     differences[i] &lt;- mean(samp[1:15]) - mean(samp[16:30])\n        \n        \n}\n\n\ndata.frame(differences) %&gt;%\n        ggplot(aes(differences)) + geom_histogram() + \n        geom_vline(xintercept = mean(a) - mean(b), color = \"red\", size = 2)"
  },
  {
    "objectID": "ws13-p-values-simulation.html#a-simple-test-of-differences",
    "href": "ws13-p-values-simulation.html#a-simple-test-of-differences",
    "title": "Populations, samples and statistical inference",
    "section": "",
    "text": "We have a two-group design and want to know if Condition A is different from Condition B in any meaningful way.\nTo accomplish this we can test if an observed difference is very different to a reference, where any difference is up to chance\nUse the code chunk below to simulate data\n\n\nset.seed(1)\n# Population \nA &lt;- rnorm(1000, mean = 100, 10)\n\nB &lt;- rnorm(1000, mean = 92, 10)\n\n# Sample\na &lt;- sample(A, 15, replace = FALSE)\nb &lt;- sample(B, 15, replace = FALSE)\n\n\n\n\n\n\n\nGroup work\n\n\n\n\nUsing the t.test function, test against the null-hypothesis of no difference between groups\nConstruct a permutation test where a reference distribution of possible outcomes is created in a loop(!). Try to explain what the code below does.\nCalculate how many cases led to a more extreme result than the observed in your experiment.\n\n\nlibrary(tidyverse)\n\ndifferences &lt;- vector()\n\nfor(i in 1:1000) {\n        \n        \n     samp &lt;- sample(c(a, b), 30, replace = FALSE)\n        \n     differences[i] &lt;- mean(samp[1:15]) - mean(samp[16:30])\n        \n        \n}\n\n\ndata.frame(differences) %&gt;%\n        ggplot(aes(differences)) + geom_histogram() + \n        geom_vline(xintercept = mean(a) - mean(b), color = \"red\", size = 2)"
  },
  {
    "objectID": "ws13-p-values-simulation.html#the-effect-of-small-and-large-samples.",
    "href": "ws13-p-values-simulation.html#the-effect-of-small-and-large-samples.",
    "title": "Populations, samples and statistical inference",
    "section": "The effect of small and large samples.",
    "text": "The effect of small and large samples.\n\nThe sample size determines what differences we may observe.\n\n::: {.callout-note}\n\nGroup work\n\nRe-do the experiment above with a smaller sample size, and a larger sample size.\nReport your experiment as a t-test and a permutation test."
  },
  {
    "objectID": "ws13-p-values-simulation.html#limitations-of-p-values",
    "href": "ws13-p-values-simulation.html#limitations-of-p-values",
    "title": "Populations, samples and statistical inference",
    "section": "Limitations of p-values",
    "text": "Limitations of p-values\n\nP-values are often misinterpreted!\nFind a scientific paper (full text) in any area and search for “P &lt; 0.05” and “P &gt; 0.05”, see how authors interpret the results."
  },
  {
    "objectID": "ws15-study-design-1.html",
    "href": "ws15-study-design-1.html",
    "title": "Study designs and statistical tests",
    "section": "",
    "text": "A regression model can be used to assess the differences between treatments in a randomized controlled trial (RCT). But what do we use as the dependent variable in this model?\nVickers (2001) identified and compared four scenarios that might be “tested” from a comparison of two groups:\n\n\nBaseline (\\(b\\)), follow-up (\\(f\\)) and group (\\(g\\)) can be analyzed as\n\nPost-scores only: \\(f\\) is compared between \\(g\\)\nPercentage change: \\(100 \\times \\frac{b-f}{b}\\) is compared between \\(g\\)\nChange: \\(b-f\\) compared between \\(g\\)\nAnalysis of co-variance (ANCOVA): \\(g\\) is compared as the effect in \\(f=\\beta_1 b + \\beta_2 g\\)\n\n\n\nThe choice of method should be based on its efficiency, the ability find true effects using a minimal number of participants.\nVickers show that the use of percentage change from baseline is inefficient (Vickers 2001) and could lead to violations of assumptions for statistical tests.\nThe ANCOVA model (see below) is the preferred model when model assumptions holds (Vickers 2001)\nWe will proceed with comparison of change-scores and later discuss ANCOVA."
  },
  {
    "objectID": "ws15-study-design-1.html#what-to-analyze",
    "href": "ws15-study-design-1.html#what-to-analyze",
    "title": "Study designs and statistical tests",
    "section": "",
    "text": "A regression model can be used to assess the differences between treatments in a randomized controlled trial (RCT). But what do we use as the dependent variable in this model?\nVickers (2001) identified and compared four scenarios that might be “tested” from a comparison of two groups:\n\n\nBaseline (\\(b\\)), follow-up (\\(f\\)) and group (\\(g\\)) can be analyzed as\n\nPost-scores only: \\(f\\) is compared between \\(g\\)\nPercentage change: \\(100 \\times \\frac{b-f}{b}\\) is compared between \\(g\\)\nChange: \\(b-f\\) compared between \\(g\\)\nAnalysis of co-variance (ANCOVA): \\(g\\) is compared as the effect in \\(f=\\beta_1 b + \\beta_2 g\\)\n\n\n\nThe choice of method should be based on its efficiency, the ability find true effects using a minimal number of participants.\nVickers show that the use of percentage change from baseline is inefficient (Vickers 2001) and could lead to violations of assumptions for statistical tests.\nThe ANCOVA model (see below) is the preferred model when model assumptions holds (Vickers 2001)\nWe will proceed with comparison of change-scores and later discuss ANCOVA."
  },
  {
    "objectID": "ws15-study-design-1.html#analysis-of-change-scores",
    "href": "ws15-study-design-1.html#analysis-of-change-scores",
    "title": "Study designs and statistical tests",
    "section": "Analysis of change scores",
    "text": "Analysis of change scores\n\n\nCode\nlibrary(tidyverse)\ndata.frame(Time = c(\"Pre\", \"Post\", \"Pre\", \"Post\"), outcome = c(0,1, 0,1)) %&gt;%\n        mutate(Time = factor(Time, levels = c(\"Pre\", \"Post\"))) %&gt;%\n        ggplot(aes(Time, outcome)) + \n        theme_classic() + \n        ylab(\"Outcome\") +\n        theme(axis.text.y = element_blank()) +\n        geom_segment(aes(x = Time, xend = 2, \n                         y = c(0.5,0.5, 0.5,1.5), \n                         yend = c(0.5,0.5, 1.5,1.5))) + \n        scale_y_continuous(limits = c(0,2)) + \n        annotate(geom = \"text\", x = 2.25, y = 1.5, label = \"Treatment\") +\n        annotate(geom = \"text\", x = 2.25, y = 0.5, label = \"Control\") \n\n\n\n\n\n\n\n\n\n\nIn a simple analysis of two parallel groups (experimental and control), the alternative hypothesis is that the two groups are different in terms of change scores.\n\n\\[ \\Delta Y_{Treatment}-\\Delta Y_{Control} \\neq 0 \\]\n\nThe null-hypothesis that we use in our statistical models is\n\n\\[ \\Delta Y_{Treatment}-\\Delta Y_{Control} = 0 \\]\n\nIn the simple case, this equals a t-test.\n\nt.test(delta ~ group, paired = FALSE)\n\nA similar scenario is a comparison between two treatments (A and B)\n\n\n\nCode\ndata.frame(Time = c(\"Pre\", \"Post\", \"Pre\", \"Post\"), outcome = c(0,1, 0,1)) %&gt;%\n        mutate(Time = factor(Time, levels = c(\"Pre\", \"Post\"))) %&gt;%\n        ggplot(aes(Time, outcome)) + \n        theme_classic() + \n        ylab(\"Outcome\") +\n        theme(axis.text.y = element_blank()) +\n        geom_segment(aes(x = Time, xend = 2, \n                         y = c(0.5,0.5, 0.5,1.5), \n                         yend = c(1.1,0.5, 1.5,1.5))) + \n        scale_y_continuous(limits = c(0,2)) + \n        annotate(geom = \"text\", x = 2.25, y = 1.5, label = \"A\") +\n        annotate(geom = \"text\", x = 2.25, y = 1.1, label = \"B\") \n\n\n\n\n\n\n\n\n\n\nThe null-hypothesis is that\n\n\\[ \\Delta Y_{A}-\\Delta Y_{B} = 0 \\]\n\nThe tenthirty data set has similar data as in the above scenario\nWe want to know what treatment to recommend for 1RM improvements.\n\n\n\nGroup work: Ten-thirty\n\nUsing the tenthirty data, use filter to keep time-point “pre” and “post” for the leg-press exercise.\nMake a plot of the averages of each group.\nCalculate a change score as post - pre.\nMake a plot of the average change score for each group.\nFit a regression model (change ~ group) and interpret the results.\n\n\n\nQuestions:\n\nConcerning the sample: Which group has the largest improvement i leg-press 1RM?\nConcerning inference to the population: Which method do you recommend for improving 1RM leg-press?\n\n\n\n\n\nPossible solutions\n## Create a plot of averages over time ----------- ##\n\nlibrary(tidyverse); library(exscidata)\ndata(\"tenthirty\")\n\n# Save the data set in a filtered version\ntenthirty_reduced &lt;- tenthirty %&gt;%\n  filter(time %in% c(\"pre\", \"post\"), \n         exercise == \"legpress\", \n         !is.na(load)) %&gt;% \n  print()\n\n# Create a line plot with one line per group\ntenthirty_reduced %&gt;%  \n  group_by(time, group) %&gt;%\n  summarise(m = mean(load)) %&gt;%\n  mutate(time = factor(time, levels = c(\"pre\", \"post\"))) %&gt;%\n  ggplot(aes(time, m, color = group, \n             group = group)) + geom_line()\n\n\n# Create a plot of average changes per group\ntenthirty_reduced %&gt;%\n  pivot_wider(names_from = time, \n              values_from = load) %&gt;%\n  mutate(change = post - pre) %&gt;%\n  group_by(group) %&gt;%\n  summarise(mean_change = mean(change, na.rm = TRUE)) %&gt;%\n  ggplot(aes(group, mean_change, fill = group)) + \n  geom_bar(stat = \"identity\", width = 0.3)\n  \n\n# Calculate the average change per group, and difference between groups\ntenthirty_reduced %&gt;%\n  pivot_wider(names_from = time, \n              values_from = load) %&gt;%\n  mutate(change = post - pre) %&gt;%\n  group_by(group) %&gt;%\n  summarise(mean_change = mean(change, na.rm = TRUE)) %&gt;%\n  pivot_wider(names_from = group, \n              values_from = mean_change) %&gt;%\n  mutate(diff = RM30 - RM10) %&gt;%\n  print()\n\n\n# Calculate change score and save new data set\ndat &lt;- tenthirty_reduced %&gt;%\n  pivot_wider(names_from = time, \n              values_from = load) %&gt;%\n  mutate(change = post - pre) %&gt;%\n  print()\n\n\n# Fit model with change scores explained by group\nm1 &lt;- lm(change ~ group, data = dat)\n\n# Get the model summary\nsummary(m1)\n\n\n\n\nCode\nlibrary(exscidata)\n\ntenthirty %&gt;%\n  filter(exercise == \"legpress\", \n         time %in% c(\"pre\", \"post\")) %&gt;%\n  mutate(time = factor(time, levels = c(\"pre\", \"post\"))) %&gt;%\n  group_by(time, group) %&gt;%\n  summarise(m = mean(load, na.rm = TRUE)) %&gt;%\n  ggplot(aes(time, m, group = group, color = group)) + geom_line() +\n  labs(x = \"Time\", y = \"Legpress performance (kg)\", color = \"Group\") +\n          theme_classic()\n\n\n\n\n\nGroup averages over time"
  },
  {
    "objectID": "ws15-study-design-1.html#from-t-test-to-ancova",
    "href": "ws15-study-design-1.html#from-t-test-to-ancova",
    "title": "Study designs and statistical tests",
    "section": "From t-test to ANCOVA",
    "text": "From t-test to ANCOVA\n\nAbove we used the simple comparison of change score. A more appropriate model to study outcomes of a RCT is an analysis of co-variance (ANCOVA)\nIn an ANCOVA, change-scores or post values are compared and the estimates tells you the adjusted differences between groups if they had the exact same starting point.\nThe ANCOVA model can be written as\n\n\\[f =\\beta_0 + \\beta_1\\times b + \\beta_2\\times g \\] where \\(f\\) is the follow-up score, \\(b\\) is the baseline and \\(g\\) is the group.\n\nIn R code, this corresponds to\n\npost ~ pre + group\n\n\nGroup work: Ten-thirty ANCOVA\n\nFit an ANCOVA model with change scores as the dependent variable (m1)\nFit an ANCOVA model with the post values as the dependent variable (m2)\nPre-training values and group should be covariates in both models.\nUse the aov command to fit an ANOVA model (m3)\nEvaluate models using summary and anova\n\n\n\nQuestions\n\nConcerning inference to the population: Which method do you recommend for improving 1RM leg-press?\nIn what ways are the models different?\nWhat is happening with the aov command, how is it different to m2?\n\n\n\n\n\nPossible solutions\n# Create a data set with change\n\ndat &lt;- tenthirty %&gt;%\n  filter(exercise == \"legpress\", \n         time %in% c(\"pre\", \"post\")) %&gt;%\n  pivot_wider(names_from = time, \n              values_from = load) %&gt;%\n  mutate(change = post - pre, \n         pre = pre - mean(pre)) %&gt;% # Mean center pre-values\n  print()\n\n\n# Fit models \n\nm1 &lt;- lm(post ~ pre + group, data = dat)\n\nm2 &lt;- lm(change ~ pre + group, data = dat)\n\n\nsummary(m1)\nsummary(m2)\n\n\nm3 &lt;- aov(post ~ pre + group, data = dat)\n\nsummary(m3)\nanova(m1)"
  },
  {
    "objectID": "ws15-study-design-1.html#understanding-the-ancova-model",
    "href": "ws15-study-design-1.html#understanding-the-ancova-model",
    "title": "Study designs and statistical tests",
    "section": "Understanding the ANCOVA model",
    "text": "Understanding the ANCOVA model\n\nThe ANCOVA estimates the difference between e.g. post scores given a (estimated) association between pre- and post-scores.\nThis can be visualized in a simple plot where the difference in the paralell lines are the treatment effect\n\n\n\nCode\nm1 &lt;- tenthirty %&gt;%\n  filter(exercise == \"legpress\", \n         time %in% c(\"pre\", \"post\")) %&gt;%\n  pivot_wider(names_from = \"time\", values_from = \"load\") %&gt;%\n  mutate(change = post - pre) %&gt;%\n  lm(post ~ pre + group, data = .) \n\n\ntenthirty %&gt;%\n  filter(exercise == \"legpress\", \n         time %in% c(\"pre\", \"post\")) %&gt;%\n    pivot_wider(names_from = \"time\", values_from = \"load\") %&gt;%\n  ggplot(aes(pre, post, color = group)) + geom_point() + \n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  geom_abline(intercept = coef(m1)[1], slope = coef(m1)[2], color = \"red\")  + \n  geom_abline(intercept = coef(m1)[1] + coef(m1)[3], slope = coef(m1)[2], color = \"blue\") + \n  labs(title = \"Estimates from an ANCOVA model\", \n       subtitle = \"post ~ pre + group\", \n       color = \"Group\") + theme_classic()\n\n\n\n\n\n\n\n\n\n\nBy correcting for baseline values we can expect different estimates for each group, notice also that the standard errors are changing\n\n\n\nCode\nlibrary(emmeans)\n\nm0 &lt;- tenthirty %&gt;%\n  filter(exercise == \"legpress\", \n         time %in% c(\"pre\", \"post\")) %&gt;%\n  pivot_wider(names_from = \"time\", values_from = \"load\") %&gt;%\n  mutate(change = post - pre) %&gt;%\n  lm(post ~ group, data = .) \n\nm2 &lt;- tenthirty %&gt;%\n  filter(exercise == \"legpress\", \n         time %in% c(\"pre\", \"post\")) %&gt;%\n  pivot_wider(names_from = \"time\", values_from = \"load\") %&gt;%\n  mutate(change = post - pre) %&gt;%\n  lm(change ~ group, data = .) \n\n\n\n\n\nbind_rows(data.frame(emmeans(m0, specs = ~ group)) %&gt;%\n        mutate(Model = \"Post only\"), \n        data.frame(emmeans(m1, specs = ~ group)) %&gt;%\n        mutate(Model = \"ANCOVA\")) %&gt;%\n        ggplot(aes(group, emmean, fill = Model)) + \n        geom_errorbar(aes(ymin = emmean, ymax = emmean + SE), \n                      position = position_dodge(), width = 0.5) +\n        geom_bar(stat = \"identity\", position = position_dodge(), \n                 width = 0.5)  \n\n\n\n\n\nGroup estimates from two different models, error bars are standard errors (SE)\n\n\n\n\n\nCalculating the differences between groups we can also see differences, here we also incorporate the change score model.\n\n\n\nCode\nbind_rows(\ndata.frame(confint(contrast(emmeans(m0, specs = ~ group)), adjust = \"none\")) %&gt;%\n        mutate(Model = \"Post only\"),\ndata.frame(confint(contrast(emmeans(m1, specs = ~ group)), adjust = \"none\")) %&gt;%\n        mutate(Model = \"ANCOVA\"),\ndata.frame(confint(contrast(emmeans(m2, specs = ~ group)), adjust = \"none\")) %&gt;%\n        mutate(Model = \"Change score\")) %&gt;%\n        filter(contrast == \"RM10 effect\") %&gt;%\n        ggplot(aes(Model, estimate, fill = Model)) + \n        geom_errorbar(aes(ymin = lower.CL, ymax = upper.CL), width = 0.2) +\n        geom_point(shape = 21, size = 3) + \n        labs(y = \"Estimated mean difference between groups\") + \n        geom_hline(yintercept = 0, lty = 2)\n\n\n\n\n\nGroup estimates from two different models, error bars are 95% confidence intervals.\n\n\n\n\n\nThe change-score model will have lower power compared to the ANCOVA model when the correlation between pre- and post-scores decrease (Vickers 2001)"
  },
  {
    "objectID": "ws3-first-graph.html",
    "href": "ws3-first-graph.html",
    "title": "Your first “real” R plot",
    "section": "",
    "text": "In this workshop, many of us will create our first real plot. For this purpose we will use the ggplot2 package. This choice of package is based on usage, many people use it and therefore you can easily find help online. ggplot2 is also integrated or highly compatible with other commonly used packages in R.\nIt is a good idea to write your code in a R script in this session. Be sure to comment your code extensively, this will help you explain to yourself what you are doing and make it easy to reuse parts of your code.\nA commented line in R code starts with #:\n\n# This is a comments\na &lt;- c(\"roses\", \"are\", \"red\")\n# This is another comment\n\n#### Sections can be specified with several number/hash/pound signs ####\n\n# Sections in scripts and code chunks help you structure your work. \n# In R studio, sections can be located from the editor.\n\nIt is also a good idea to use the comments to write a statement about the purpose of the script or analysis your are writing. Later we will talk about keeping files in a structured way in projects.\n\n\nWe need to start by installing required packages. From the console we can type\n\ninstall.packages(\"tidyverse\")\n\nThis will install the tidyverse package, a package containing many package. On the tidyverse website you can read:\n\nThe tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures.\n\nBy using the tidyverse you will adopt a special dialect of R. A dialect that is very efficient and fairly easy to read (as a human).\nTidyverse will install ggplot2 for you. Notice that you only need to install a package once. It is therefore not a good idea to have an unconditional install.packages() in your script.\nTo get ggplot2 to start working we need to use another command:\n\nlibrary(\"ggplot2\")\n\nNotice that I’ve put ggplot2 inside citation marks \". This is optional!\nThe library function loads all function contained in the package to your R session. This means that you can access and use them.\nFor these exercises we will use another package. The exscidata package contains data sets related to exercise physiology. To install it we need a bit more code. Since exscidata is not on CRAN, but on github we can use the remotes package.\n\n# Install package from cran\nif(!(\"remotes\" %in% .packages(all.available = TRUE))) {\n        install.packages(\"remotes\")\n}\n\n\n# Using remotes, install exscidata from github\nif(!(\"exscidata\" %in% .packages(all.available = TRUE))) {\n        remotes::install_github(\"dhammarstrom/exscidata\")\n}\n\n\n\n# Load the exscidata package\nlibrary(exscidata)\n\n\n\n\n\n\n\nNote\n\n\n\nAbove is a if statement. Ordinarily, the statement can be read as: “If the condition is TRUE, then do whatever is in the brackets”. However, we also use a ! around a parentheses containing the %in% operator. The ! negates the test. If the package name is not contained in the vector of all packages created by the .packages(all.available = TRUE), then we want to install the package.\nThis is a way not having to install packages that are already installed when running your script.\n\n\nWe are now set to load data into our session.\n\n\n\nThe data set we will use in these exercises is called cyclingstudy. You can have a look at the variables by using the help command ?cyclingstudy.\nTo load data from a package we can use data(\"cyclingstudy\")\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nExplain the difference between install.packages() and library()\nWhat is the tidyverse, what will we use ggplot2 for?\nWhat does it mean when a package is not on CRAN?\nIdentify at least one numeric variable in the help pages for cyclingstudy, identify at least one categorical variable.\nWhat happens in your environment when you type data(\"cyclingstudy\")?\n\n\n\n\nA data set can be accessed in multiple ways. We may want to see the data. We can do this by typing View(cyclingstudy) in the console (notice the capital V). Or we can show a couple of rows and columns in the console by typing cyclingstudy.\n\n\n\nThe ggplot function (from the ggplot2 package) takes quite a lot of arguments. However, very few are needed to create a graph.\nThe ggplot2 system uses:\n\ndata → The dataset containing variables to plot\naesthetics → Scales where the data are mapped\ngeometries → Geometric representations of the data\nfacet → A part of the dataset\nstatistical transformations → Summaries of data\ncoordinates → The coordinate space\nthemes → Plot components not linked to data\n\nWe build a graph by mapping variables to different locations and visual characteristics of what is called geoms. This system makes it easy to build different types of graphs using similar syntax.\nWe will start by mapping to continuous variables to the coordinate system. For this exercise, use the variables weight.T1 and sj.max.\nggplot needs to know were the variables can be found, we therefore have to specify the data argument first. Next we map the variables to the x and y coordinates of the graph. You can copy the code below to your R script.\n\nggplot(data = cyclingstudy, aes(x = cmj.max, y = sj.max))\n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nExplain to your friend what we have done so far.\nWhat is mapping in this context?\nDefine “continuous variables”\n\n\n\n\n\n\n\nWe have not yet added graphical representations of the data mapped to coordinates (x and y). These can be added to the plot using the + operator, as we will do below.\nThink about a ggplot as a layered construction. Layers can be added (+) to build the graph you want. Layers are added to the graph sequentially, this means it matters in which order you add them.\nBut what to add?\nThere are many geoms, for an overview go to Help &gt; Cheat Sheets &gt; Data Visualization with ggplot2\n\nTask 1: Identify a geom suitable for two continuous variables, that will show individual data points on x- and y- coordinates.\nTask 2: Call up the help page of the selected geom and find out what you need to add as arguments to the geom\n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nExplain what the argument inherit.aes = TRUE means.\nExplain the sentence (from the help page): “If NULL, the default, the data is inherited from the plot data as specified in the call to ggplot().\n\n\n\n\nBy adding, for example, points to the plot, we will be able to see the data. How would you add the geom that creates points to your plot?\n\n\nShow the code for a possible solution\nggplot(data = cyclingstudy, aes(x = cmj.max, y = sj.max)) + geom_point()\n\n\n\n\n\nTask 1: Using the cheat sheet, beside x and y. What other aesthetics (aes) may be added to the plot that will affect the appearance of the points?\nTask 2: Using pen and paper, draw a figure of the cyclingstudy data using one categorical variable and one continuous variable and hand the figure to the next group.\nTask 3: Code the figure! Create the figure that the other group has drafted for you.\n\n\n\n\n\nCharacteristics such as shapes or colors can also be added to geoms outside the aes(). This means we will override any mapping already given in aes(). As we already have seen, mappings that are inherited from the ggplot function to geoms.\n\n\n\n\nTask 1: Change characteristics of your plot outside data mapping using color and fill, linetype, size and shape. What geoms are responsive to each change?\n\n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nExplain the what the code will produce, without running the code below (google “r shapes” to see what number each shape has.)\n\n\n# Example 1\nggplot(data = cyclingstudy, aes(x = cmj.max, y = sj.max, color = group)) + geom_point(color = \"blue\")\n\n# Example 2\nggplot(data = cyclingstudy, aes(x = cmj.max, y = sj.max, color = group)) + geom_point(shape = 8)\n\n# Example 3\nggplot(data = cyclingstudy, aes(x = cmj.max, y = sj.max, color = weight.T1)) + geom_point(shape = 8)\n\n# Example 4\nggplot(data = cyclingstudy, aes(x = cmj.max, y = sj.max, shape = timepoint)) + geom_point(color = \"red\")\n\n\n\n\n\n\n\n\nIn ggplot2 there are pre-set palettes for colors, orders of shapes and line types etc. Often you would want to control such settings.\nThere are many sources for informed selection of colors, one is colorbrewer2. We\nTo change color scales we use scale_*_* functions that will help you set, e.g., colors manually. In the example below we create a gradient from two colors\n\nggplot(data = cyclingstudy, \n       aes(x = cmj.max, y = sj.max, color = weight.T1)) + \n        geom_point() +\n        scale_colour_gradient(\n                low = \"#e41a1c\",  \n                high = \"#4daf4a\")\n\nDiscrete variables can also be set with colors using scale_color_manual\n\nggplot(data = cyclingstudy, \n       aes(x = cmj.max, y = sj.max, color = timepoint)) + \n        geom_point() +\n        scale_color_manual(values = c(\"#66c2a5\",\"#fc8d62\",\"#8da0cb\",\"#e78ac3\"))\n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nExplain to a friend what scales do\n\n\n\n\n\n\n\nSome aspects of a plot requires that data points are connected together. This essentially means that some a variable needs to group data points. In our example data set, subject gives the identity of each participant. We may use this information to group data points, or connect them with e.g. geom_line(). By adding group = subject to the aes() call in ggplot we will group all geoms that allow grouping.\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nBefore running the code below. Explain what you expect it will show.\n\n\n\n\n\n# Example 1\nggplot(data = cyclingstudy, aes(x = cmj.max, \n                                y = sj.max, \n                                color = group,\n                                shape = timepoint,\n                                group = subject)) + \n        geom_point() +\n        geom_line()\n\n\n\n\nA plot can be quite cluttered, and in this case, give a false impression of a lot of data. The design of this study results in a data set where each participant is tested at multiple time-points. We can therefore create facets based on some aspect of the data, such as time-point.\nThe facet_wrap() and facet_grid() creates facets.\n\n# Example 1\nggplot(data = cyclingstudy, aes(x = cmj.max, \n                                y = sj.max, \n                                color = group)) + \n        geom_point() +\n        facet_wrap(~ timepoint)\n\nAs can be seen in the code above, facet_wrap takes a one-handed formula, the ~ (tilde), indicates a formula. We can read this as “wrap by time-point”.\nIn facet_grid() we will use a two-handed formula, meaning that both sides of the tilde needs information. If we want to group only by rows, we will use a . to indicate that nothing will be used to group by column.\n\nggplot(data = cyclingstudy, aes(x = cmj.max, \n                                y = sj.max, \n                                color = group)) + \n        geom_point() +\n        facet_grid( timepoint ~ .)\n\nIn facet_grid above, we could replace the . with another variable. How would you write the code to facet the graph by group in rows and time-point in columns?\n\n\nShow the code\nggplot(data = cyclingstudy, aes(x = cmj.max, \n                                y = sj.max, \n                                color = group)) + \n        geom_point() +\n        facet_grid( group ~ timepoint)\n\n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nExplain to a friend, what do the plot produced by the code above show? Include everything that is important to reproduce the plot in your description. Try to describe it without pointing at the plot!\n\n\n\n\n\n\n\nAnnotations can be added to plots, these are often user specified, such as labels and plot titles.\nWe specify labels with the labs() function and add annotations with the annotate() function.\n\nggplot(data = cyclingstudy, aes(x = cmj.max, \n                                y = sj.max, \n                                color = group)) + \n        geom_point() +\n        labs(x = \"Maximal Counter movement jump height (cm)\", \n             y = \"Maximal squat jump height (cm)\", \n             title = \"This is the title\", \n             subtitle = \"This is the subtitle\", \n             caption = \"This is a caption\", \n             color = \"This is the group aesthetics\") +\n        \n        annotate(geom = \"text\", x = 25, y = 35, label = \"This is a text annotation\")\n\nNotice that labs()makes use of all aesthetic mappings and annotate requires a geom.\n\n\n\nThe theme() function is used for non-data layers in the plot.\n\n\n\nThere are two commonly used system for combining individual plots, patchwork, https://patchwork.data-imaginist.com/ and cowplot, https://cran.r-project.org/web/packages/cowplot/vignettes/introduction.html.\nThe idea here is create figures with multiple individual figures.\npatchwork has a very simple syntax.\n\nlibrary(patchwork)\n\n\na &lt;- ggplot(data = cyclingstudy, aes(x = cmj.max, \n                                y = sj.max, \n                                color = group)) + \n        geom_point() \n\nb &lt;- ggplot(data = cyclingstudy, aes(y = sj.max, \n                                x = group)) + \n        geom_boxplot() \n\n\nc &lt;- ggplot(data = cyclingstudy, aes(x = timepoint, \n                                y = sj.max, \n                                group = subject)) + \n        geom_line() \n\n\n\n(a | b) / c\n\ncowplot uses plot_grid to arrange plots\n\nlibrary(cowplot)\n\nplot_grid(a, b, c, nrow = 2)\n\nplot_grid can also use a “nested” structure.\n\nplot_grid(plot_grid(a, b, nrow = 1), \n          c, nrow = 2)\n\nIn both frameworks, annotations can be added to plots to indicate panels/sub-plots.\n\n\n\nOutput from ggplot2 can be saved from RStudio using the export buttom. However, a more reproducible manner is to save the output using ggsave\n\n\n\n\nTask 1: Create three separate plots from the cycling data set and save them as objects in your environment.\nTask 2: Use cowplot and patchwork to group the plots together.\nTask 3: Save the plot using ggsave, explore the help pages to find what arguments are needed!\n\n\n\n\n\n\n\nReproduce figure 1.3 from Spiegelhalter (2019)\n\n\n# download data\nchild_heart &lt;- read_csv(\"https://raw.githubusercontent.com/dspiegel29/ArtofStatistics/master/01-1-2-3-child-heart-survival-times/01-1-child-heart-survival-x.csv\")\n\n\nReproduce figure 2.2 and 2.3\n\n\nbeans &lt;- read.csv(\"https://raw.githubusercontent.com/dspiegel29/ArtofStatistics/master/02-2-3-jelly-bean-counts/02-1-bean-data-full-x.csv\", header = FALSE)"
  },
  {
    "objectID": "ws3-first-graph.html#installing-packages",
    "href": "ws3-first-graph.html#installing-packages",
    "title": "Your first “real” R plot",
    "section": "",
    "text": "We need to start by installing required packages. From the console we can type\n\ninstall.packages(\"tidyverse\")\n\nThis will install the tidyverse package, a package containing many package. On the tidyverse website you can read:\n\nThe tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures.\n\nBy using the tidyverse you will adopt a special dialect of R. A dialect that is very efficient and fairly easy to read (as a human).\nTidyverse will install ggplot2 for you. Notice that you only need to install a package once. It is therefore not a good idea to have an unconditional install.packages() in your script.\nTo get ggplot2 to start working we need to use another command:\n\nlibrary(\"ggplot2\")\n\nNotice that I’ve put ggplot2 inside citation marks \". This is optional!\nThe library function loads all function contained in the package to your R session. This means that you can access and use them.\nFor these exercises we will use another package. The exscidata package contains data sets related to exercise physiology. To install it we need a bit more code. Since exscidata is not on CRAN, but on github we can use the remotes package.\n\n# Install package from cran\nif(!(\"remotes\" %in% .packages(all.available = TRUE))) {\n        install.packages(\"remotes\")\n}\n\n\n# Using remotes, install exscidata from github\nif(!(\"exscidata\" %in% .packages(all.available = TRUE))) {\n        remotes::install_github(\"dhammarstrom/exscidata\")\n}\n\n\n\n# Load the exscidata package\nlibrary(exscidata)\n\n\n\n\n\n\n\nNote\n\n\n\nAbove is a if statement. Ordinarily, the statement can be read as: “If the condition is TRUE, then do whatever is in the brackets”. However, we also use a ! around a parentheses containing the %in% operator. The ! negates the test. If the package name is not contained in the vector of all packages created by the .packages(all.available = TRUE), then we want to install the package.\nThis is a way not having to install packages that are already installed when running your script.\n\n\nWe are now set to load data into our session."
  },
  {
    "objectID": "ws3-first-graph.html#loading-data",
    "href": "ws3-first-graph.html#loading-data",
    "title": "Your first “real” R plot",
    "section": "",
    "text": "The data set we will use in these exercises is called cyclingstudy. You can have a look at the variables by using the help command ?cyclingstudy.\nTo load data from a package we can use data(\"cyclingstudy\")\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nExplain the difference between install.packages() and library()\nWhat is the tidyverse, what will we use ggplot2 for?\nWhat does it mean when a package is not on CRAN?\nIdentify at least one numeric variable in the help pages for cyclingstudy, identify at least one categorical variable.\nWhat happens in your environment when you type data(\"cyclingstudy\")?\n\n\n\n\nA data set can be accessed in multiple ways. We may want to see the data. We can do this by typing View(cyclingstudy) in the console (notice the capital V). Or we can show a couple of rows and columns in the console by typing cyclingstudy."
  },
  {
    "objectID": "ws3-first-graph.html#mapping-data-into-aes",
    "href": "ws3-first-graph.html#mapping-data-into-aes",
    "title": "Your first “real” R plot",
    "section": "",
    "text": "The ggplot function (from the ggplot2 package) takes quite a lot of arguments. However, very few are needed to create a graph.\nThe ggplot2 system uses:\n\ndata → The dataset containing variables to plot\naesthetics → Scales where the data are mapped\ngeometries → Geometric representations of the data\nfacet → A part of the dataset\nstatistical transformations → Summaries of data\ncoordinates → The coordinate space\nthemes → Plot components not linked to data\n\nWe build a graph by mapping variables to different locations and visual characteristics of what is called geoms. This system makes it easy to build different types of graphs using similar syntax.\nWe will start by mapping to continuous variables to the coordinate system. For this exercise, use the variables weight.T1 and sj.max.\nggplot needs to know were the variables can be found, we therefore have to specify the data argument first. Next we map the variables to the x and y coordinates of the graph. You can copy the code below to your R script.\n\nggplot(data = cyclingstudy, aes(x = cmj.max, y = sj.max))\n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nExplain to your friend what we have done so far.\nWhat is mapping in this context?\nDefine “continuous variables”"
  },
  {
    "objectID": "ws3-first-graph.html#adding-geometric-representations",
    "href": "ws3-first-graph.html#adding-geometric-representations",
    "title": "Your first “real” R plot",
    "section": "",
    "text": "We have not yet added graphical representations of the data mapped to coordinates (x and y). These can be added to the plot using the + operator, as we will do below.\nThink about a ggplot as a layered construction. Layers can be added (+) to build the graph you want. Layers are added to the graph sequentially, this means it matters in which order you add them.\nBut what to add?\nThere are many geoms, for an overview go to Help &gt; Cheat Sheets &gt; Data Visualization with ggplot2\n\nTask 1: Identify a geom suitable for two continuous variables, that will show individual data points on x- and y- coordinates.\nTask 2: Call up the help page of the selected geom and find out what you need to add as arguments to the geom\n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nExplain what the argument inherit.aes = TRUE means.\nExplain the sentence (from the help page): “If NULL, the default, the data is inherited from the plot data as specified in the call to ggplot().\n\n\n\n\nBy adding, for example, points to the plot, we will be able to see the data. How would you add the geom that creates points to your plot?\n\n\nShow the code for a possible solution\nggplot(data = cyclingstudy, aes(x = cmj.max, y = sj.max)) + geom_point()\n\n\n\n\n\nTask 1: Using the cheat sheet, beside x and y. What other aesthetics (aes) may be added to the plot that will affect the appearance of the points?\nTask 2: Using pen and paper, draw a figure of the cyclingstudy data using one categorical variable and one continuous variable and hand the figure to the next group.\nTask 3: Code the figure! Create the figure that the other group has drafted for you."
  },
  {
    "objectID": "ws3-first-graph.html#changing-colors-and-shapes-outside-mapping",
    "href": "ws3-first-graph.html#changing-colors-and-shapes-outside-mapping",
    "title": "Your first “real” R plot",
    "section": "",
    "text": "Characteristics such as shapes or colors can also be added to geoms outside the aes(). This means we will override any mapping already given in aes(). As we already have seen, mappings that are inherited from the ggplot function to geoms.\n\n\n\n\nTask 1: Change characteristics of your plot outside data mapping using color and fill, linetype, size and shape. What geoms are responsive to each change?\n\n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nExplain the what the code will produce, without running the code below (google “r shapes” to see what number each shape has.)\n\n\n# Example 1\nggplot(data = cyclingstudy, aes(x = cmj.max, y = sj.max, color = group)) + geom_point(color = \"blue\")\n\n# Example 2\nggplot(data = cyclingstudy, aes(x = cmj.max, y = sj.max, color = group)) + geom_point(shape = 8)\n\n# Example 3\nggplot(data = cyclingstudy, aes(x = cmj.max, y = sj.max, color = weight.T1)) + geom_point(shape = 8)\n\n# Example 4\nggplot(data = cyclingstudy, aes(x = cmj.max, y = sj.max, shape = timepoint)) + geom_point(color = \"red\")"
  },
  {
    "objectID": "ws3-first-graph.html#scales",
    "href": "ws3-first-graph.html#scales",
    "title": "Your first “real” R plot",
    "section": "",
    "text": "In ggplot2 there are pre-set palettes for colors, orders of shapes and line types etc. Often you would want to control such settings.\nThere are many sources for informed selection of colors, one is colorbrewer2. We\nTo change color scales we use scale_*_* functions that will help you set, e.g., colors manually. In the example below we create a gradient from two colors\n\nggplot(data = cyclingstudy, \n       aes(x = cmj.max, y = sj.max, color = weight.T1)) + \n        geom_point() +\n        scale_colour_gradient(\n                low = \"#e41a1c\",  \n                high = \"#4daf4a\")\n\nDiscrete variables can also be set with colors using scale_color_manual\n\nggplot(data = cyclingstudy, \n       aes(x = cmj.max, y = sj.max, color = timepoint)) + \n        geom_point() +\n        scale_color_manual(values = c(\"#66c2a5\",\"#fc8d62\",\"#8da0cb\",\"#e78ac3\"))\n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nExplain to a friend what scales do"
  },
  {
    "objectID": "ws3-first-graph.html#grouping-data",
    "href": "ws3-first-graph.html#grouping-data",
    "title": "Your first “real” R plot",
    "section": "",
    "text": "Some aspects of a plot requires that data points are connected together. This essentially means that some a variable needs to group data points. In our example data set, subject gives the identity of each participant. We may use this information to group data points, or connect them with e.g. geom_line(). By adding group = subject to the aes() call in ggplot we will group all geoms that allow grouping.\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nBefore running the code below. Explain what you expect it will show.\n\n\n\n\n\n# Example 1\nggplot(data = cyclingstudy, aes(x = cmj.max, \n                                y = sj.max, \n                                color = group,\n                                shape = timepoint,\n                                group = subject)) + \n        geom_point() +\n        geom_line()"
  },
  {
    "objectID": "ws3-first-graph.html#faceting-plots",
    "href": "ws3-first-graph.html#faceting-plots",
    "title": "Your first “real” R plot",
    "section": "",
    "text": "A plot can be quite cluttered, and in this case, give a false impression of a lot of data. The design of this study results in a data set where each participant is tested at multiple time-points. We can therefore create facets based on some aspect of the data, such as time-point.\nThe facet_wrap() and facet_grid() creates facets.\n\n# Example 1\nggplot(data = cyclingstudy, aes(x = cmj.max, \n                                y = sj.max, \n                                color = group)) + \n        geom_point() +\n        facet_wrap(~ timepoint)\n\nAs can be seen in the code above, facet_wrap takes a one-handed formula, the ~ (tilde), indicates a formula. We can read this as “wrap by time-point”.\nIn facet_grid() we will use a two-handed formula, meaning that both sides of the tilde needs information. If we want to group only by rows, we will use a . to indicate that nothing will be used to group by column.\n\nggplot(data = cyclingstudy, aes(x = cmj.max, \n                                y = sj.max, \n                                color = group)) + \n        geom_point() +\n        facet_grid( timepoint ~ .)\n\nIn facet_grid above, we could replace the . with another variable. How would you write the code to facet the graph by group in rows and time-point in columns?\n\n\nShow the code\nggplot(data = cyclingstudy, aes(x = cmj.max, \n                                y = sj.max, \n                                color = group)) + \n        geom_point() +\n        facet_grid( group ~ timepoint)\n\n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nExplain to a friend, what do the plot produced by the code above show? Include everything that is important to reproduce the plot in your description. Try to describe it without pointing at the plot!"
  },
  {
    "objectID": "ws3-first-graph.html#plot-annotations",
    "href": "ws3-first-graph.html#plot-annotations",
    "title": "Your first “real” R plot",
    "section": "",
    "text": "Annotations can be added to plots, these are often user specified, such as labels and plot titles.\nWe specify labels with the labs() function and add annotations with the annotate() function.\n\nggplot(data = cyclingstudy, aes(x = cmj.max, \n                                y = sj.max, \n                                color = group)) + \n        geom_point() +\n        labs(x = \"Maximal Counter movement jump height (cm)\", \n             y = \"Maximal squat jump height (cm)\", \n             title = \"This is the title\", \n             subtitle = \"This is the subtitle\", \n             caption = \"This is a caption\", \n             color = \"This is the group aesthetics\") +\n        \n        annotate(geom = \"text\", x = 25, y = 35, label = \"This is a text annotation\")\n\nNotice that labs()makes use of all aesthetic mappings and annotate requires a geom."
  },
  {
    "objectID": "ws3-first-graph.html#themes",
    "href": "ws3-first-graph.html#themes",
    "title": "Your first “real” R plot",
    "section": "",
    "text": "The theme() function is used for non-data layers in the plot."
  },
  {
    "objectID": "ws3-first-graph.html#combine-separate-plots",
    "href": "ws3-first-graph.html#combine-separate-plots",
    "title": "Your first “real” R plot",
    "section": "",
    "text": "There are two commonly used system for combining individual plots, patchwork, https://patchwork.data-imaginist.com/ and cowplot, https://cran.r-project.org/web/packages/cowplot/vignettes/introduction.html.\nThe idea here is create figures with multiple individual figures.\npatchwork has a very simple syntax.\n\nlibrary(patchwork)\n\n\na &lt;- ggplot(data = cyclingstudy, aes(x = cmj.max, \n                                y = sj.max, \n                                color = group)) + \n        geom_point() \n\nb &lt;- ggplot(data = cyclingstudy, aes(y = sj.max, \n                                x = group)) + \n        geom_boxplot() \n\n\nc &lt;- ggplot(data = cyclingstudy, aes(x = timepoint, \n                                y = sj.max, \n                                group = subject)) + \n        geom_line() \n\n\n\n(a | b) / c\n\ncowplot uses plot_grid to arrange plots\n\nlibrary(cowplot)\n\nplot_grid(a, b, c, nrow = 2)\n\nplot_grid can also use a “nested” structure.\n\nplot_grid(plot_grid(a, b, nrow = 1), \n          c, nrow = 2)\n\nIn both frameworks, annotations can be added to plots to indicate panels/sub-plots."
  },
  {
    "objectID": "ws3-first-graph.html#saving-output",
    "href": "ws3-first-graph.html#saving-output",
    "title": "Your first “real” R plot",
    "section": "",
    "text": "Output from ggplot2 can be saved from RStudio using the export buttom. However, a more reproducible manner is to save the output using ggsave\n\n\n\n\nTask 1: Create three separate plots from the cycling data set and save them as objects in your environment.\nTask 2: Use cowplot and patchwork to group the plots together.\nTask 3: Save the plot using ggsave, explore the help pages to find what arguments are needed!"
  },
  {
    "objectID": "ws3-first-graph.html#exercises",
    "href": "ws3-first-graph.html#exercises",
    "title": "Your first “real” R plot",
    "section": "",
    "text": "Reproduce figure 1.3 from Spiegelhalter (2019)\n\n\n# download data\nchild_heart &lt;- read_csv(\"https://raw.githubusercontent.com/dspiegel29/ArtofStatistics/master/01-1-2-3-child-heart-survival-times/01-1-child-heart-survival-x.csv\")\n\n\nReproduce figure 2.2 and 2.3\n\n\nbeans &lt;- read.csv(\"https://raw.githubusercontent.com/dspiegel29/ArtofStatistics/master/02-2-3-jelly-bean-counts/02-1-bean-data-full-x.csv\", header = FALSE)"
  },
  {
    "objectID": "ws4b-tables.html",
    "href": "ws4b-tables.html",
    "title": "Data wrangling and tables, part 2",
    "section": "",
    "text": "See part 1 of this workshop here.\n\n\nTables are generated in R in specific output formats. These formats are commonly html, pdf or docx files. The output format type determines how a table is coded by the table generator. However, common to all outputs is the source file. Reports with tables, figures and text are built from Rmarkdown (Rmd) or quarto (qmd) files.\nThe transition between Rmarkdown-files and quarto is effortless as they use the same syntax but quarto has benefits when it comes to publishing.\nQuarto requires the installation of plug-in software. The workshop is possible to follow using a Rmarkdown file also.\nWe will talk more about publishing reports later. For now, we just need a source file to create our table. Start a new quarto or Rmarkdown file and save it somewhere on your computer. Preferably as part of a PROJECT\n\n---\ntitle: \"Untitled\"\n---\n\n\n\n\nThere are at least 15 commonly used packages in R used for tables.1 This really creates a jungle for the user.\nThis course have previously been focusing on knitr and the kable function as well as flextable. Both has their benefits, however, both also has drawbacks.\nDuring this workshop we will focus on the gt package. This package is promising in bridging gaps in previous packages.\nThe gt package needs to be installed from CRAN.\n\n\n\nWe will produce a very basic table of group means at baseline from the cyclingstudy data set.\nFirst we load packages and data\n\nlibrary(gt)\nlibrary(exscidata)\nlibrary(tidyverse)\n\n\ndata(\"cyclingstudy\")\n\nThe next step is to summarize the data of interest. We will use VO2.max, sj.max and weight.T1 in our table, averaged over group\n\ncyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        select(subject, group, VO2.max, sj.max, weight.T1) %&gt;%\n        group_by(group) %&gt;%\n        summarise(VO2max = mean(VO2.max), \n                  sj = mean(sj.max),\n                  weight = mean(weight.T1)) %&gt;%\n        print()\n\n# A tibble: 3 × 4\n  group VO2max    sj weight\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 DECR   4864.  31.9   83.5\n2 INCR   4988.  31.5   81.3\n3 MIX    4419.  28.8   75.3\n\n\nThe output from a code chunk without a table generator will display what your would typically see in the console. To format the output you will need the table generator.\nWe will add the generator as the last part of our pipe.\n\ncyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        select(subject, group, VO2.max, sj.max, weight.T1) %&gt;%\n        group_by(group) %&gt;%\n        summarise(VO2max = mean(VO2.max), \n                  sj = mean(sj.max),\n                  weight = mean(weight.T1)) %&gt;%\n        gt()\n\n\n\n\n\n\n\n\ngroup\nVO2max\nsj\nweight\n\n\n\n\nDECR\n4864.247\n31.90143\n83.52857\n\n\nINCR\n4987.845\n31.50000\n81.25714\n\n\nMIX\n4418.663\n28.78000\n75.30000\n\n\n\n\n\n\n\n\nIf the code is executed without compiling the source-file you will see a table in the Viewer pane in RStudio.\nWe proceed by formatting numbers. gt has many formatting functions created to make formatting of cell values easy. The function fmt_numbers() needs the columns to be formatted, otherwise defaults seems to give reasonable output. Be aware however of the sep_mark = \",\" option.\n\n\n\n\n\n\nExercises (1)\n\n\n\n\nFind documentation for the fmt_number function and find out what how you may format numeric columns, including separation of thousands.\nLook up the argument drop_trailing_zeros in the help page for fmt_number.\n\n\n\n\ncyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        select(subject, group, VO2.max, sj.max, weight.T1) %&gt;%\n        group_by(group) %&gt;%\n        summarise(VO2max = mean(VO2.max), \n                  sj = mean(sj.max),\n                  weight = mean(weight.T1)) %&gt;%\n        gt() %&gt;%\n        fmt_number(columns = VO2max:weight, sep_mark = \"\")\n\n\n\n\n\n\n\n\ngroup\nVO2max\nsj\nweight\n\n\n\n\nDECR\n4864.25\n31.90\n83.53\n\n\nINCR\n4987.84\n31.50\n81.26\n\n\nMIX\n4418.66\n28.78\n75.30\n\n\n\n\n\n\n\n\nFor our variables, VO2max may not need to have two decimal points. We can remove it from the first formatting and add another one.\n\ncyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        select(subject, group, VO2.max, sj.max, weight.T1) %&gt;%\n        group_by(group) %&gt;%\n        summarise(VO2max = mean(VO2.max), \n                  sj = mean(sj.max),\n                  weight = mean(weight.T1)) %&gt;%\n        gt() %&gt;%\n        fmt_number(columns = sj:weight) %&gt;%\n        fmt_number(columns = VO2max, \n                   decimals = 0) \n\n\n\n\n\n\n\n\ngroup\nVO2max\nsj\nweight\n\n\n\n\nDECR\n4,864\n31.90\n83.53\n\n\nINCR\n4,988\n31.50\n81.26\n\n\nMIX\n4,419\n28.78\n75.30\n\n\n\n\n\n\n\n\nColumn names can be added with the cols_label function, added as the other as part of the pipe\n\ncyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        select(subject, group, VO2.max, sj.max, weight.T1) %&gt;%\n        group_by(group) %&gt;%\n        summarise(VO2max = mean(VO2.max), \n                  sj = mean(sj.max),\n                  weight = mean(weight.T1)) %&gt;%\n        gt() %&gt;%\n        fmt_number(columns = sj:weight) %&gt;%\n        fmt_number(columns = VO2max, \n                   decimals = 0)  %&gt;%\n        cols_label(group = \"Group\", \n                   VO2max = md(\"VO&lt;sub&gt;2max&lt;/sub&gt;\"), \n                   sj = \"Squat jump\", \n                   weight = \"Weight\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGroup\nVO2max\nSquat jump\nWeight\n\n\n\n\nDECR\n4,864\n31.90\n83.53\n\n\nINCR\n4,988\n31.50\n81.26\n\n\nMIX\n4,419\n28.78\n75.30\n\n\n\n\n\n\n\n\nNotice that I’ve added the md() function around “VO2max” and added some additional code. This indicates with html tags that 2max should be written in subscript.\nWe can also add units to each column name and print them on a different row as long as we use the md() function to wrap formatted text.\n\ncyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        select(subject, group, VO2.max, sj.max, weight.T1) %&gt;%\n        group_by(group) %&gt;%\n        summarise(VO2max = mean(VO2.max), \n                  sj = mean(sj.max),\n                  weight = mean(weight.T1)) %&gt;%\n        gt() %&gt;%\n        fmt_number(columns = sj:weight) %&gt;%\n        fmt_number(columns = VO2max, \n                   decimals = 0)  %&gt;%\n        cols_label(group = \"Group\", \n                   VO2max = md(\"VO&lt;sub&gt;2max&lt;/sub&gt;&lt;br&gt;&lt;small&gt;(ml min&lt;sup&gt;-1&lt;/sup&gt;)\"), \n                   sj = md(\"Squat jump&lt;br&gt;&lt;small&gt;(cm)\"), \n                   weight = md(\"Weight&lt;br&gt;&lt;small&gt;(kg)\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGroup\nVO2max(ml min-1)\nSquat jump(cm)\nWeight(kg)\n\n\n\n\nDECR\n4,864\n31.90\n83.53\n\n\nINCR\n4,988\n31.50\n81.26\n\n\nMIX\n4,419\n28.78\n75.30\n\n\n\n\n\n\n\n\nIn addition to html tags, we may use plain markdown inside the md() function. We will talk more about text formatting in coming workshops.\nNext we will add a caption. A caption may be added to the gt function directly.\n\ncyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        select(subject, group, VO2.max, sj.max, weight.T1) %&gt;%\n        group_by(group) %&gt;%\n        summarise(VO2max = mean(VO2.max), \n                  sj = mean(sj.max),\n                  weight = mean(weight.T1)) %&gt;%\n        gt(caption = \"Group characteristics\") %&gt;%\n        fmt_number(columns = sj:weight) %&gt;%\n        fmt_number(columns = VO2max, \n                   decimals = 0)  %&gt;%\n        cols_label(group = \"Group\", \n                   VO2max = md(\"VO&lt;sub&gt;2max&lt;/sub&gt;&lt;br&gt;&lt;small&gt;(ml min&lt;sup&gt;-1&lt;/sup&gt;)\"), \n                   sj = md(\"Squat jump&lt;br&gt;&lt;small&gt;(cm)\"), \n                   weight = md(\"Weight&lt;br&gt;&lt;small&gt;(kg)\"))\n\n\n\n\n\n\nGroup characteristics\n\n\n\n\n\n\n\n\nGroup\nVO2max(ml min-1)\nSquat jump(cm)\nWeight(kg)\n\n\n\n\nDECR\n4,864\n31.90\n83.53\n\n\nINCR\n4,988\n31.50\n81.26\n\n\nMIX\n4,419\n28.78\n75.30\n\n\n\n\n\n\n\n\nAn alternative is to add a tab_header with a title. However, we can also add a caption as part of the code chunk. This will replace the caption used in gt. This comes with the additional benefit of being able to label tables. From the quarto documentation we can read that “for tables produced by executable code cells, include a label with a tbl- prefix to make them cross-referenceable”.\nThe top of the code chunk should therefore look like this\n```{r}\n#| label: tbl-char\n#| tbl-cap: \"Group characteristics (code chunk)\"\n```\nThe label part must start with tbl- followed by a unique identifier. This identifier can be used to cross reference the table that will be auto numbered using the syntax @tbl-char, which will produce a link to the table: Table 1\n\ncyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        select(subject, group, VO2.max, sj.max, weight.T1) %&gt;%\n        group_by(group) %&gt;%\n        summarise(VO2max = mean(VO2.max), \n                  sj = mean(sj.max),\n                  weight = mean(weight.T1)) %&gt;%\n        gt(caption = \"Group characteristics (Caption)\") %&gt;%\n        fmt_number(columns = sj:weight) %&gt;%\n        fmt_number(columns = VO2max, \n                   decimals = 0)  %&gt;%\n        cols_label(group = \"Group\", \n                   VO2max = md(\"VO&lt;sub&gt;2max&lt;/sub&gt;&lt;br&gt;&lt;small&gt;(ml min&lt;sup&gt;-1&lt;/sup&gt;)\"), \n                   sj = md(\"Squat jump&lt;br&gt;&lt;small&gt;(cm)\"), \n                   weight = md(\"Weight&lt;br&gt;&lt;small&gt;(kg)\")) %&gt;%\n        tab_header(title = \"Group characteristics (Title)\")\n\n\n\nTable 1: Group characteristics (code chunk)\n\n\n\n\n\n\n\n\nGroup characteristics (Caption)\n\n\n\n\n\n\n\n\nGroup characteristics (Title)\n\n\nGroup\nVO2max(ml min-1)\nSquat jump(cm)\nWeight(kg)\n\n\n\n\nDECR\n4,864\n31.90\n83.53\n\n\nINCR\n4,988\n31.50\n81.26\n\n\nMIX\n4,419\n28.78\n75.30\n\n\n\n\n\n\n\n\n\n\n\nFinally we will add footnotes to the table. This is done using the tab_footnote function.\n\ncyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        select(subject, group, VO2.max, sj.max, weight.T1) %&gt;%\n        group_by(group) %&gt;%\n        summarise(VO2max = mean(VO2.max), \n                  sj = mean(sj.max),\n                  weight = mean(weight.T1)) %&gt;%\n        gt(caption = \"Group characteristics (Caption)\") %&gt;%\n        fmt_number(columns = sj:weight) %&gt;%\n        fmt_number(columns = VO2max, \n                   decimals = 0)  %&gt;%\n        cols_label(group = \"Group\", \n                   VO2max = md(\"VO&lt;sub&gt;2max&lt;/sub&gt;&lt;br&gt;&lt;small&gt;(ml min&lt;sup&gt;-1&lt;/sup&gt;)\"), \n                   sj = md(\"Squat jump&lt;br&gt;&lt;small&gt;(cm)\"), \n                   weight = md(\"Weight&lt;br&gt;&lt;small&gt;(kg)\")) %&gt;%\n        tab_header(title = \"Group characteristics (Title)\") %&gt;%\n        tab_footnote(footnote = \"Values are means\")\n\n\n\nTable 2: Group characteristics (code chunk)\n\n\n\n\n\n\n\n\nGroup characteristics (Caption)\n\n\n\n\n\n\n\n\nGroup characteristics (Title)\n\n\nGroup\nVO2max(ml min-1)\nSquat jump(cm)\nWeight(kg)\n\n\n\n\nDECR\n4,864\n31.90\n83.53\n\n\nINCR\n4,988\n31.50\n81.26\n\n\nMIX\n4,419\n28.78\n75.30\n\n\n\nValues are means\n\n\n\n\n\n\n\n\n\n\n\n\nWe can specify a location for a reference using the locations argument.\n\ncyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        select(subject, group, VO2.max, sj.max, weight.T1) %&gt;%\n        group_by(group) %&gt;%\n        summarise(VO2max = mean(VO2.max), \n                  sj = mean(sj.max),\n                  weight = mean(weight.T1)) %&gt;%\n        gt(caption = \"Group characteristics (Caption)\") %&gt;%\n        fmt_number(columns = sj:weight) %&gt;%\n        fmt_number(columns = VO2max, \n                   decimals = 0)  %&gt;%\n        cols_label(group = \"Group\", \n                   VO2max = md(\"VO&lt;sub&gt;2max&lt;/sub&gt;&lt;br&gt;&lt;small&gt;(ml min&lt;sup&gt;-1&lt;/sup&gt;)\"), \n                   sj = md(\"Squat jump&lt;br&gt;&lt;small&gt;(cm)\"), \n                   weight = md(\"Weight&lt;br&gt;&lt;small&gt;(kg)\")) %&gt;%\n        tab_header(title = \"Group characteristics (Title)\") %&gt;%\n        tab_footnote(footnote = \"Values are means for weight\", \n        locations = cells_column_labels(columns = weight))\n\n\n\nTable 3: Group characteristics (code chunk)\n\n\n\n\n\n\n\n\nGroup characteristics (Caption)\n\n\n\n\n\n\n\n\nGroup characteristics (Title)\n\n\nGroup\nVO2max(ml min-1)\nSquat jump(cm)\nWeight(kg)\n1\n\n\n\n\nDECR\n4,864\n31.90\n83.53\n\n\nINCR\n4,988\n31.50\n81.26\n\n\nMIX\n4,419\n28.78\n75.30\n\n\n\n1 Values are means for weight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe might want to combine multiple statistics in one table, for example the mean and stamdard deviation for each variable. Below I use a different approach to summarise values per variable and group by first adding a pivot_longer which makes it possible to do this operation without creating multiple new variables.\nA mean-and-standard-deviation-variable could be created by combining vectors. The mean and standard deviation is commonly presented as mean (SD). Data from a column of means and a column of SD’s can be combined to create a nice display using the paste0() function. Example:\n\ndata.frame(m = c(46.7, 47.89, 43.5),  # A vector of means\n           s = c(4.21, 4.666, 3.1)) %&gt;% # A vector of SD's\n        mutate(stat = paste0(round(m, 1), \n                             \" (\",\n                             round(s, 1), \n                             \")\")) %&gt;%\n        print()\n\nHowever, the gt package has a nice helper function that makes it possible to do this in the gt part of the pipe.\n\ncyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        select(subject, group, VO2.max, sj.max, weight.T1) %&gt;%\n1        pivot_longer(names_to = \"variable\",\n                     values_to = \"value\",\n                     cols = VO2.max:weight.T1) %&gt;%\n2        group_by(group, variable) %&gt;%\n        summarise(mean = mean(value),\n                  sd = sd(value)) %&gt;%\n3        pivot_wider(names_from = variable,\n                        values_from = c(mean, sd)) %&gt;%\n        select(group, mean_VO2.max, sd_VO2.max,\n                        mean_sj.max, sd_sj.max,\n                        mean_weight.T1, sd_weight.T1) %&gt;%\n4        ungroup() %&gt;%\n        \n        gt(caption = \"Group characteristics (Caption)\") %&gt;%\n        \n5        fmt_number(columns = mean_VO2.max:sd_VO2.max, decimals = 0) %&gt;%\n        fmt_number(columns = mean_sj.max:sd_weight.T1, decimals = 1) %&gt;%\n         \n6        cols_merge(columns = c(\"mean_VO2.max\", \"sd_VO2.max\"),\n                   pattern = \"&lt;&lt;{1}&gt;&gt; &lt;&lt;({2})&gt;&gt;\") %&gt;%\n        \n        cols_merge(columns = c(\"mean_sj.max\", \"sd_sj.max\"),\n                   pattern = \"&lt;&lt;{1}&gt;&gt; &lt;&lt;({2})&gt;&gt;\") %&gt;%\n        \n        cols_merge(columns = c(\"mean_weight.T1\", \"sd_weight.T1\"),\n                   pattern = \"&lt;&lt;{1}&gt;&gt; &lt;&lt;({2})&gt;&gt;\") %&gt;%\n                   \n\n        cols_label(group = \"Group\", \n                   mean_VO2.max = md(\"VO&lt;sub&gt;2max&lt;/sub&gt;&lt;br&gt;&lt;small&gt;(ml min&lt;sup&gt;-1&lt;/sup&gt;)\"), \n                   mean_sj.max = md(\"Squat jump&lt;br&gt;&lt;small&gt;(cm)\"), \n                   mean_weight.T1 = md(\"Weight&lt;br&gt;&lt;small&gt;(kg)\")) %&gt;%\n        tab_header(title = \"Group characteristics (Title)\") %&gt;%\n        tab_footnote(footnote = \"Values are means for weight\")\n\n\n1\n\nUsing pivot_longer to combine multiple variables in one value/name combination.\n\n2\n\nHere we group by variable and group and create two new variables in a summarised data frame. This requires less code than the above example.\n\n3\n\nMaking the table “wide” again and using select to sort the columns.\n\n4\n\ngt tables are group-sensitive. This means that if we have a grouped data frame/tibble, gt will use this grouping when it creates the table.\n\n5\n\nIt is good practice to have the same number of decimals in both the mean and SD.\n\n6\n\nThis is where we combine the columns using column_merge, for each new column we need a specified column_merge.\n\n\n\n\n`summarise()` has grouped output by 'group'. You can override using the\n`.groups` argument.\n\n\n\n\nTable 4: Group characteristics (code chunk)\n\n\n\n\n\n\n\n\nGroup characteristics (Caption)\n\n\n\n\n\n\n\n\nGroup characteristics (Title)\n\n\nGroup\nVO2max(ml min-1)\nSquat jump(cm)\nWeight(kg)\n\n\n\n\nDECR\n4,864 (541)\n31.9 (2.6)\n83.5 (10.7)\n\n\nINCR\n4,988 (488)\n31.5 (2.6)\n81.3 (7.9)\n\n\nMIX\n4,419 (253)\n28.8 (4.0)\n75.3 (9.9)\n\n\n\nValues are means for weight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData from (Haun et al. 2018) are part of the exscidata package as the hypertrophy data set. Access it by using\n\nlibrary(exscidata); data(hypertrophy)\n\nglimpse(hypertrophy)\n\nTry to find as many variables presented in Table 1 in the original publication as possible in the available data and format your version of the table to the best of your ability!"
  },
  {
    "objectID": "ws4b-tables.html#a-file-type-for-reports",
    "href": "ws4b-tables.html#a-file-type-for-reports",
    "title": "Data wrangling and tables, part 2",
    "section": "",
    "text": "Tables are generated in R in specific output formats. These formats are commonly html, pdf or docx files. The output format type determines how a table is coded by the table generator. However, common to all outputs is the source file. Reports with tables, figures and text are built from Rmarkdown (Rmd) or quarto (qmd) files.\nThe transition between Rmarkdown-files and quarto is effortless as they use the same syntax but quarto has benefits when it comes to publishing.\nQuarto requires the installation of plug-in software. The workshop is possible to follow using a Rmarkdown file also.\nWe will talk more about publishing reports later. For now, we just need a source file to create our table. Start a new quarto or Rmarkdown file and save it somewhere on your computer. Preferably as part of a PROJECT\n\n---\ntitle: \"Untitled\"\n---"
  },
  {
    "objectID": "ws4b-tables.html#table-generators",
    "href": "ws4b-tables.html#table-generators",
    "title": "Data wrangling and tables, part 2",
    "section": "",
    "text": "There are at least 15 commonly used packages in R used for tables.1 This really creates a jungle for the user.\nThis course have previously been focusing on knitr and the kable function as well as flextable. Both has their benefits, however, both also has drawbacks.\nDuring this workshop we will focus on the gt package. This package is promising in bridging gaps in previous packages.\nThe gt package needs to be installed from CRAN."
  },
  {
    "objectID": "ws4b-tables.html#a-basic-table",
    "href": "ws4b-tables.html#a-basic-table",
    "title": "Data wrangling and tables, part 2",
    "section": "",
    "text": "We will produce a very basic table of group means at baseline from the cyclingstudy data set.\nFirst we load packages and data\n\nlibrary(gt)\nlibrary(exscidata)\nlibrary(tidyverse)\n\n\ndata(\"cyclingstudy\")\n\nThe next step is to summarize the data of interest. We will use VO2.max, sj.max and weight.T1 in our table, averaged over group\n\ncyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        select(subject, group, VO2.max, sj.max, weight.T1) %&gt;%\n        group_by(group) %&gt;%\n        summarise(VO2max = mean(VO2.max), \n                  sj = mean(sj.max),\n                  weight = mean(weight.T1)) %&gt;%\n        print()\n\n# A tibble: 3 × 4\n  group VO2max    sj weight\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 DECR   4864.  31.9   83.5\n2 INCR   4988.  31.5   81.3\n3 MIX    4419.  28.8   75.3\n\n\nThe output from a code chunk without a table generator will display what your would typically see in the console. To format the output you will need the table generator.\nWe will add the generator as the last part of our pipe.\n\ncyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        select(subject, group, VO2.max, sj.max, weight.T1) %&gt;%\n        group_by(group) %&gt;%\n        summarise(VO2max = mean(VO2.max), \n                  sj = mean(sj.max),\n                  weight = mean(weight.T1)) %&gt;%\n        gt()\n\n\n\n\n\n\n\n\ngroup\nVO2max\nsj\nweight\n\n\n\n\nDECR\n4864.247\n31.90143\n83.52857\n\n\nINCR\n4987.845\n31.50000\n81.25714\n\n\nMIX\n4418.663\n28.78000\n75.30000\n\n\n\n\n\n\n\n\nIf the code is executed without compiling the source-file you will see a table in the Viewer pane in RStudio.\nWe proceed by formatting numbers. gt has many formatting functions created to make formatting of cell values easy. The function fmt_numbers() needs the columns to be formatted, otherwise defaults seems to give reasonable output. Be aware however of the sep_mark = \",\" option.\n\n\n\n\n\n\nExercises (1)\n\n\n\n\nFind documentation for the fmt_number function and find out what how you may format numeric columns, including separation of thousands.\nLook up the argument drop_trailing_zeros in the help page for fmt_number.\n\n\n\n\ncyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        select(subject, group, VO2.max, sj.max, weight.T1) %&gt;%\n        group_by(group) %&gt;%\n        summarise(VO2max = mean(VO2.max), \n                  sj = mean(sj.max),\n                  weight = mean(weight.T1)) %&gt;%\n        gt() %&gt;%\n        fmt_number(columns = VO2max:weight, sep_mark = \"\")\n\n\n\n\n\n\n\n\ngroup\nVO2max\nsj\nweight\n\n\n\n\nDECR\n4864.25\n31.90\n83.53\n\n\nINCR\n4987.84\n31.50\n81.26\n\n\nMIX\n4418.66\n28.78\n75.30\n\n\n\n\n\n\n\n\nFor our variables, VO2max may not need to have two decimal points. We can remove it from the first formatting and add another one.\n\ncyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        select(subject, group, VO2.max, sj.max, weight.T1) %&gt;%\n        group_by(group) %&gt;%\n        summarise(VO2max = mean(VO2.max), \n                  sj = mean(sj.max),\n                  weight = mean(weight.T1)) %&gt;%\n        gt() %&gt;%\n        fmt_number(columns = sj:weight) %&gt;%\n        fmt_number(columns = VO2max, \n                   decimals = 0) \n\n\n\n\n\n\n\n\ngroup\nVO2max\nsj\nweight\n\n\n\n\nDECR\n4,864\n31.90\n83.53\n\n\nINCR\n4,988\n31.50\n81.26\n\n\nMIX\n4,419\n28.78\n75.30\n\n\n\n\n\n\n\n\nColumn names can be added with the cols_label function, added as the other as part of the pipe\n\ncyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        select(subject, group, VO2.max, sj.max, weight.T1) %&gt;%\n        group_by(group) %&gt;%\n        summarise(VO2max = mean(VO2.max), \n                  sj = mean(sj.max),\n                  weight = mean(weight.T1)) %&gt;%\n        gt() %&gt;%\n        fmt_number(columns = sj:weight) %&gt;%\n        fmt_number(columns = VO2max, \n                   decimals = 0)  %&gt;%\n        cols_label(group = \"Group\", \n                   VO2max = md(\"VO&lt;sub&gt;2max&lt;/sub&gt;\"), \n                   sj = \"Squat jump\", \n                   weight = \"Weight\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGroup\nVO2max\nSquat jump\nWeight\n\n\n\n\nDECR\n4,864\n31.90\n83.53\n\n\nINCR\n4,988\n31.50\n81.26\n\n\nMIX\n4,419\n28.78\n75.30\n\n\n\n\n\n\n\n\nNotice that I’ve added the md() function around “VO2max” and added some additional code. This indicates with html tags that 2max should be written in subscript.\nWe can also add units to each column name and print them on a different row as long as we use the md() function to wrap formatted text.\n\ncyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        select(subject, group, VO2.max, sj.max, weight.T1) %&gt;%\n        group_by(group) %&gt;%\n        summarise(VO2max = mean(VO2.max), \n                  sj = mean(sj.max),\n                  weight = mean(weight.T1)) %&gt;%\n        gt() %&gt;%\n        fmt_number(columns = sj:weight) %&gt;%\n        fmt_number(columns = VO2max, \n                   decimals = 0)  %&gt;%\n        cols_label(group = \"Group\", \n                   VO2max = md(\"VO&lt;sub&gt;2max&lt;/sub&gt;&lt;br&gt;&lt;small&gt;(ml min&lt;sup&gt;-1&lt;/sup&gt;)\"), \n                   sj = md(\"Squat jump&lt;br&gt;&lt;small&gt;(cm)\"), \n                   weight = md(\"Weight&lt;br&gt;&lt;small&gt;(kg)\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGroup\nVO2max(ml min-1)\nSquat jump(cm)\nWeight(kg)\n\n\n\n\nDECR\n4,864\n31.90\n83.53\n\n\nINCR\n4,988\n31.50\n81.26\n\n\nMIX\n4,419\n28.78\n75.30\n\n\n\n\n\n\n\n\nIn addition to html tags, we may use plain markdown inside the md() function. We will talk more about text formatting in coming workshops.\nNext we will add a caption. A caption may be added to the gt function directly.\n\ncyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        select(subject, group, VO2.max, sj.max, weight.T1) %&gt;%\n        group_by(group) %&gt;%\n        summarise(VO2max = mean(VO2.max), \n                  sj = mean(sj.max),\n                  weight = mean(weight.T1)) %&gt;%\n        gt(caption = \"Group characteristics\") %&gt;%\n        fmt_number(columns = sj:weight) %&gt;%\n        fmt_number(columns = VO2max, \n                   decimals = 0)  %&gt;%\n        cols_label(group = \"Group\", \n                   VO2max = md(\"VO&lt;sub&gt;2max&lt;/sub&gt;&lt;br&gt;&lt;small&gt;(ml min&lt;sup&gt;-1&lt;/sup&gt;)\"), \n                   sj = md(\"Squat jump&lt;br&gt;&lt;small&gt;(cm)\"), \n                   weight = md(\"Weight&lt;br&gt;&lt;small&gt;(kg)\"))\n\n\n\n\n\n\nGroup characteristics\n\n\n\n\n\n\n\n\nGroup\nVO2max(ml min-1)\nSquat jump(cm)\nWeight(kg)\n\n\n\n\nDECR\n4,864\n31.90\n83.53\n\n\nINCR\n4,988\n31.50\n81.26\n\n\nMIX\n4,419\n28.78\n75.30\n\n\n\n\n\n\n\n\nAn alternative is to add a tab_header with a title. However, we can also add a caption as part of the code chunk. This will replace the caption used in gt. This comes with the additional benefit of being able to label tables. From the quarto documentation we can read that “for tables produced by executable code cells, include a label with a tbl- prefix to make them cross-referenceable”.\nThe top of the code chunk should therefore look like this\n```{r}\n#| label: tbl-char\n#| tbl-cap: \"Group characteristics (code chunk)\"\n```\nThe label part must start with tbl- followed by a unique identifier. This identifier can be used to cross reference the table that will be auto numbered using the syntax @tbl-char, which will produce a link to the table: Table 1\n\ncyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        select(subject, group, VO2.max, sj.max, weight.T1) %&gt;%\n        group_by(group) %&gt;%\n        summarise(VO2max = mean(VO2.max), \n                  sj = mean(sj.max),\n                  weight = mean(weight.T1)) %&gt;%\n        gt(caption = \"Group characteristics (Caption)\") %&gt;%\n        fmt_number(columns = sj:weight) %&gt;%\n        fmt_number(columns = VO2max, \n                   decimals = 0)  %&gt;%\n        cols_label(group = \"Group\", \n                   VO2max = md(\"VO&lt;sub&gt;2max&lt;/sub&gt;&lt;br&gt;&lt;small&gt;(ml min&lt;sup&gt;-1&lt;/sup&gt;)\"), \n                   sj = md(\"Squat jump&lt;br&gt;&lt;small&gt;(cm)\"), \n                   weight = md(\"Weight&lt;br&gt;&lt;small&gt;(kg)\")) %&gt;%\n        tab_header(title = \"Group characteristics (Title)\")\n\n\n\nTable 1: Group characteristics (code chunk)\n\n\n\n\n\n\n\n\nGroup characteristics (Caption)\n\n\n\n\n\n\n\n\nGroup characteristics (Title)\n\n\nGroup\nVO2max(ml min-1)\nSquat jump(cm)\nWeight(kg)\n\n\n\n\nDECR\n4,864\n31.90\n83.53\n\n\nINCR\n4,988\n31.50\n81.26\n\n\nMIX\n4,419\n28.78\n75.30\n\n\n\n\n\n\n\n\n\n\n\nFinally we will add footnotes to the table. This is done using the tab_footnote function.\n\ncyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        select(subject, group, VO2.max, sj.max, weight.T1) %&gt;%\n        group_by(group) %&gt;%\n        summarise(VO2max = mean(VO2.max), \n                  sj = mean(sj.max),\n                  weight = mean(weight.T1)) %&gt;%\n        gt(caption = \"Group characteristics (Caption)\") %&gt;%\n        fmt_number(columns = sj:weight) %&gt;%\n        fmt_number(columns = VO2max, \n                   decimals = 0)  %&gt;%\n        cols_label(group = \"Group\", \n                   VO2max = md(\"VO&lt;sub&gt;2max&lt;/sub&gt;&lt;br&gt;&lt;small&gt;(ml min&lt;sup&gt;-1&lt;/sup&gt;)\"), \n                   sj = md(\"Squat jump&lt;br&gt;&lt;small&gt;(cm)\"), \n                   weight = md(\"Weight&lt;br&gt;&lt;small&gt;(kg)\")) %&gt;%\n        tab_header(title = \"Group characteristics (Title)\") %&gt;%\n        tab_footnote(footnote = \"Values are means\")\n\n\n\nTable 2: Group characteristics (code chunk)\n\n\n\n\n\n\n\n\nGroup characteristics (Caption)\n\n\n\n\n\n\n\n\nGroup characteristics (Title)\n\n\nGroup\nVO2max(ml min-1)\nSquat jump(cm)\nWeight(kg)\n\n\n\n\nDECR\n4,864\n31.90\n83.53\n\n\nINCR\n4,988\n31.50\n81.26\n\n\nMIX\n4,419\n28.78\n75.30\n\n\n\nValues are means\n\n\n\n\n\n\n\n\n\n\n\n\nWe can specify a location for a reference using the locations argument.\n\ncyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        select(subject, group, VO2.max, sj.max, weight.T1) %&gt;%\n        group_by(group) %&gt;%\n        summarise(VO2max = mean(VO2.max), \n                  sj = mean(sj.max),\n                  weight = mean(weight.T1)) %&gt;%\n        gt(caption = \"Group characteristics (Caption)\") %&gt;%\n        fmt_number(columns = sj:weight) %&gt;%\n        fmt_number(columns = VO2max, \n                   decimals = 0)  %&gt;%\n        cols_label(group = \"Group\", \n                   VO2max = md(\"VO&lt;sub&gt;2max&lt;/sub&gt;&lt;br&gt;&lt;small&gt;(ml min&lt;sup&gt;-1&lt;/sup&gt;)\"), \n                   sj = md(\"Squat jump&lt;br&gt;&lt;small&gt;(cm)\"), \n                   weight = md(\"Weight&lt;br&gt;&lt;small&gt;(kg)\")) %&gt;%\n        tab_header(title = \"Group characteristics (Title)\") %&gt;%\n        tab_footnote(footnote = \"Values are means for weight\", \n        locations = cells_column_labels(columns = weight))\n\n\n\nTable 3: Group characteristics (code chunk)\n\n\n\n\n\n\n\n\nGroup characteristics (Caption)\n\n\n\n\n\n\n\n\nGroup characteristics (Title)\n\n\nGroup\nVO2max(ml min-1)\nSquat jump(cm)\nWeight(kg)\n1\n\n\n\n\nDECR\n4,864\n31.90\n83.53\n\n\nINCR\n4,988\n31.50\n81.26\n\n\nMIX\n4,419\n28.78\n75.30\n\n\n\n1 Values are means for weight"
  },
  {
    "objectID": "ws4b-tables.html#adding-more-statistics",
    "href": "ws4b-tables.html#adding-more-statistics",
    "title": "Data wrangling and tables, part 2",
    "section": "",
    "text": "We might want to combine multiple statistics in one table, for example the mean and stamdard deviation for each variable. Below I use a different approach to summarise values per variable and group by first adding a pivot_longer which makes it possible to do this operation without creating multiple new variables.\nA mean-and-standard-deviation-variable could be created by combining vectors. The mean and standard deviation is commonly presented as mean (SD). Data from a column of means and a column of SD’s can be combined to create a nice display using the paste0() function. Example:\n\ndata.frame(m = c(46.7, 47.89, 43.5),  # A vector of means\n           s = c(4.21, 4.666, 3.1)) %&gt;% # A vector of SD's\n        mutate(stat = paste0(round(m, 1), \n                             \" (\",\n                             round(s, 1), \n                             \")\")) %&gt;%\n        print()\n\nHowever, the gt package has a nice helper function that makes it possible to do this in the gt part of the pipe.\n\ncyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        select(subject, group, VO2.max, sj.max, weight.T1) %&gt;%\n1        pivot_longer(names_to = \"variable\",\n                     values_to = \"value\",\n                     cols = VO2.max:weight.T1) %&gt;%\n2        group_by(group, variable) %&gt;%\n        summarise(mean = mean(value),\n                  sd = sd(value)) %&gt;%\n3        pivot_wider(names_from = variable,\n                        values_from = c(mean, sd)) %&gt;%\n        select(group, mean_VO2.max, sd_VO2.max,\n                        mean_sj.max, sd_sj.max,\n                        mean_weight.T1, sd_weight.T1) %&gt;%\n4        ungroup() %&gt;%\n        \n        gt(caption = \"Group characteristics (Caption)\") %&gt;%\n        \n5        fmt_number(columns = mean_VO2.max:sd_VO2.max, decimals = 0) %&gt;%\n        fmt_number(columns = mean_sj.max:sd_weight.T1, decimals = 1) %&gt;%\n         \n6        cols_merge(columns = c(\"mean_VO2.max\", \"sd_VO2.max\"),\n                   pattern = \"&lt;&lt;{1}&gt;&gt; &lt;&lt;({2})&gt;&gt;\") %&gt;%\n        \n        cols_merge(columns = c(\"mean_sj.max\", \"sd_sj.max\"),\n                   pattern = \"&lt;&lt;{1}&gt;&gt; &lt;&lt;({2})&gt;&gt;\") %&gt;%\n        \n        cols_merge(columns = c(\"mean_weight.T1\", \"sd_weight.T1\"),\n                   pattern = \"&lt;&lt;{1}&gt;&gt; &lt;&lt;({2})&gt;&gt;\") %&gt;%\n                   \n\n        cols_label(group = \"Group\", \n                   mean_VO2.max = md(\"VO&lt;sub&gt;2max&lt;/sub&gt;&lt;br&gt;&lt;small&gt;(ml min&lt;sup&gt;-1&lt;/sup&gt;)\"), \n                   mean_sj.max = md(\"Squat jump&lt;br&gt;&lt;small&gt;(cm)\"), \n                   mean_weight.T1 = md(\"Weight&lt;br&gt;&lt;small&gt;(kg)\")) %&gt;%\n        tab_header(title = \"Group characteristics (Title)\") %&gt;%\n        tab_footnote(footnote = \"Values are means for weight\")\n\n\n1\n\nUsing pivot_longer to combine multiple variables in one value/name combination.\n\n2\n\nHere we group by variable and group and create two new variables in a summarised data frame. This requires less code than the above example.\n\n3\n\nMaking the table “wide” again and using select to sort the columns.\n\n4\n\ngt tables are group-sensitive. This means that if we have a grouped data frame/tibble, gt will use this grouping when it creates the table.\n\n5\n\nIt is good practice to have the same number of decimals in both the mean and SD.\n\n6\n\nThis is where we combine the columns using column_merge, for each new column we need a specified column_merge.\n\n\n\n\n`summarise()` has grouped output by 'group'. You can override using the\n`.groups` argument.\n\n\n\n\nTable 4: Group characteristics (code chunk)\n\n\n\n\n\n\n\n\nGroup characteristics (Caption)\n\n\n\n\n\n\n\n\nGroup characteristics (Title)\n\n\nGroup\nVO2max(ml min-1)\nSquat jump(cm)\nWeight(kg)\n\n\n\n\nDECR\n4,864 (541)\n31.9 (2.6)\n83.5 (10.7)\n\n\nINCR\n4,988 (488)\n31.5 (2.6)\n81.3 (7.9)\n\n\nMIX\n4,419 (253)\n28.8 (4.0)\n75.3 (9.9)\n\n\n\nValues are means for weight"
  },
  {
    "objectID": "ws4b-tables.html#exercisehomework-recreating-table-1-in-haun2018.",
    "href": "ws4b-tables.html#exercisehomework-recreating-table-1-in-haun2018.",
    "title": "Data wrangling and tables, part 2",
    "section": "",
    "text": "Data from (Haun et al. 2018) are part of the exscidata package as the hypertrophy data set. Access it by using\n\nlibrary(exscidata); data(hypertrophy)\n\nglimpse(hypertrophy)\n\nTry to find as many variables presented in Table 1 in the original publication as possible in the available data and format your version of the table to the best of your ability!"
  },
  {
    "objectID": "ws4b-tables.html#footnotes",
    "href": "ws4b-tables.html#footnotes",
    "title": "Data wrangling and tables, part 2",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee the list in the R Markdown Cookbook↩︎"
  },
  {
    "objectID": "ws8-linear-model.html",
    "href": "ws8-linear-model.html",
    "title": "The Linear Model",
    "section": "",
    "text": "A straight line can be described using the simple equation \\(y = a + bx\\). Here \\(a\\) is the intercept (\\(y\\) when \\(x=0\\)) and \\(b\\) is the slope (difference in \\(y\\) for every unit difference in \\(x\\)).\nThe line is heading upwards if \\(b &gt; 0\\) and downwards if \\(b &lt; 0\\).\n\n\n\n\n\nStraight lines following the equation \\(y = a + bx\\)"
  },
  {
    "objectID": "ws8-linear-model.html#the-equation-of-a-straight-line",
    "href": "ws8-linear-model.html#the-equation-of-a-straight-line",
    "title": "The Linear Model",
    "section": "",
    "text": "A straight line can be described using the simple equation \\(y = a + bx\\). Here \\(a\\) is the intercept (\\(y\\) when \\(x=0\\)) and \\(b\\) is the slope (difference in \\(y\\) for every unit difference in \\(x\\)).\nThe line is heading upwards if \\(b &gt; 0\\) and downwards if \\(b &lt; 0\\).\n\n\n\n\n\nStraight lines following the equation \\(y = a + bx\\)"
  },
  {
    "objectID": "ws8-linear-model.html#creating-models-of-data-using-straight-lines",
    "href": "ws8-linear-model.html#creating-models-of-data-using-straight-lines",
    "title": "The Linear Model",
    "section": "Creating models of data using straight lines",
    "text": "Creating models of data using straight lines\n\nNotes on models1\n\nWe can use the straight line to create a model that describes data.\nA statistical model is an abstract representation of the underlying data that we hope captures some characteristics of the real world(!).\nThe straight line effectively avoids complexity of the real world\nStatistical models are constructions made for some purpose (e.g., prediction or explanation)\n\nWe will start our journey in statistical modelling using straight lines.\n\n\n\n\n\n\nGroup work\n\n\n\nStraight lines can be added to a ggplot using the geom_abline() function. It takes the arguments slope and intercept. For example:\n\n\nCode\nggplot(data = data.frame(x = c(1, 2, 3), \n                         y = c(3, 2, 1)), \n       aes(x, y)) + geom_point() +\n        \n        geom_abline(slope = 1, intercept = 0, color = \"steelblue\", linewidth  = 1) + \n        \n        scale_y_continuous(limits = c(0, 3)) +\n        \n        scale_x_continuous(limits = c(0, 3))\n\n\n\n\n\n\n\n\n\n\nWhat line would best describe the data shown in the plots below? Copy the code in the code chunk below the figure and add straight lines that best describes the data.\nDiscuss: What made you decide where to place the lines?\n\n\n\n\n\n\n\n\n\n\n\n\nCode\na &lt;- ggplot(data.frame(x = c(1, 2, 3), \n                  y = c(1, 2, 3)), \n       aes(x, y)) +\n        geom_point(size = 3) +\n        scale_y_continuous(limits = c(0, 5)) +\n        scale_x_continuous(limits = c(0, 5)) \n\nb &lt;- ggplot(data.frame(x = c(1, 1, 2), \n                  y = c(1, 2, 2)), \n       aes(x, y)) +\n        geom_point(size = 3) +\n        scale_y_continuous(limits = c(0, 5)) +\n        scale_x_continuous(limits = c(0, 5)) \n        \nc &lt;- ggplot(data.frame(x = c(1, 1, 2, 2), \n                  y = c(1, 2, 1, 2)), \n       aes(x, y)) +\n        geom_point(size = 3) +\n        scale_y_continuous(limits = c(0, 5)) +\n        scale_x_continuous(limits = c(0, 5))\n\n\nd &lt;- ggplot(data.frame(x = c(1, 2, 3, 3, 4, 5), \n                  y = c(2, 3, 1, 3, 2, 3)), \n       aes(x, y)) +\n        geom_point(size = 3) +\n        scale_y_continuous(limits = c(0, 5)) +\n        scale_x_continuous(limits = c(0, 5))"
  },
  {
    "objectID": "ws8-linear-model.html#fitting-a-straight-line-to-data",
    "href": "ws8-linear-model.html#fitting-a-straight-line-to-data",
    "title": "The Linear Model",
    "section": "Fitting a straight line to data",
    "text": "Fitting a straight line to data\nTo achieve the goal of describing data with a model, a straight line can be fitted to data by minimizing the error in \\(y\\).\nFor every observation (\\(y_i\\)) a line produces a prediction \\(\\hat{y}_i\\). The best fitting line is the line that minimizes the sum of squared errors:\n\\[\\sum(y_i - \\hat{y}_i)^2\\]\n\n\n\n\n\n\n\n\n\nBy adding the residual error (\\(y_i - \\hat{y}_i\\)) to the equation mentioned above we can formalize the model using the following notation:\n\\[y_i = \\beta_0 + \\beta_1x_{i} + \\epsilon_i\\]\nEach observation \\(y_i\\) can be described with coefficients describing the straight line \\(\\beta_0 + \\beta_1x_i\\) and some error \\(\\epsilon_i\\).\nWhich can be translated to (Spiegelhalter 2019):\n\\[\\text{observation = deterministic model + residual error}\\]\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\nDefine these concepts:\n\nModel\nCoefficients\nIntercept\nSlope\nResiduals"
  },
  {
    "objectID": "ws8-linear-model.html#fitting-a-regression-model-in-r",
    "href": "ws8-linear-model.html#fitting-a-regression-model-in-r",
    "title": "The Linear Model",
    "section": "Fitting a regression model in R",
    "text": "Fitting a regression model in R\nWork in pairs and use the code below to fit and analyze a regression model\nWe will use the cyclingstudy data set to fit regression models. In R, a linear model can be fitted with the lm function. This function needs a formula and a data set (data frame).\nA formula is written as y ~ x, this formula can be read as “y explained by x”.\nLet’s use the pre time-point data to predict/explain VO2.max with height.T1.\n\n\nCode\nlibrary(exscidata)\n\ndata(\"cyclingstudy\")\n\n# Reduce the data set \ndat &lt;- cyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        select(subject, height.T1, VO2.max) \n\nmod &lt;- lm(VO2.max ~ height.T1, data = dat)\n\n\nThe resulting model object is a list that contains a lot of information. We will put some of these components in a figure by first creating a data frame. Copy the code and run it in your own environment!\nFirst, mod$model is a data.frame of the data used to fit the model. mod$fitted.values contains the predicted values from the regression model and mod$residuals contains each residual (\\(y_i - \\hat{y}_i\\)). We can store these together in a new data frame:\n\n\nCode\nmod_dat &lt;- data.frame(mod$model, \n           fitted = mod$fitted.values, \n           resid = mod$residuals)\n\n\nWe will start by adding the fitted values as a function of the predictor values (height.T1). Let’s make the points a bit larger and filled circles.\n\n\nCode\nmod_dat %&gt;%\n        ggplot(aes(height.T1, fitted)) + geom_point(size = 3, shape = 21, fill = \"steelblue\")\n\n\nNext we will add the residuals as segments starting from the fitted values. geom_segment takes the aesthetics (aes) x, xend, y and yend. x and xend will be the predictor values (height.T1), y will be the fitted values and yend will be the fitted values + residuals.\n\n\nCode\nmod_dat %&gt;%\n        ggplot(aes(height.T1, fitted)) + geom_point(size = 4, shape = 21, fill = \"steelblue\") + \n        # Add residuals\n        geom_segment(aes(x = height.T1, xend = height.T1, y = fitted, yend = fitted + resid))\n\n\nNotice that there are some overlap between individuals in height.\nNext, let’s add the observed values in a new geom_point. We make the points a bit bigger and make the filled circles.\n\n\nCode\nmod_dat %&gt;%\n        ggplot(aes(height.T1, fitted)) + geom_point(size = 4, shape = 21, fill = \"steelblue\") + \n        # Add residuals\n        geom_segment(aes(x = height.T1, xend = height.T1, y = fitted, yend = fitted + resid)) +\n        # Add observed values\n        geom_point(aes(height.T1, VO2.max), shape = 21, fill = \"hotpink\", size = 4)\n\n\nAt last, let’s add the model prediction as an annotation. Using annotate we can specify a geom and if we chose \"segment\" it let’s us specify a start and an end on the x and y axis. We can use the function coef() to get the coefficients from the model where coef(mod)[1] is the intercept and coef(mod)[2] is the slope.\n\n\nCode\nmod_dat %&gt;%\n        ggplot(aes(height.T1, fitted)) + geom_point(size = 4, shape = 21, fill = \"steelblue\") + \n        # Add residuals\n        geom_segment(aes(x = height.T1, xend = height.T1, y = fitted, yend = fitted + resid)) +\n        # Add observed values\n        geom_point(aes(height.T1, VO2.max), shape = 21, fill = \"hotpink\", size = 4) +\n        # Add the model\n        annotate(geom = \"segment\", \n                 x = min(mod_dat$height.T1), \n                 xend = max(mod_dat$height.T1), \n                 y = coef(mod)[1] + coef(mod)[2] * min(mod_dat$height.T1), \n                 yend = coef(mod)[1] + coef(mod)[2] * max(mod_dat$height.T1), \n                 \n                 color = \"mediumorchid1\", size = 1)\n\n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\nUse the figure and explain to a friend:\n\nWhat do the object mod$fitted.values contain?\nWhat information can we get from mod$residuals?\nExplain how the line draw between these points gives a graphical representation of the model?:\n\n\n\nCode\nx = min(mod_dat$height.T1) \n\nxend = max(mod_dat$height.T1) \n\ny = coef(mod)[1] + coef(mod)[2] * min(mod_dat$height.T1) \n\nyend = coef(mod)[1] + coef(mod)[2] * max(mod_dat$height.T1)"
  },
  {
    "objectID": "ws8-linear-model.html#predicting-from-a-regression-model",
    "href": "ws8-linear-model.html#predicting-from-a-regression-model",
    "title": "The Linear Model",
    "section": "Predicting from a regression model",
    "text": "Predicting from a regression model\nSimple predictions can be made from our model using the model coefficients. When the intercept and slope is known, we can simply plug in \\(x\\) values to get predictions.\n\\[\\hat{y} = \\beta_0 + \\beta_1x\\] \\[\\hat{y} = 10 + 2.2x\\] if \\(x=2\\), then\n\\[\\hat{y} = 10 + 2.2 \\times 2\\] \\[ = 14.4\\]\n\n\n\n\n\n\nGroup work\n\n\n\nUse R and solve the following problems\n\nCalculate the expected difference in VO2max between a person that has a stature of 175 and 185 cm\nWhat would be the expected VO2max of a person of height 201 cm?"
  },
  {
    "objectID": "ws8-linear-model.html#assumptions",
    "href": "ws8-linear-model.html#assumptions",
    "title": "The Linear Model",
    "section": "Assumptions",
    "text": "Assumptions\nA regression model provides a valid representation of underlying data when assumptions are met.\nFor an ordinary regression model we have the following main assumptions:\n\nIndependent observations. This is an assumption about the design of the study and the data at hand. If we have observations that are related, the ordinary linear model will give us biased conclusions. As an example, if we collect data from the same participants over time we will not have independent observations and this will lead to pseudo-replication, lower standard errors and biased inference. Another way to see it is that non-independent observations will give non-independence of the residuals which is the mechanism that creates bad inference (as the residuals are used to estimate the sampling distribution of parameters).\nLinear relationship. In the basic case, we expect a linear trend that can be described with a straight line. If the relationship is curve-linear, we can adjust the model.\nNormal residuals. This condition might be violated when there are outliers.\nConstant variance. This assumption says that we want to be equally wrong all along our data. If we predict \\(y\\) with greater error at large \\(x\\) we have heteroscedasticity (unequal variance), if we are “equally wrong” we have homoscedasticity (equal variance).\n\n\nIndependent observations and linear relationship\nWe will use height and VO2max from cyclingstudy to explore assumptions. We need to:\n\nFilter only to use one time-point, e.g. pre values (why?)\nSelect relevant columns (optional)\nMake a plot to explore if there is a straight-line relationship.\n\n\nexscidata::cyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        select(subject, height.T1, VO2.max) %&gt;%\n        ggplot(aes(height.T1, VO2.max)) + geom_point() + \n        geom_smooth(se = FALSE) + \n        geom_smooth(se = FALSE, method = \"lm\", color = \"red\") +\n        geom_smooth(method = \"lm\", formula = y ~ poly(x, 2), \n                    color = \"green\", se = FALSE)\n\nWe have the possibility to plot the results of preliminary models in ggplot2.\nBy adding the following we will get an idea about the relationship. Notice that se = FALSE turns of confidence bands, method = \"lm\" tells geom_smooth to use an ordinary linear model. The default model is a “local polynomial regression model”.\n\n\nNormal residual and constant variance.\nThese are assumptions about the model, and the resulting residuals. Normal residuals can be assessed with a qq-plot which shows every observed residual against its theoretical position in a normal distribution.\nA qq-plot can be created using ggplot:\n\n\nCode\nm1 &lt;- lm(VO2.max ~ height.T1, data = exscidata::cyclingstudy)\n\n\n\n\nexscidata::cyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        select(subject, group, VO2.max, height.T1)  %&gt;%\n        mutate(resid = resid(m1), \n               st.resid = resid/sd(resid)) %&gt;%\n        ggplot(aes(sample = st.resid)) +\n         stat_qq(size = 3, fill = \"lightblue\", shape = 21) + \n                 stat_qq_line() +\n        labs(title = \"A qq-plot of residuals\") +\n                 theme_minimal()\n\n\nA residual plot can be created to assess the assumtion regarding equal variance. We can plot residuals against fitted values or the predictor.\n\n\nCode\nexscidata::cyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        select(subject, group, VO2.max, height.T1)  %&gt;%\n        mutate(resid = resid(m1),\n               fitted = fitted(m1),\n               st.resid = resid/sd(resid)) %&gt;%\n        ggplot(aes(fitted, st.resid))  +\n        geom_hline(yintercept = 0) + \n        geom_point()  \n\n\n\n\n\n\n\n\nPractice: Body dimensions and ventilation\n\n\n\nWe think that body dimensions influence physiological characteristics. To test if if the stature (height.T1) influence maximum ventilatory capacity (VE.max) fit a regression model, check model assumptions and interpret the results."
  },
  {
    "objectID": "ws8-linear-model.html#footnotes",
    "href": "ws8-linear-model.html#footnotes",
    "title": "The Linear Model",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee (Spiegelhalter 2019) for inspiration on understanding models↩︎"
  }
]