[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "assignment-1.html",
    "href": "assignment-1.html",
    "title": "Quantitative Methods - Workshop, Assigments and Tutorials",
    "section": "",
    "text": "References\n\nHalperin, I., D. B. Pyne, and D. T. Martin. 2015. “Threats to Internal Validity in Exercise Science: A Review of Overlooked Confounding Variables.” Journal Article. Int J Sports Physiol Perform 10 (7): 823–29. https://doi.org/10.1123/ijspp.2014-0566.\n\n\nHopkins, W. G. 2000. “Measures of Reliability in Sports Medicine and Science.” Journal Article. Sports Med 30 (1): 1–15. http://www.ncbi.nlm.nih.gov/pubmed/10907753.\n\n\nTanner, R. K., and C. J. Gore. 2012. Physiological Tests for Elite Athletes 2nd Edition. Book. Human Kinetics. https://books.google.no/books?id=0OPIiMks58MC."
  },
  {
    "objectID": "assignment-2.html",
    "href": "assignment-2.html",
    "title": "Quantitative Methods - Workshop, Assigments and Tutorials",
    "section": "",
    "text": "References\n\nCampbell, Michael J., Stephen John Walters, and David Machin. 2020. Medical Statistics: A Textbook for the Health Sciences. Fifth edition. Hoboken, NJ: Wiley-Blackwell.\n\n\nFrigessi, Arnoldo, and Odd O Aalen. 2018. Statistiske Metoder i Medisin Og Helsefag. Oslo: Gyldendal akademisk.\n\n\nNewell, J., D. Higgins, N. Madden, J. Cruickshank, J. Einbeck, K. McMillan, and R. McDonald. 2007. “Software for Calculating Blood Lactate Endurance Markers.” Journal Article. Journal of Sports Sciences 25 (12): 1403–9. https://doi.org/10.1080/02640410601128922.\n\n\nSpiegelhalter, D. J. 2019. The Art of Statistics : How to Learn from Data. Book. First US edition. New York: Basic Books.\n\n\nTanner, R. K., and C. J. Gore. 2012. Physiological Tests for Elite Athletes 2nd Edition. Book. Human Kinetics. https://books.google.no/books?id=0OPIiMks58MC."
  },
  {
    "objectID": "assignment-3.html",
    "href": "assignment-3.html",
    "title": "Quantitative Methods - Workshop, Assigments and Tutorials",
    "section": "",
    "text": "This assignment is set up as a statistical laboratory, we will perform simulations and your assignment is to interpret and explain the results. Create a report based on the code used in the lab and make sure you answer the specified questions (1-8). You can be as creative as you want and explore the results further.\nThe report should be handed in on canvas as a link to github repository containing a reproducible .Rmd (or qmd) file.\n\n\nIn this assignment we will simulate a population of possible values, from this population we will draw random samples, calculate statistics and interpret them. The population of values can be regarded as the possible differences between two treatments in a cross-over study where participants have performed both treatments. The values in the population are calculate as \\(Treatment - Control\\).\nWe will simulate a population of one million numbers with a mean of 1.5 and a standard deviation of 3. We will make two different set of studies, one set with a sample size of 8 and one set with a sample size of 40. In order to be sure you replicate your results, include and run set.seed() before simulations in your final script.\nWe will use the lm function to estimate the average value of the population. We do this in an “intercept-only” model. This model can be written as\n\\[Y_i = \\beta_0 + \\epsilon_i\\]\nwhere \\(\\beta_0\\) is the intercept and can be interpreted as the average value of \\(Y\\), our dependent variable. \\(\\epsilon\\) is the error term, each observation (\\(i\\)) deviates from the intercept to some degree. If the intercept term is positive or negative we can interpret it as a difference between the two treatments (described above). This model is equivalent to a one-sample t-test. Let’s get started!\nIn the code chunk below, we will simulate the population of differences between treatments. We will then draw two random samples corresponding sample sizes of 8 and 40 and save these data in data frames with the dependent variable named y. We fit the very simple model y ~ 1 as a linear model and save the model object as m1 and m2.\n\nlibrary(tidyverse)\n\nset.seed(1)\npopulation <- rnorm(1000000, mean = 1.5, sd = 3)\n\n\nsamp1 <- data.frame(y = sample(population, 8, replace = FALSE))\n\nsamp2 <- data.frame(y = sample(population, 40, replace = FALSE))\n\n\nm1 <- lm(y ~ 1, data = samp1)\nm2 <- lm(y ~ 1, data = samp2)\n\nsummary(m1)\n\n\nCall:\nlm(formula = y ~ 1, data = samp1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.5322 -1.2523 -0.0883  1.3540  4.8692 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)    1.840      1.251    1.47    0.185\n\nResidual standard error: 3.539 on 7 degrees of freedom\n\n\nThe results from a simple model can be calculated by hand. The Estimate corresponds to the average of all values in the sample, from the smaller sample, samp1 we can do mean(samp1$y). This average should correspond to coef(m1) which should be 1.84. The variation of the data is most often described with the standard deviation (SD). The SD of y in the smaller sample is sd(samp1$y) (corresponding to 3.539). However, the regression table (summary(m1)) show you the standard error (SE). This statistic is an attempt to estimate the variation in a hypothetical distribution of means. The standard error is (in this simple case) \\(SE_y = \\frac{SD_y}{\\sqrt{n}}\\). Calculating by hand using the data in samp1 we would do sd(samp1$y)/sqrt(8). Amazingly this corresponds to 1.251!\nBy using the estimate 1.84 and the corresponding SE (1.251) we can calculate the t-value as the ratio \\(\\frac{Estimate}{SE}\\). The t-value may in turn be used to determine the are under the curve of a t-distribution. The t-value from the above calculation is 1.4702611. Using our single \\(n=8\\) study, we estimate that values of t, as extreme or even more extreme as our observed value both above and below 0, would occur in 18.5% of studies if the null-hypothesis was true. This corresponds to a p-value of 0.185. The figure below shows a graphical representation of a t-value distribution under the assumption that the null-hypothesis is true.\n\n\n\n\n\nA t-distribution estimated from model m1 with the shaded area corresponding to the observed p-value.\n\n\n\n\n\nIn light of what you know now about the process of conducting a study with a random sample, use your own words and…\n\nExplain the estimate, SE, t-value, and p-value from the regression models that we created previously (m1 and m2).\nDiscuss what contributes to the different results in the two studies (m1 and m2).\nWhy do we use the shaded area in the lower and upper tail of the t-distribution (See Figure @ref(fig:t-dist-fig)).\n\n\n\n\n\nBelow we will perform 1000 studies and save the results from each study. This will make it possible for us to get an actual sampling distribution. Copy the code to your own document to run the experiment.\n\n# Create data frames to store the model estimates\nresults_8 <- data.frame(estimate = rep(NA, 1000), \n                      se = rep(NA, 1000), \n                      pval = rep(NA, 1000), \n                      n = 8)  \n\nresults_40 <- data.frame(estimate = rep(NA, 1000), \n                      se = rep(NA, 1000), \n                      pval = rep(NA, 1000), \n                      n = 40)\n\n# A for loop used to sample 1000 studies, each iteration (i) will draw a new sample\n# from the population. \n\nfor(i in 1:1000) {\n  \n  # Draw a sample \n  samp1 <- data.frame(y = sample(population, 8, replace = FALSE))\n  samp2 <- data.frame(y = sample(population, 40, replace = FALSE))\n\n  # Model the data\n  m1 <- lm(y ~ 1, data = samp1)\n  m2 <- lm(y ~ 1, data = samp2)\n  \n  # Extract values from the models\n  results_8[i, 1] <- coef(summary(m1))[1, 1]\n  results_8[i, 2] <- coef(summary(m1))[1, 2]\n  results_8[i, 3] <- coef(summary(m1))[1, 4]\n\n  results_40[i, 1] <- coef(summary(m2))[1, 1]\n  results_40[i, 2] <- coef(summary(m2))[1, 2]\n  results_40[i, 3] <- coef(summary(m2))[1, 4]\n  \n  \n}\n\n\n# Save the results in a combined data frame\n\nresults <- bind_rows(results_8, results_40)\n\n\nUsing the results data frame…\n\nCalculate the standard deviation of the estimate variable, and the average of the se variable for each of the study sample sizes (8 and 40). Explain why these numbers are very similar. How can you define the Standard Error (SE) in light of these calculations?\nCreate a histogram (see example code below) of the p-values from each study sample-size. How do you interpret these histograms, what do they tell you about the effect of sample size on statistical power?\nCalculate the number of studies from each sample size that declare a statistical significant effect (specify a threshold for \\(\\alpha\\), your significance level).\nUsing the pwr package, calculate the power of a one-sample t-test, with a effect size of 1.5/3, your specified significance level and sample sizes 8 and 40. Explain the results in the light of your simulations.\n\n\n\n# Example code for copy and paste\n\n# A two facets histogram can be created with ggplot2\nresults %>%\n  ggplot(aes(pval)) + \n  geom_histogram() +\n  facet_wrap(~ n)\n\n\n# Count the proportion of tests below a certain p-value for each \nresults %>%\n  filter(pval < 0.05) %>%\n  group_by(n) %>%\n  summarise(sig_results = n()/1000)\n\n# Using the pwr package\nlibrary(pwr)\n\npwr.t.test(n = 40, sig.level = 0.05, d = 1.5/3, type = \"one.sample\")\n\n\n\n\nWe will now simulate a population without differences between treatment and control. The code below is very similar to the one we use above, except that we use an average effect of 0 in the population.\n\npopulation <- rnorm(1000000, mean = 0, sd = 3)\n\n\n# Create data frames to store the model estimates\nresults_8 <- data.frame(estimate = rep(NA, 1000), \n                      se = rep(NA, 1000), \n                      pval = rep(NA, 1000), \n                      n = 8)  \n\nresults_40 <- data.frame(estimate = rep(NA, 1000), \n                      se = rep(NA, 1000), \n                      pval = rep(NA, 1000), \n                      n = 40)\n\n# A for loop used to sample 1000 studies, each iteration (i) will draw a new sample\n# from the population. \n\nfor(i in 1:1000) {\n  \n  # Draw a sample \n  samp1 <- data.frame(y = sample(population, 8, replace = FALSE))\n  samp2 <- data.frame(y = sample(population, 40, replace = FALSE))\n\n  # Model the data\n  m1 <- lm(y ~ 1, data = samp1)\n  m2 <- lm(y ~ 1, data = samp2)\n  \n  # Extract values from the models\n  results_8[i, 1] <- coef(summary(m1))[1, 1]\n  results_8[i, 2] <- coef(summary(m1))[1, 2]\n  results_8[i, 3] <- coef(summary(m1))[1, 4]\n\n  results_40[i, 1] <- coef(summary(m2))[1, 1]\n  results_40[i, 2] <- coef(summary(m2))[1, 2]\n  results_40[i, 3] <- coef(summary(m2))[1, 4]\n  \n  \n}\n\n\n# Save the results in a combined data frame\n\nresults_null <- bind_rows(results_8, results_40)\n\n\nUsing the new data frame with results from studies of a population with an average effect of zero, create new histograms.\n\nWith a significance level of 5%, how many studies would give you a “false positive” result if you did many repeated studies?"
  },
  {
    "objectID": "assignment-4.html",
    "href": "assignment-4.html",
    "title": "Quantitative Methods - Workshop, Assigments and Tutorials",
    "section": "",
    "text": "Footnotes\n\n\nAvoid using review articles or meta-analyses↩︎\nSee Teaching undergraduate students to read empirical articles: An evaluation and revision of the QALMRI method, this advice was also heavily influenced by this website↩︎\nHulley, S. B. (2013). Designing clinical research. Philadelphia, Wolters Kluwer/Lippincott Williams & Wilkins.↩︎"
  },
  {
    "objectID": "assignment-5.html",
    "href": "assignment-5.html",
    "title": "Quantitative Methods - Workshop, Assigments and Tutorials",
    "section": "",
    "text": "References\n\nHammarström, Daniel, Sjur Øfsteng, Lise Koll, Marita Hanestadhaugen, Ivana Hollan, William Apró, Jon Elling Whist, Eva Blomstrand, Bent R. Rønnestad, and Stian Ellefsen. 2020. “Benefits of Higher Resistance-Training Volume Are Related to Ribosome Biogenesis.” Journal Article. The Journal of Physiology 598 (3): 543–65. https://doi.org/10.1113/JP278455."
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "Assignment\nDue date\nIncluded in portfolio\nGroup assignment\n\n\n\n\nDescriptive statistics, reliability and validity, tools for reproducible data science\n2022-09-26\nYes\nYes\n\n\nRegression models and prediction from data\n2022-11-04\nNo\nYes\n\n\nExtraction and analysis of DNA\n2022-11-23\nOptionala\nYes\n\n\nExtraction of RNA and analysis of qPCR experiments\n2022-11-23\nOptionala\nYes\n\n\nExtraction and analysis of Protein\n2022-11-23\nOptionala\nYes\n\n\nPhilosophy of scienceb (See canvas)\n2022-10-28\nYes\nNo\n\n\nDrawing inference from statistical models and statistical power\n2022-11-11\nNo\nYes\n\n\nStudy designs\n2022-11-25\nYes\nNo\n\n\nAnalyzing repeated measures experiments\n2022-11-25\nYes\nNo\n\n\n\na Select one laboratory assignments for your portfolio exam. All groups presents one selected method on 2022-11-23. b This assignment is presented in connection with lectures."
  },
  {
    "objectID": "feedback-assignments.html",
    "href": "feedback-assignments.html",
    "title": "Sjekkliste for arbeidskrav og eksamen",
    "section": "",
    "text": "Følgende punkter er basert på de arbeidskrav som til nå er levert i emnet. Jeg oppdaterer teksten etter neste innlevering."
  },
  {
    "objectID": "feedback-assignments.html#å-skrive-rapporter",
    "href": "feedback-assignments.html#å-skrive-rapporter",
    "title": "Sjekkliste for arbeidskrav og eksamen",
    "section": "Å skrive rapporter",
    "text": "Å skrive rapporter\n\nUnngå å forfatte rapporter i punkter/lister. Denne formen av tekst kan hjelpe leseren men kan også bryte opp og skape unødvendig forvirring. En punktliste kan også få spørsmålet «hvorfor» å forsvinne. Leseren kan være interessert i å vite hvorfor man velger en tilnærming eller metode.\nSkriv for en lesere som er kjent med området men savner informasjon om hvorfor man skal lese rapporten. Du må fylle i «blanks» hos leseren!\nRapporter kan med fordel struktureres som introduksjon, metode, resultat og diskusjon. Man kan velge å ikke ha med disse overskriftene men til tross organisere paragrafene på denne måten. Introduksjonen forteller leseren hva teksten skal belyse, metoden gir et innblikk i hva og hvordan du gjennomført arbeidet, resultatene beskriver og diskusjon tolker.\nVær sikker på at begreper og metoder er definerte! «Typical error», «Coefficient of variation», «laktatterskel» osv. kan være mange ulike ting. Man kan bruke en referanse eller en formel for å definere beregninger, referanser for konsepter osv.\nHusk å definer feilstapler (error bars), farger, punkter enheter osv. I tabeller, figurer og tekst."
  },
  {
    "objectID": "feedback-assignments.html#spesifikt-innhold",
    "href": "feedback-assignments.html#spesifikt-innhold",
    "title": "Sjekkliste for arbeidskrav og eksamen",
    "section": "Spesifikt innhold",
    "text": "Spesifikt innhold\n\nArbeidskrav 1\n\nSe til å organiser github-mapper på en overskuelig måte. En metode for å underlette for leseren er å bruke en README.md. Denne vil bli konvertert til HTML på github og kan dermed gi et overblikk over mappen. Hvis du trenger å spare gamle filer så kan disse flyttes til en mappe («archive» eller lignende).\nDet er viktig å vise at du bruker kilder som er angitt i oppgaven. Bruk disse for å vise til definisjoner og gi bakgrunn til hvorfor du ønsker å vise dine resultater til leseren.\nHva betyr CV, typical error osv. som du beregnet? Her holder det ikke bare med definisjoner, prøv deg også på en tolkning. En måte kan være å bruke annen litteratur som beregnet CV, eller sammenligne med et annet test som du gjort.\nNoen detaljer i en metode kan med fordel holdes kort og kanskje til og med utelates da det tilhører eks. «good laboratory practice». Her kan man spørre seg hvilken informasjon som kreves for å repetere forsøkene gitt at man praktiserer gode rutiner i laboratoriet?\n\n\n\nArbeidskrav 2\n\nAlle gruppene/rapportene er meget sparsomt skrevne. Bruk rapportens deler til å skrive noe om de grafer og analysene som dere presenter. Hvordan kan man tolke laktatterskel? hvorfor predikerer man størrelse på DNA fragmenter? Etc!\nStandard error er spredning i en estimert fordeling av «teoretiske» utvalg. Den forteller ikke hvor bra modellen er, men snarere med hvilken usikkerhet vi bør tolke estimatet. Hvordan skiller denne tolkningen fra deres tolkning?\nT-verdien i en regresjonstabell er en ratio mellom estimat og SE, denne brukes for å beregne p-verdien. P-verdien i sin tur forteller oss om hvor usannsynlig vår data, eller enda mer ekstrem data er hvis nullhypotesen er sann. Hvordan kan dere justere beskrivelsen av regresjonsoutput basert på denne beskrivelsen?\n\n\n\nArbeidskrav 3\nSpørsmål 1-3:\n\nForskjell i SE mellom modellene henger sammen med utvalgsstørrelsen. P-verdien forteller ikke sannsynligheten for at null-hypotesen er sann. Hvordan henger utvalgsstørrelse sammen p-verdien?\nDen skyggete delen av t-fordeling er resultater like eller mer ekstreme enn et observert resultat. T-fordelingen representerer vårt beste estimat av mulige resultater under null-hypotesen.\n\nSpørsmål 4:\n\nHer er del lurt å igjen definere SE! Den gjennomsnittlige standardfeilen er et estimat på spredningen i utvalgsfordelingen! Utvalgsfordelingen er alle beregnede gjennomsnittene!\n\nSpørsmål 5-7:\n\nHvordan kan vi bruke dette for å definere statistisk styrke? Statistisk styrke kan defineres ved hjelp av tankeeksperimentet at man gjennomfører 1000 studier på samme populasjon. Hvor mange vill finne en definert effekt når man setter grensen for p-verdien og utvalgsstørrelsen til et spesifikt nivå?\n\nSpørsmål 8:\n\nNår vi ikke har noen effekt så vil vi finne en effekt i et antall studier basert på den grense vi setter på \\(\\alpha\\). Dette betyr at \\(\\alpha\\) definerer våre antall falske positive fynd og at fordeling av p-verdier ved ingen effekt er uniform, hva betyr dette?\n\n\n\nArbeidskrav 4:\n\nHusk primær målsetting med denne oppgaven: «fokus på design av studiene og valg av statistiske metoder/test for å besvare studienes problemstilling». For å løfte flere av deres tekster kan man tenke at man fokuserer på å sammenligne studiene på disse punktene.\nJeg savner noe informasjon i mange arbeidskrav om studiedesigner i stort, hvordan forholder seg de studier dere har analysert til andre studiedesigner. Her kan tenkes at man gir en generell innføring i studiedesigner i en paragraf for å vise for leseren hvordan man kan forstå de studiene som du har analysert. Det er da også mulig å bruke forslag på pensum!:)\nBruk gjerne også noe plass på å beskrive de statistiske metodene som blir brukt, dette kan med fordel gjøres med henvisning til pensum (eller annen passende litteratur). Har du kjennskap til testene som blir brukt på den nivå at du vet hvilke kommandoen som skal brukes i R for å gjenskape analysene?\nTil tross for at jeg foreslå at man kan bruke QALMRI så sa jeg også at rapporten ikke skulle inneholde QALMRI-tabellen. (Jeg vil ikke underkjenne oppgaver som har QALMRI som bærende struktur).\nTenk på om din tekst kan bli mer strukturert ved å bruke tydeligere temaer per paragraf. Når en paragraf er veldig lang, og inneholder flere ulike temaer blir det vanskelig for leseren å følge. En lettlest paragraf har kanskje 100-200 ord!\nI oppgaven er det lov på kommer med noen meninger om studiedesignene, eller analysene er «gode», hva bør gjøres annerledes for å lage bedre vitenskap. Et eksempel som flere er inne på er statistiske tester innad eksperimentelle grupper, hva sier disse oss?\nEn tabell eller figur er et veldig godt innslag i en rapport. Men den må følges opp i løpende tekst.\n\n\n\nReproduserbare rapporter på Github\n\nMed en reproduserbar rapport mener jeg at tekst, kode og data blir brukt til å skape et «output» som en pdf-, html- eller word-fil. Fordelen med github (og lignende løsninger) er at man har mulighet å samle alle delene i et versjonskontrollsystem. Dette har fordeler for deg som forfatter og for vitenskapelig arbeid i stort (transparens, muligheter å reprodusere osv.). Hensikten med å bruke Github i emnet er å øve på denne måten å lage rapporter.\n\nMed det sagt vil jeg ikke underkjenne rapporter som ikke er reproduserbar i denne betydningen. - Vær konsekvent med filstruktur, filnavn osv på github. Filer som ikke blir brukt bør f.eks. flyttes til en annen mappe.\n\nKontroller at din mappe går å laste ned fra github, at den seneste versjonen av prosjektet ligger oppe osv!"
  },
  {
    "objectID": "feedback-assignments.html#å-bruke-r",
    "href": "feedback-assignments.html#å-bruke-r",
    "title": "Sjekkliste for arbeidskrav og eksamen",
    "section": "Å bruke R",
    "text": "Å bruke R\n\nR bruker punktum som kommaseparerare. For en helhetlig rapport kan det være lurt å velge en måte å rapportere resultater på.\nHensikten/styrken med reproduserbar rapportering er at tall, tabeller, figurer osv. er direkte koblet til dataene. Prøv å unngå å skrive inn tall direkte. Bruk isteden variabler (eks. var1) som du skaper i code chunks och settes inn i teksten. R vill i dette fallet lete etter en variable i environment som heter var1. Se her https://rmarkdown.rstudio.com/lesson-4.html for mer info.\nÅ definere innstillinger for code chunks vil gjøre rapportene lettere å lese. F.eks. ved å inkludere koden som beskrives under vil ta vekk meddelende, «warnings» og koden fra din rapport. Hus at innstillinger må settes først i code chunken.\n\n#| echo: false\n#| message: false\n#| warning: false\n\n\n\nUnngå å bruke print() i en rapport da dette vil resultere i uformatert tekst. Bruk istedenfor tabellverktøy (som gt eller knitr::kable()) for tabeller.\nSe her Quarto - Citations & Footnotes for hvordan du kan sette en egen overskrift for referanser og inkludere referanser hvor du ønsker."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quantitative methods and statistics (In Sport and Exercise Science)",
    "section": "",
    "text": "Welcome to the course Quantitative methods and Statistics (IDR4000). The course aims to give students an overview of methodological aspects within the field of sport and exercise-physiology. Specifically, planning, conducting and analysing research projects with human participants will be covered. The lecture notes for the course can be found here. This website will contain tutorials that we will work on in class, assignments and additional materials related to the course content."
  },
  {
    "objectID": "index.html#practical-information",
    "href": "index.html#practical-information",
    "title": "Quantitative methods and statistics (In Sport and Exercise Science)",
    "section": "Practical information",
    "text": "Practical information\nThese notes were updated on 2022-11-18 and cover the course held during 2022 autumn semester. Contact Daniel Hammarström if you have any questions regarding this content.\n\nLearning objectives\nLearning objectives can be read in Norwegian here.\n\n\nLearning strategies\nThe course will include lectures, laboratory exercises, computer exercises and workshops, seminars and student presentations. Lectures will be held in-person and as pre-recorded published on these websites.\nComputer exercises will eventually require that you have special computer software installed on your computer. The software is free (see specific chapters in lecture notes).\nAssignments will be presented on this website with information on how to hand them in. The whole course is evaluated based on a portfolio exam (see below).\n\n\nCourse evaluation\nAs a student you can contribute to the quality of the course by engaging in course evaluation throughout the course. You will be asked to answer a pre-course questionnaire about your expectations and a post-course questionnaire about your experiences. You are also welcomed to take part in systematic discussions during the course about the quality of teaching and course material. With these notes I want to underline the importance of student participation in the continuous development of the course (and program) teaching/learning quality.\n\n\nLecturers and course administration\nIn order of appearance\n\nDaniel Hammarström (daniel.hammarstrom@inn.no), is responsible for course administration and will be teaching statistics.\nTomas Urianstad will organize laboratory work in the physiology lab.\nKristian Lian will be involved in molecular methods.\nProf. Carsten Lundby will cover aspects CO2 re-breathing techniques (physiology).\nProf. Finnur Dellsén will cover philosophy of science.\nProf. Stian Ellefsen will teach molecular methods.\n\n\n\nUpdates, notifications and general communication\nThese course notes will be updated during the course. General information and last minute changes will be posted on Canvas, make sure to check it as part of your daily study routine.\n\n\nLiterature\nA full list of recommended literature can be found here. Literature will be referenced in specific sections in the lecture notes.\n\n\nGrades\nThe course is graded pass/fail.\n\n\nLanguage\nMy (Daniel) first language is Swedish, I’m sure most of you will understand what I’m talking about. However, due to the fact that we accept international students to the program, most written communication and some lectures will be in English. You are not expected to write in English, it is however possible!"
  },
  {
    "objectID": "index.html#assignments-and-portfolio-exam",
    "href": "index.html#assignments-and-portfolio-exam",
    "title": "Quantitative methods and statistics (In Sport and Exercise Science)",
    "section": "Assignments and Portfolio exam",
    "text": "Assignments and Portfolio exam\nThe course is based on several assignments. Some of these assignments are to be handed in as part of a portfolio exam upon which your grade is based.\nAssignments that are due during the course (arbeidskrav) are expected to be further improved after feedback from fellow students and teachers before inclusion in your portfolio.\nThe table below shows all assignments that are part of the course. Some are not to be included in the portfolio and some assignments are group assignments (see Table). In addition to these assignments, non-mandatory homework will be presented during the course.\n\n\n\nAssignment\nDue date\nIncluded in portfolio\nGroup assignment\n\n\n\n\nDescriptive statistics, reliability and validity, tools for reproducible data science\n2022-09-26\nYes\nYes\n\n\nRegression models and prediction from data\n2022-10-03\nNo\nYes\n\n\nExtraction and analysis of DNA\n2022-11-25\nOptionala\nYes\n\n\nExtraction of RNA and analysis of qPCR experiments\n2022-11-25\nOptionala\nYes\n\n\nExtraction and analysis of Protein\n2022-11-25\nOptionala\nYes\n\n\nPhilosophy of scienceb (See canvas)\n2022-10-28\nYes\nNo\n\n\nDrawing inference from statistical models and statistical power\n2022-11-07\nNo\nYes\n\n\nStudy designs\n2022-11-17\nYes\nNo\n\n\nAnalyzing repeated measures experiments\n2022-11-21\nYes\nNo\n\n\n\na Select one laboratory assignments for your portfolio exam. All groups presents one selected method on 2021-11-23. b This assignment is presented in connection with lectures. \nIn addition to arbeidskrav/assignments, you are required to contribute to the course wiki. The wiki page is hosted at github.com/dhammarstrom/IDR4000-2022/. In order to contribute you need to set up your own github account. The language of the wiki should be Norwegian.\nSmaller assignments and quizzes are presented as part of the course, but you are not required to do them to pass the course."
  },
  {
    "objectID": "index.html#other-information",
    "href": "index.html#other-information",
    "title": "Quantitative methods and statistics (In Sport and Exercise Science)",
    "section": "Other information",
    "text": "Other information"
  },
  {
    "objectID": "notes1-linear-models.html",
    "href": "notes1-linear-models.html",
    "title": "Notes: Univariate regression models by hand",
    "section": "",
    "text": "The univariate regression model has one dependent and one independent variable. The goal is to determine the relationship between the two variables.\nHere we will explore the mathematics of this model and replicate results from R “by hand”.\nFirst, let’s simulate some data."
  },
  {
    "objectID": "notes1-linear-models.html#a-categorical-predictor-variable",
    "href": "notes1-linear-models.html#a-categorical-predictor-variable",
    "title": "Notes: Univariate regression models by hand",
    "section": "A categorical predictor variable",
    "text": "A categorical predictor variable\n\nxcat <- ifelse(x < 7, 0, 1)\n\n\n## Calculate the correlation \nzx <- (xcat - mean(xcat)) / sd(xcat)\nzy <- (y - mean(y)) / sd(y)\n\n\nrxy <- sum(zx * zy) / (length(xcat) - 1)\n\n\n## Calculate the slope \n\nb1 <- rxy * (sd(y)/sd(xcat))\n\nb0 <- mean(y) - b1 * mean(xcat)\n\n\nb1\n\n[1] 3.891868\n\nb0\n\n[1] 22.09147\n\n## Calculate the errors\n\ne <- y - (b0 + b1*xcat)\n\n## \n\nvarx <- (sum((xcat-mean(xcat))^2))/length(xcat)\n\nseb0 <- se/sqrt(length(xcat)) * sqrt(1 + ((mean(xcat)^2) / varx))\n\nsx <- sqrt(sum((xcat - mean(xcat))^2) / length(xcat))\n\nseb1 <- se/sqrt(length(y)) * 1 /  sx\n\n\n## Calculate"
  },
  {
    "objectID": "workshops.html",
    "href": "workshops.html",
    "title": "Workshops",
    "section": "",
    "text": "Workshop\nAdditional material\n\n\n\n\nIntroduction to data science (Norwegian)\n\n\n\nInstalling and starting R\n\n\n\nCreating your first graph\n\n\n\nData wrangling and tables\n\n\n\nData wrangling and tables, part 2\n\n\n\nWriting reports\n\n\n\nCollaborative coding\n\n\n\nReliability and writing up a report (in Norwegian)\n\n\n\nDiscussions on reliability\n\n\n\nThe linear model (1): Straight lines and predicting from models\n\n\n\nThe linear model (2): Categorical predictors and diagnostics\n\n\n\nThe linear model (3): Curve-linear regression\n\n\n\nCreating functions in R\n\n\n\nUnderstanding p-values through simulations\n\n\n\nStudy designs and statistical tests\n\n\n\nAnalyzing trials\n\n\n\nMixed effects models for repeated measures"
  },
  {
    "objectID": "ws1-data-science-intro.html",
    "href": "ws1-data-science-intro.html",
    "title": "Quantitative Methods - Workshop, Assigments and Tutorials",
    "section": "",
    "text": "References\n\nBroman, Karl W., and Kara H. Woo. 2018. “Data Organization in Spreadsheets.” Journal Article. The American Statistician 72 (1): 2–10. https://doi.org/10.1080/00031305.2017.1375989."
  },
  {
    "objectID": "ws12-writing-functions.html",
    "href": "ws12-writing-functions.html",
    "title": "Writing R functions and calculate a lactate threshold",
    "section": "",
    "text": "For a comprehensive overview of writing functions in R, Chapter 19 in (Wickham and Grolemund 2017) contains an excellent introduction."
  },
  {
    "objectID": "ws12-writing-functions.html#why-write-functions",
    "href": "ws12-writing-functions.html#why-write-functions",
    "title": "Writing R functions and calculate a lactate threshold",
    "section": "Why write functions",
    "text": "Why write functions\n\nYou should invest in writing functions, if you plan to re-use code by copy paste more than once (Wickham and Grolemund 2017).\nFunctions can make your code more readable, reusable and make sure you minimize errors.\nThe DRY principle applies → Do not repeat yourself (Wickham and Grolemund 2017)."
  },
  {
    "objectID": "ws12-writing-functions.html#what-is-a-function",
    "href": "ws12-writing-functions.html#what-is-a-function",
    "title": "Writing R functions and calculate a lactate threshold",
    "section": "What is a function?",
    "text": "What is a function?\n\nR is easily “extended” through the creation of function.\nFunctions can live inside package that you or someone else has written.\nFunctions can also live in your environment after being defined in your script.\nmean(), sd() mutate()and ggplot() are examples of functions that comes with the basic installation of R or packages.\nFor a function to live in your environment, it must have a name\nThe function can come with a set of arguments\nThe actual mechanics of the function is defined in the body\n\n\n# An example function\nmy_fun  <- function(arg1, arg2) { # Name and arguments of the function\n        \n        # This is the body\n        sum <- arg1 + arg2\n        \n        return(sum)\n        \n}\n\nmy_fun(2, 4)\n\n\nNaming the function\n\nThe same rules apply to naming functions as other R objects.\n(Wickham and Grolemund 2017) recommend longer and informative/descriptive names written in “snake_case” (as opposed to “camelCase”).\nBe consistent!\nDo not use names of other function!\n\n\n\nArguments\n\nA function may be defined with named arguments, this is the “input” of the function.\nAn argument may be a data frame, a single value or vector that the function will use.\nThe function can be used with the arguments used in their place or named out of place.\n\n\n# An example function\nmy_fun  <- function(value1, value2) { # Name and arguments of the function\n        \n        # This is the body\n        diff <- value1 - value2\n        \n        return(diff)\n        \n}\n\n# these are different\nmy_fun(value2 = 1, value1 = 4)\n\nmy_fun(1, 4)\n\n\n\nBody\n\nThe function body defines the output of the function\nIt usually makes use of the data/values/variables defined in the arguments and returns a given output\nOutput can be any R object.\nThe return() function is used to explicitly make some part of the body to the output of the function\n\n\n# An example function\nmy_fun  <- function(value1, value2) { # Name and arguments of the function\n        \n        # This is the body\n        diff <- value1 - value2\n        \n        sum <- value1 + value2\n        \n        results <- list()\n        \n        results$diff <- diff\n        results$sum <- sum\n        \n        return(results)\n        \n}\n\n# these are different\nmy_fun(value2 = 1, value1 = 4)\n\nmy_fun(1, 4)\n\n\nEnvironment\n\nIf a variable is not defined in the function, R will try to look for it in the environment.\nIt is considered good practice to not rely on variables not defined in the function\n\n\n# An example function\n\n# A variable defined outside the function\nvalue1 <- 3\n\n\nmy_fun  <- function(value2) { # Name and arguments of the function\n        \n        # This is the body\n        diff <- value1 - value2\n        \n        sum <- value1 + value2\n        \n        results <- list()\n        \n        results$diff <- diff\n        results$sum <- sum\n        \n        return(results)\n        \n}\n\nmy_fun(value2 = 1)\n\nmy_fun(5)\n\n\n\n\nConditions, errors and messages\n\nFunctions can include conditional sections, using if makes it possible to make the function flexible depending on input variables\n\n\nmy_fun <- function(value1, value2, calculate.sum = FALSE) {\n        \n        if (calculate.sum == TRUE) {\n                \n                sum <- value1 + value2\n                \n                return(sum)\n                \n        } else {\n                \n                print(\"No sum calculated\")\n                \n        }\n        \n}\n\n# Calculates the sum \nmy_fun(2, 4, TRUE)\n\n# Does not calculate the sum\nmy_fun(2, 4, FALSE)\n\n\nA function can stop if the input variable is of the wrong type,\n\n\nmy_fun <- function(value1, value2) {\n        \n        if (!any(is.numeric(c(value1, value2)))) {\n                \n                stop(\"One or more values are not numeric\")\n                \n        } else {\n                \n                sum <- value1 + value2\n                \n                return(sum)\n                \n        }\n        \n}\n\n# Calculates the sum \nmy_fun(value1 = \"error?\", value2 = 4)\n\n# Does not calculate the sum\nmy_fun(value1 = 2, value2 = 4)\n\n\nThere are several additional ways to create conditional operations (see (Wickham and Grolemund 2017)).\n\n\n\n\n\n\n\nGroup work\n\n\n\n\nWrite a function that is reusable using this code:\n\n\nz <- df$var1 - mean(df$var1, na.rm = TRUE) / sd(df$var1, na.rm = TRUE)\n\n\nWrite a function that stops if the input is not a character vector\nWrite a function that calculates the standard deviation\n\n\\[s=\\sqrt{\\frac{\\sum(x_i-\\bar{x})^2}{n-1}}\\]\n\n# An example vector\nx <- c(2, 5, 7, 8)\n\n# Sum of squares\nss <- sum((x - mean(x))^2)\n\n## Calculates standard deviation\ns <- sqrt(ss / (length(x)-1))"
  },
  {
    "objectID": "ws12-writing-functions.html#a-function-to-calculate-lactate-thresholds",
    "href": "ws12-writing-functions.html#a-function-to-calculate-lactate-thresholds",
    "title": "Writing R functions and calculate a lactate threshold",
    "section": "A function to calculate lactate thresholds",
    "text": "A function to calculate lactate thresholds\n\nLactate threshold (LT) tests are commonly performed in the laboratory\nCalculations of the LT differs between labs and there are many methods\nA possible method is to calculate the power at a fixed lactate value (e.g. 4 mmoL L-1)\n\n\nlibrary(tidyverse)\nlibrary(exscidata)\n\ndata(\"cyclingstudy\")\n\n\ncyclingstudy %>%\n  # Select columns needed for analysis\n  select(subject, group, timepoint, lac.225:lac.375) %>%\n  # Only one participant and time-point\n  filter(timepoint == \"pre\", subject == 10) %>%\n  # Pivot to long format data using the lactate columns\n  pivot_longer(names_to = \"watt\", \n               values_to = \"lactate\", \n               names_prefix = \"lac.\",\n               names_transform = list(watt = as.numeric),\n               cols = lac.225:lac.375) %>%\n  # Plot the data, group = subject needed to connect the points\n  ggplot(aes(watt, lactate, group = subject))  + \n  geom_line(lty = 2) +\n  geom_point(shape = 21, fill = \"lightblue\", size = 2.5) \n\n\n\n\nA workload and lactate relationship\n\n\n\n\n\nManually, we could estimate the watt at a specific lactate using ocular inspection\n\n\ncyclingstudy %>%\n  # Select columns needed for analysis\n  select(subject, group, timepoint, lac.225:lac.375) %>%\n  # Only one participant and time-point\n  filter(timepoint == \"pre\", subject == 10) %>%\n  # Pivot to long format data using the lactate columns\n  pivot_longer(names_to = \"watt\", \n               values_to = \"lactate\", \n               names_prefix = \"lac.\",\n               names_transform = list(watt = as.numeric),\n               cols = lac.225:lac.375) %>%\n  # Plot the data, group = subject needed to connect the points\n  ggplot(aes(watt, lactate, group = subject))  + \n  geom_line(lty = 2) +\n  geom_point(shape = 21, fill = \"lightblue\", size = 2.5) +\n  # Adding straight lines at specific values\n  geom_hline(yintercept = 4, color = \"red\") +\n  geom_vline(xintercept = 341.5, color = \"blue\")\n\n\n\n\nA workload and lactate relationship, manual identification of the lactate threshold\n\n\n\n\n\nA better approximation can be derived from the curve linear model\n\n\ncyclingstudy %>%\n  # Select columns needed for analysis\n  select(subject, group, timepoint, lac.225:lac.375) %>%\n  # Only one participant and time-point\n  filter(timepoint == \"pre\", subject == 10) %>%\n  # Pivot to long format data using the lactate columns\n  pivot_longer(names_to = \"watt\", \n               values_to = \"lactate\", \n               names_prefix = \"lac.\",\n               names_transform = list(watt = as.numeric),\n               cols = lac.225:lac.375) %>%\n  # Plot the data, group = subject needed to connect the points\n  ggplot(aes(watt, lactate, group = subject))  + \n  geom_line(lty = 2) +\n  geom_point(shape = 21, fill = \"lightblue\", size = 2.5) +\n  geom_hline(yintercept = 4, color = \"red\") +\n  geom_vline(xintercept = 341.5, color = \"blue\") +\n  # Adding a straight line from a linear model\n  geom_smooth(method = \"lm\", se = FALSE, formula = y ~ x, color = \"#e41a1c\") +\n  \n  # Adding a polynomial linear model to the plot\n  \n  # poly(x, 2) add a second degree polynomial model.\n  geom_smooth(method = \"lm\", se = FALSE, formula = y ~ poly(x, 2), color = \"#377eb8\") +\n  # poly(x, 3) add a third degree polynomial model.\n  geom_smooth(method = \"lm\", se = FALSE, formula = y ~ poly(x, 3), color = \"#4daf4a\") +\n  # poly(x, 4) add a forth degree polynomial model.\n  geom_smooth(method = \"lm\", se = FALSE, formula = y ~ poly(x, 4), color = \"#ff7f00\") \n\n\n\n\nA workload and lactate relationship, adding curve linear models\n\n\n\n\n\nThese models are all “wrong but some are useful”1\n\n\nlactate <- cyclingstudy %>%\n  # Select columns needed for analysis\n  select(subject, group, timepoint, lac.225:lac.375) %>%\n  # Only one participant and time-point\n  filter(timepoint == \"pre\", subject == 10) %>%\n  # Pivot to long format data using the lactate columns\n  pivot_longer(names_to = \"watt\", \n               values_to = \"lactate\", \n               names_prefix = \"lac.\",\n               names_transform = list(watt = as.numeric),\n               cols = lac.225:lac.375) %>%\n  # Remove NA (missing) values to avoid warning/error messages.\n  filter(!is.na(lactate))\n\n# fit \"straight line\" model\nm1 <- lm(lactate ~ watt, data = lactate)\n\n# fit second degree polynomial\nm2 <- lm(lactate ~ poly(watt, 2, raw = TRUE), data = lactate)\n\n# fit third degree polynomial\nm3 <- lm(lactate ~ poly(watt, 3, raw = TRUE), data = lactate)\n\n# fit forth degree polynomial\nm4 <- lm(lactate ~ poly(watt, 4, raw = TRUE), data = lactate)\n\n# Store all residuals as new variables\nlactate$resid.m1 <- resid(m1)\nlactate$resid.m2 <- resid(m2)\nlactate$resid.m3 <- resid(m3)\nlactate$resid.m4 <- resid(m4)\n\nlactate %>%\n  # gather all the data from the models\n  pivot_longer(names_to = \"model\", \n               values_to = \"residual\", \n               names_prefix = \"resid.\", \n               names_transform = list(residual = as.numeric), \n               cols = resid.m1:resid.m4) %>%\n  # Plot values with the observed watt on x axis and residual values at the y\n  ggplot(aes(watt, residual, fill = model)) + geom_point(shape = 21, size = 3) +\n  \n  # To set the same colors/fills as above we use scale fill manual\n  scale_fill_manual(values = c(\"#e41a1c\", \"#377eb8\", \"#4daf4a\", \"#ff7f00\"))\n\n\n\n\nAssessing the fit of different linear models on a exercise intensity to lactate accumulation relationship\n\n\n\n\n\nUsing the predict() function we can predict lactate values at a specific power output.\nWe are modelling the effect of watt on lactate, so we are unable to input a specific lactate value, instead we could approximate with “inverse prediction\n\n\nndf <- data.frame(watt = seq(from = 225, to = 350, by = 0.1)) # high resolution, we can find the nearest10:th a watt\n\nndf$predictions <- predict(m3, newdata = ndf)\n\n# Which value of the predictions comes closest to our value of 4 mmol L-1?\n# abs finds the absolute value, makes all values positive, \n# predictions - 4 givs an exact prediction of 4 mmol the value zero\n# filter the row which has the prediction - 4 equal to the minimal absolut difference between prediction and 4 mmol\nlactate_threshold <- ndf %>%\n  filter(abs(predictions - 4) == min(abs(predictions - 4)))\n\n\n\n\n\n\n\nGroup work\n\n\n\n\nWrite a function that calculates the lactate threshold using a fixed lactate value.\nThe function should have arguments that defines the data and lactate value\nThe output should be a value of the approximate watt at the fixed lactate"
  },
  {
    "objectID": "ws13-p-values-simulation.html",
    "href": "ws13-p-values-simulation.html",
    "title": "Populations, samples and statistical inference",
    "section": "",
    "text": "We have a two-group design and want to know if Condition A is different from Condition B in any meaningful way.\nTo accomplish this we can test if an observed difference is very different to a reference, where any difference is up to chance\nUse the code chunk below to simulate data\n\n\nset.seed(1)\n# Population \nA <- rnorm(1000, mean = 100, 10)\n\nB <- rnorm(1000, mean = 92, 10)\n\n# Sample\na <- sample(A, 15, replace = FALSE)\nb <- sample(B, 15, replace = FALSE)\n\n\n\n\n\n\n\nGroup work\n\n\n\n\nUsing the t.test function, test against the null-hypothesis of no difference between groups\nConstruct a permutation test where a reference distribution of possible outcomes is created in a loop(!). Try to explain what the code below does.\nCalculate how many cases led to a more extreme result than the observed in your experiment.\n\n\nlibrary(tidyverse)\n\ndifferences <- vector()\n\nfor(i in 1:1000) {\n        \n        \n     samp <- sample(c(a, b), 30, replace = FALSE)\n        \n     differences[i] <- mean(samp[1:15]) - mean(samp[16:30])\n        \n        \n}\n\n\ndata.frame(differences) %>%\n        ggplot(aes(differences)) + geom_histogram() + \n        geom_vline(xintercept = mean(a) - mean(b), color = \"red\", size = 2)"
  },
  {
    "objectID": "ws13-p-values-simulation.html#the-effect-of-small-and-large-samples.",
    "href": "ws13-p-values-simulation.html#the-effect-of-small-and-large-samples.",
    "title": "Populations, samples and statistical inference",
    "section": "The effect of small and large samples.",
    "text": "The effect of small and large samples.\n\nThe sample size determines what differences we may observe.\n\n:::{.callout-Practice}\n\nGroup work\n\nRe-do the experiment above with a smaller sample size, and a larger sample size.\nReport your experiment as a t-test and a permutation test."
  },
  {
    "objectID": "ws13-p-values-simulation.html#limitations-of-p-values",
    "href": "ws13-p-values-simulation.html#limitations-of-p-values",
    "title": "Populations, samples and statistical inference",
    "section": "Limitations of p-values",
    "text": "Limitations of p-values\n\nP-values are often misinterpreted!\nFind a scientific paper (full text) in any area and search for “P < 0.05” and “P > 0.05”, see how authors interpret the results."
  },
  {
    "objectID": "ws14-sampling-power-effects.html",
    "href": "ws14-sampling-power-effects.html",
    "title": "Sampling, effects and power",
    "section": "",
    "text": "Statistical power is the long-run ability of a study design to detect a true effect.\nIn frequentist statistics, the true effect is fixed and we want to use a sample to estimate it.\nSince we do not know the true effect, the goal of a power analysis is to reach sufficient power for an assumed effect (Lakens 2022).\nThis assumption can be based on a hypothesis of a true effect, or\nAn effect that is considered clinically meaningful\n\n\n\nWe now will build a simulation study to investigate\n\nHow often will we find a “true” population effect in studies of different sizes?\nWhat is the relationship between effect size, power and statistical significance?\nWhat is the effect of sample size on the precision of estimates (confidence intervals)\n\nOur simulation will investigate a known population effect of 0.5. This standardized effect (\\(d\\)) is in the one-sample case \\(d = \\frac{mean}{SD}\\).\n\n\nCode\nlibrary(tidyverse)\nset.seed(1)\n\n\n# Create a population to sample from with a known effect\n# In standardized terms, the population effect is 0.5 = mean / sd.\npopulation <- rnorm(10^6, 5, 10) \n\n# Calculate the population effect size \npop.es <- mean(population) / sd(population)\n\n\nresults_total <- list()\n\n# 0. Inside for-loop:\nfor(i in 1:1000) {\n  \n# 1. Sample from the population with sample size 10 to 100\n  \n  # 1.1 create a vector of sample sizes\n  sample.sizes <- seq(from = 10, to = 100, by = 10)\n  # 1.2 create a list to store results\n  results_sub <- list()\n  \n  # 1.3 Inside a nested for-loop, perform sampling with each sample size\n  for(j in 1:length(sample.sizes)) {\n    \n  samp <- sample(population, sample.sizes[j], \n                 replace = FALSE)\n  \n  \n# 2. Create a model\n  m <- lm(y ~ 1, data = data.frame(y = samp))\n  \n  # 2.1 Store results from each model as a data frame in a list:\n results_sub[[j]] <- data.frame(mean = coef(summary(m))[1, 1],  # Estimated mean\n             se = coef(summary(m))[1, 2],  # Standard error\n             pval = coef(summary(m))[1, 4],  # p-value\n             ci.lwr = confint(m)[1], # confidence interval \n             ci.upr = confint(m)[2], # confidence interval\n             effect.size = mean(samp) / sd(samp),\n             sample.size = sample.sizes[j]) # Sample size\n  \n    \n    \n  }\n  # 2.2 Combine all data frames from each sample size as a data frame in a list\n  results_total[[i]] <- bind_rows(results_sub)\n\n  \n  # 3. Repeat 1 and 2 1000 times, performing 1000 studies each with \n  # sample sizes 10 to 100.\n  \n}\n\n# Combine all results\nresults_total <- bind_rows(results_total)\n\n\n\n\n\n\nAs the sample size increases, we are more likely to have a study that detects a true effect\nWe simply count the proportion of “studies” that declear a statistically significant effect at \\(p<0.05\\)\n\n\n\nCode\n# Count numbers of studies with p < 0.05\nresults_total %>%\n  filter(pval < 0.05) %>%\n  group_by(sample.size) %>%\n  summarise(n = n(), \n            prop = n / 1000) %>%\n  ggplot(aes(sample.size, 100 * prop)) + geom_line() + geom_point(shape = 21, fill = \"lightblue\", size = 3) + \n  labs(x = \"Sample size\", \n       y = \"Percentage of studies rejecting the null-hypothesis\", \n       title = \"Statistical power and sample size\", \n       subtitle = \"Statistical significance set to 0.05, standardized effect-size 0.5\") +\n  theme_bw()\n\n\n\n\n\n\nThe relationship between power and sample size can be used to make cost-benefit analyses of future studies.\nThe “cost” of a study can be regarded as e.g. economical or ethical.\nIn most cases the power calculation can be done without simulations, using e.g. the pwr package.\n\n\n\nCode\nlibrary(pwr)\n\nsample.sizes <- seq(from = 10, to = 100, by = 10)\nresults.pwr <- list()\n\nfor(i in 1:length(sample.sizes)) {\n  \n  pwr_analysis <- pwr.t.test(type = \"one.sample\", d = 5/10, sig.level = 0.05, n = sample.sizes[i])\n  \n\n  results.pwr[[i]] <- data.frame(sample.size = sample.sizes[i], \n                                 prop = pwr_analysis$power)\n  \n  \n  }\n\nresults.pwr <- bind_rows(results.pwr)\n\n# Count numbers of studies with p < 0.05\nresults_total %>%\n  filter(pval < 0.05) %>%\n  group_by(sample.size) %>%\n  summarise(n = n(), \n            prop = n / 1000) %>%\n  ggplot(aes(sample.size, 100 * prop)) + \n  geom_line() + \n  geom_point(shape = 21, fill = \"lightblue\", size = 3) + \n  \n  geom_line(data = results.pwr, color = \"red\") +\n  \n  labs(x = \"Sample size\", \n       y = \"Percentage of studies rejecting the null-hypothesis\", \n       title = \"Statistical power and sample size\", \n       subtitle = \"Statistical significance set to 0.05, standardized effect-size 0.5.\") +\n  theme_bw()\n\n\n\n\n\nPower analysis using simulations (blue circles) and analytically (red line)\n\n\n\n\n\n\n\n\nA standardized (observed) effect size may be calculated from each study. What is the relationship between effect-sizes, sample sizes and p-values?\nThe p-value is directly related to the observed effect-size. The sample size determines the p-value at a specific effect-size.\n\n\n\nCode\nlibrary(cowplot)\n\n\nplotA <- results_total %>%\n  ggplot(aes(effect.size, pval, \n             color = as.factor(sample.size))) + geom_point() +\n  labs(color = \"Sample size\") +\n  \n    geom_hline(yintercept = 0.05, color = \"red\") + \n  annotate(\"text\", x = 1.5, y = 0.05 + 0.1, \n           color = \"red\",\n           label = \"P-value = 0.05\", \n           size = 2.5) +\n  \n  geom_hline(yintercept = 0.001, color = \"blue\") + \n  annotate(\"text\", x = 1.5, y = 0.001 + 0.1, \n           color = \"blue\",\n           label = \"P-value = 0.01\", \n           size = 2.5) +\n  \n  labs(title = \"Effect size vs. p-values\", \n       x = \"Effect size\", \n       y = \"P-value\") + \n\n  theme_bw() + \n  theme(legend.position = \"none\")\n\n\n\nplotB <- results_total %>%\n  ggplot(aes(effect.size, -log(pval), \n             color = as.factor(sample.size))) + geom_point() +\n  labs(color = \"Sample size\", \n       title = \" \", \n       x = \"Effect size\", \n       y = \"-log(p-value)\") +\n  \n  geom_hline(yintercept = -log(0.05), color = \"red\") + \n  annotate(\"text\", x = -0.2, y = -log(0.05) + 1, \n           color = \"red\",\n           label = \"P-value = 0.05\", \n           size = 2.5) +\n  \n  geom_hline(yintercept = -log(0.001), color = \"blue\") + \n  annotate(\"text\", x = -0.2, y = -log(0.001) + 1, \n           color = \"blue\",\n           label = \"P-value = 0.01\", \n           size = 2.5) +\n  scale_x_continuous(breaks = c(0, 0.5, 1, 2)) +\n  \n  \n  theme_bw()\n\nlibrary(cowplot)\nplot_grid(plotA, plotB, ncol = 2, rel_widths = c(0.8, 1))\n\n\n\n\n\n\nThe observed effect size is an estimation of the population effect size. In our case, the population effect-size is \\(\\frac{mean}{SD} = \\frac{5}{10} = 0.5\\).\nHow well do we estimate effect sizes?\n\n\n\nCode\nlibrary(ggtext)\n\nresults_total %>%\n  ggplot(aes(sample.size, effect.size, color = pval)) + \n  \n  geom_point(position = position_jitter(), alpha = 0.5) +\n    geom_hline(yintercept = pop.es, lty = 1, size = 2, color = \"red\") +\n  labs(x = \"Sample size\", \n       y = \"Observed standardized effect-size\", \n       color = \"P-value\", \n           title = \"Observed effect-sizes per sample size\") +\n  annotate(\"richtext\", x = 150, y = pop.es, \n           color = \"red\", \n           hjust = 1,\n           label = \"Population effect-size\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nPrecision could be regarded as the ability to estimate an effect with some degree of certainty.\nThe confidence interval (CI) is constructed to find the true population effect at a given rate (e.g. 95% of studies).\nCI depends on the sample size:\n\n\\[95\\%~CI:~Estimate \\pm t_{\\text{critical}} \\times SE\\]\nFor n = 10:\n\\[95\\%~CI:~Estimate \\pm 2.26 \\times SE\\]\n\n\nCode\nresults_total %>%\n  ggplot(aes(sample.size, ci.upr-ci.lwr, fill = as.factor(sample.size))) +\n  geom_point(position = position_jitter(width = 0.8),\n             shape = 21, alpha = 0.4) +\n  labs(x = \"Sample size\", \n       y = \"Range of confidence intervals (Upper - Lower)\", \n       title = \"Confidence interval width and sample size\") + \n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\nTo decrease the width of the confidence interval by half, we need to increase the sample size about 3-4 times.\n\n\n\nCode\nlibrary(knitr)\n\nresults_total %>% \n  mutate(ci_width = ci.upr-ci.lwr) %>%\n  group_by(sample.size) %>%\n  summarise(ci_width = mean(ci_width)) %>%\n  kable(col.names = c(\"Sample size\", \"95% CI width\"), \n        digits = c(1, 1))\n\n\n\n\n\nSample size\n95% CI width\n\n\n\n\n10\n14.0\n\n\n20\n9.3\n\n\n30\n7.4\n\n\n40\n6.4\n\n\n50\n5.7\n\n\n60\n5.1\n\n\n70\n4.7\n\n\n80\n4.4\n\n\n90\n4.2\n\n\n100\n4.0\n\n\n\n\n\n\nThe confidence interval covers the true population parameter in 95% of repeated studies. 1000 studies is not enough to reach exactly 95%.\n\n\n\nCode\n#| echo: true\n#| message: false\n#| warning: false\n\n\n\nresults_total %>%\n  mutate(sig = if_else(ci.lwr <= 5 & ci.upr >= 5, \"sig\", \"ns\"), \n         interval = paste0(rep(seq(1:1000), each = 10))) %>%\n  filter(sig == \"sig\") %>%\n  group_by(sample.size) %>%\n  summarise(prop = 100 * (n() / 1000)) %>%\n    kable(col.names = c(\"Sample size\", \"Proportion of 95% CI finding the true effect\"), \n        digits = c(1, 1))\n\n\n\n\n\nSample size\nProportion of 95% CI finding the true effect\n\n\n\n\n10\n95.1\n\n\n20\n95.1\n\n\n30\n95.1\n\n\n40\n95.1\n\n\n50\n95.2\n\n\n60\n94.1\n\n\n70\n94.5\n\n\n80\n94.8\n\n\n90\n94.0\n\n\n100\n94.6\n\n\n\n\n\n\nWe will be wrong at the same rate with a given coverage of the confidence interval independent of the sample size.\nHowever, we could increase the coverage of the interval and maintain precision with a larger sample size\n\n\n\nCode\n#| echo: true\n#| message: false\n#| warning: false\n\n\nresults_total %>%\n  mutate(sig = if_else(ci.lwr <= 5 & ci.upr >= 5, \"sig\", \"ns\"), \n         interval = paste0(rep(seq(1:1000), each = 10))) %>%\n  filter(interval %in% seq(1:100)) %>%\n  \n  ggplot(aes(mean, interval, color = sig, alpha = sig)) + \n  geom_errorbarh(aes(xmin = ci.lwr, \n                    xmax = ci.upr)) + \n  \n  scale_alpha_manual(values = c(1, 0.3)) +\n  \n  geom_vline(xintercept = 5, color = \"red\") +\n  \n  facet_wrap(~ sample.size) + \n  \n  labs(title = \"Confidence intervals from 100 studies\", \n       subtitle = \"Red intervals misses the population average\") +\n  theme_bw() +\n  \n  theme(axis.text.y = element_blank(), \n        axis.ticks.y = element_blank(), \n        legend.position = \"none\", \n        axis.title = element_blank())"
  },
  {
    "objectID": "ws14-sampling-power-effects.html#what-if-there-is-no-effect",
    "href": "ws14-sampling-power-effects.html#what-if-there-is-no-effect",
    "title": "Sampling, effects and power",
    "section": "What if there is no effect?",
    "text": "What if there is no effect?\n\nStudies that examine a population effect that is close to zero will be wrong at rate of 5%, if the \\(\\alpha\\) level is set to 0.05.\nThe “power” of such studies is 5% regardless of the sample size.\n\n\n\nCode\n#| echo: true\n#| message: false\n#| warning: false\n\n\n\nlibrary(tidyverse)\nset.seed(1)\n\n\n# Create a population to sample from with a known effect\n# In standardized terms, the population effect is 0 = mean / sd.\npopulation <- rnorm(10^6, 0, 10) \n\n# Calculate the population effect size \npop.es <- mean(population) / sd(population)\n\n\nresults_total_null <- list()\n\n\n# 0. Inside for-loop:\nfor(i in 1:1000) {\n  \n# 1. Sample from the population with sample size 10 to 100\n  \n  # 1.1 create a vector of sample sizes\n  sample.sizes <- seq(from = 10, to = 100, by = 10)\n  # 1.2 create a list to store results\n  results_sub <- list()\n  \n  # 1.3 Inside a nested for-loop, perform sampling with each sample size\n  for(j in 1:length(sample.sizes)) {\n    \n  samp <- sample(population, sample.sizes[j], \n                 replace = FALSE)\n  \n  \n# 2. Create a model\n  \n  m <- lm(y ~ 1, data = data.frame(y = samp))\n  \n  # 2.1 Store results from each model as a data frame in a list:\n  \n results_sub[[j]] <- data.frame(mean = coef(summary(m))[1, 1],  # Estimated mean\n             se = coef(summary(m))[1, 2],  # Standard error\n             pval = coef(summary(m))[1, 4],  # p-value\n             ci.lwr = confint(m)[1], # confidence interval \n             ci.upr = confint(m)[2], # confidence interval\n             effect.size = mean(samp) / sd(samp),\n             sample.size = sample.sizes[j]) # Sample size\n  \n    \n    \n  }\n  # 2.2 Combine all data frames from each sample size as a data frame in a list\n\n  results_total_null[[i]] <- bind_rows(results_sub)\n\n  \n  # 3. Repeat 1 and 2 1000 times, performing 1000 studies each with \n  # sample sizes 10 to 100.\n  \n  # If we use a \"progress bar\"\n  # setTxtProgressBar(pb, i)\n  \n}\n\n# Close the progress bar\n# close(pb)\n\n# Combine all results\nresults_total_null <- bind_rows(results_total_null)\n\n\n\n\nCode\n# Count numbers of studies with p < 0.05\nresults_total_null %>%\n  filter(pval < 0.05) %>%\n  group_by(sample.size) %>%\n  summarise(n = n(), \n            prop = n / 1000) %>%\n  ggplot(aes(sample.size, 100 * prop)) + \n  geom_hline(yintercept = 5, lty = 2, color = \"grey50\") +\n  geom_line() + \n  geom_point(shape = 21, fill = \"lightblue\", size = 3) + \n  labs(x = \"Sample size\", \n       y = \"Percentage of studies rejecting the null-hypothesis\", \n       title = \"Statistical power and sample size\", \n       subtitle = \"Statistical significance set to 0.05, standardized effect-size 0\") +\n  scale_y_continuous(limits = c(0, 100)) +\n  theme_bw()\n\n\n\n\n\n\nP-values in studies examining population zero-effects will be uniformly distributed\n\n\n\nCode\nresults_total_null %>%\n  ggplot(aes(pval)) + \n  geom_histogram(aes(y=100 * ..count../1000), \n                 binwidth = 0.05, boundary = 0, color = \"gray30\", fill = \"white\") + \n  labs(x = \"P-value\", \n       y = \"Percentage of studies\", \n       title = \"Distribution of p-values from studies of a population zero-effect\") +\n  scale_y_continuous(limits = c(0, 10), expand = c(0, 0)) +\n  facet_wrap(~ sample.size) +\n  theme_bw()\n\n\n\n\n\n\nBe aware of p = 0.047, this p-value can be more probable when sampling from the zero-effect population in certain cases, an example:\n\n\n\nCode\nresults_total %>%\n  filter(sample.size == 50, \n         pval <= 0.05) %>%\n    ggplot(aes(pval)) + \n  geom_histogram(aes(y=100 * ..count../1000), \n                 binwidth = 0.005, boundary = 0, color = \"gray30\", fill = \"darkolivegreen4\") +\n  \n  \n    geom_histogram(data = results_total_null %>%\n                          filter(sample.size == 50, \n                                  pval <= 0.05), \n  aes(y=100 * ..count../1000), \n                 binwidth = 0.005, boundary = 0, color = \"gray30\", fill = \"darkblue\") +\n  \n    labs(x = \"P-value\", \n       y = \"Percentage of studies\", \n       title = \"Distribution of p-values from studies of true zero-effect and true d = 0.5\", \n       subtitle = \"Sample size: n = 50\") +\n  annotate(\"text\", x = 0.03, y = 5, label = \"Population effect: 0\", color = \"darkblue\") +\n  annotate(\"text\", x = 0.03, y = 6, label = \"Population effect: 0.5\", color = \"darkolivegreen4\") +\n  coord_cartesian(ylim=c(0,10)) +\n  theme_bw()"
  },
  {
    "objectID": "ws15-study-design-1.html",
    "href": "ws15-study-design-1.html",
    "title": "Study designs and statistical tests",
    "section": "",
    "text": "A regression model can be used to assess the differences between treatments in a randomized controlled trial (RCT). But what do we use as the dependent variable in this model?\nVickers (2001) identified and compared four scenarios that might be “tested” from a comparison of two groups:\n\n\nBaseline (\\(b\\)), follow-up (\\(f\\)) and group (\\(g\\)) can be analyzed as\n\nPost-scores only: \\(f\\) is compared between \\(g\\)\nPercentage change: \\(100 \\times \\frac{b-f}{b}\\) is compared between \\(g\\)\nChange: \\(b-f\\) compared between \\(g\\)\nAnalysis of co-variance (ANCOVA): \\(g\\) is compared as the effect in \\(f=\\beta_1 b + \\beta_2 g\\)\n\n\n\nThe choice of method should be based on its efficiency, the ability find true effects using a minimal number of participants.\nVickers show that the use of percentage change from baseline is inefficient (Vickers 2001) and could lead to violations of assumptions for statistical tests.\nThe ANCOVA model (see below) is the preferred model when model assumptions holds (Vickers 2001)\nWe will proceed with comparison of change-scores and later discuss ANCOVA."
  },
  {
    "objectID": "ws15-study-design-1.html#analysis-of-change-scores",
    "href": "ws15-study-design-1.html#analysis-of-change-scores",
    "title": "Study designs and statistical tests",
    "section": "Analysis of change scores",
    "text": "Analysis of change scores\n\n\nCode\nlibrary(tidyverse)\ndata.frame(Time = c(\"Pre\", \"Post\", \"Pre\", \"Post\"), outcome = c(0,1, 0,1)) %>%\n        mutate(Time = factor(Time, levels = c(\"Pre\", \"Post\"))) %>%\n        ggplot(aes(Time, outcome)) + \n        theme_classic() + \n        ylab(\"Outcome\") +\n        theme(axis.text.y = element_blank()) +\n        geom_segment(aes(x = Time, xend = 2, \n                         y = c(0.5,0.5, 0.5,1.5), \n                         yend = c(0.5,0.5, 1.5,1.5))) + \n        scale_y_continuous(limits = c(0,2)) + \n        annotate(geom = \"text\", x = 2.25, y = 1.5, label = \"Treatment\") +\n        annotate(geom = \"text\", x = 2.25, y = 0.5, label = \"Control\") \n\n\n\n\n\n\nIn a simple analysis of two parallel groups (experimental and control), the alternative hypothesis is that the two groups are different in terms of change scores.\n\n\\[ \\Delta Y_{Treatment}-\\Delta Y_{Control} \\neq 0 \\]\n\nThe null-hypothesis that we use in our statistical models is\n\n\\[ \\Delta Y_{Treatment}-\\Delta Y_{Control} = 0 \\]\n\nIn the simple case, this equals a t-test.\n\nt.test(delta ~ group, paired = FALSE)\n\nA similar scenario is a comparison between two treatments (A and B)\n\n\n\nCode\ndata.frame(Time = c(\"Pre\", \"Post\", \"Pre\", \"Post\"), outcome = c(0,1, 0,1)) %>%\n        mutate(Time = factor(Time, levels = c(\"Pre\", \"Post\"))) %>%\n        ggplot(aes(Time, outcome)) + \n        theme_classic() + \n        ylab(\"Outcome\") +\n        theme(axis.text.y = element_blank()) +\n        geom_segment(aes(x = Time, xend = 2, \n                         y = c(0.5,0.5, 0.5,1.5), \n                         yend = c(1.1,0.5, 1.5,1.5))) + \n        scale_y_continuous(limits = c(0,2)) + \n        annotate(geom = \"text\", x = 2.25, y = 1.5, label = \"A\") +\n        annotate(geom = \"text\", x = 2.25, y = 1.1, label = \"B\") \n\n\n\n\n\n\nThe null-hypothesis is that\n\n\\[ \\Delta Y_{A}-\\Delta Y_{B} = 0 \\]\n\nThe tenthirty data set has similar data as in the above scenario\nWe want to know what treatment to recommend for 1RM improvements.\n\n\n\n\n\n\n\nGroup work: Ten-thirty\n\n\n\n\nUsing the tenthirty data, use filter to keep time-point “pre” and “post” for the leg-press exercise.\nMake a plot of the averages of each group.\nCalculate a change score as post - pre.\nMake a plot of the average change score for each group.\nFit a regression model (change ~ group) and interpret the results.\n\n\nQuestions:\n\nConcerning the sample: Which group has the largest improvement i leg-press 1RM?\nConcerning inference to the population: Which method do you recommend for improving 1RM leg-press?\n\n\n\n\n\n\nPossible solutions\n## Create a plot of averages over time ----------- ##\n\nlibrary(tidyverse); library(exscidata)\ndata(\"tenthirty\")\n\n# Save the data set in a filtered version\ntenthirty_reduced <- tenthirty %>%\n  filter(time %in% c(\"pre\", \"post\"), \n         exercise == \"legpress\", \n         !is.na(load)) %>% \n  print()\n\n# Create a line plot with one line per group\ntenthirty_reduced %>%  \n  group_by(time, group) %>%\n  summarise(m = mean(load)) %>%\n  mutate(time = factor(time, levels = c(\"pre\", \"post\"))) %>%\n  ggplot(aes(time, m, color = group, \n             group = group)) + geom_line()\n\n\n# Create a plot of average changes per group\ntenthirty_reduced %>%\n  pivot_wider(names_from = time, \n              values_from = load) %>%\n  mutate(change = post - pre) %>%\n  group_by(group) %>%\n  summarise(mean_change = mean(change, na.rm = TRUE)) %>%\n  ggplot(aes(group, mean_change, fill = group)) + \n  geom_bar(stat = \"identity\", width = 0.3)\n  \n\n# Calculate the average change per group, and difference between groups\ntenthirty_reduced %>%\n  pivot_wider(names_from = time, \n              values_from = load) %>%\n  mutate(change = post - pre) %>%\n  group_by(group) %>%\n  summarise(mean_change = mean(change, na.rm = TRUE)) %>%\n  pivot_wider(names_from = group, \n              values_from = mean_change) %>%\n  mutate(diff = RM30 - RM10) %>%\n  print()\n\n\n# Calculate change score and save new data set\ndat <- tenthirty_reduced %>%\n  pivot_wider(names_from = time, \n              values_from = load) %>%\n  mutate(change = post - pre) %>%\n  print()\n\n\n# Fit model with change scores explained by group\nm1 <- lm(change ~ group, data = dat)\n\n# Get the model summary\nsummary(m1)\n\n\n\n\nCode\nlibrary(exscidata)\n\ntenthirty %>%\n  filter(exercise == \"legpress\", \n         time %in% c(\"pre\", \"post\")) %>%\n  mutate(time = factor(time, levels = c(\"pre\", \"post\"))) %>%\n  group_by(time, group) %>%\n  summarise(m = mean(load, na.rm = TRUE)) %>%\n  ggplot(aes(time, m, group = group, color = group)) + geom_line() +\n  labs(x = \"Time\", y = \"Legpress performance (kg)\", color = \"Group\") +\n          theme_classic()\n\n\n\n\n\nGroup averages over time"
  },
  {
    "objectID": "ws15-study-design-1.html#from-t-test-to-ancova",
    "href": "ws15-study-design-1.html#from-t-test-to-ancova",
    "title": "Study designs and statistical tests",
    "section": "From t-test to ANCOVA",
    "text": "From t-test to ANCOVA\n\nAbove we used the simple comparison of change score. A more appropriate model to study outcomes of a RCT is an analysis of co-variance (ANCOVA)\nIn an ANCOVA, change-scores or post values are compared and the estimates tells you the adjusted differences between groups if they had the exact same starting point.\nThe ANCOVA model can be written as\n\n\\[f =\\beta_0 + \\beta_1\\times b + \\beta_2\\times g \\]\nwhere \\(f\\) is the follow-up score, \\(b\\) is the baseline and \\(g\\) is the group.\n\nIn R code, this corresponds to\n\npost ~ pre + group\n\n\n\n\n\n\nGroup work: Ten-thirty ANCOVA\n\n\n\n\nFit an ANCOVA model with change scores as the dependent variable (m1)\nFit an ANCOVA model with the post values as the dependent variable (m2)\nPre-training values and group should be covariates in both models.\nUse the aov command to fit an ANOVA model (m3)\nEvaluate models using summary and anova\n\n\nQuestions\n\nConcerning inference to the population: Which method do you recommend for improving 1RM leg-press?\nIn what ways are the models different?\nWhat is happening with the aov command, how is it different to m2?\n\n\n\n\n\n\nPossible solutions\n# Create a data set with change\n\ndat <- tenthirty %>%\n  filter(exercise == \"legpress\", \n         time %in% c(\"pre\", \"post\")) %>%\n  pivot_wider(names_from = time, \n              values_from = load) %>%\n  mutate(change = post - pre, \n         pre = pre - mean(pre)) %>% # Mean center pre-values\n  print()\n\n\n# Fit models \n\nm1 <- lm(post ~ pre + group, data = dat)\n\nm2 <- lm(change ~ pre + group, data = dat)\n\n\nsummary(m1)\nsummary(m2)\n\n\nm3 <- aov(post ~ pre + group, data = dat)\n\nsummary(m3)\nanova(m1)"
  },
  {
    "objectID": "ws15-study-design-1.html#understanding-the-ancova-model",
    "href": "ws15-study-design-1.html#understanding-the-ancova-model",
    "title": "Study designs and statistical tests",
    "section": "Understanding the ANCOVA model",
    "text": "Understanding the ANCOVA model\n\nThe ANCOVA estimates the difference between e.g. post scores given a (estimated) association between pre- and post-scores.\nThis can be visualized in a simple plot where the difference in the paralell lines are the treatment effect\n\n\n\nCode\nm1 <- tenthirty %>%\n  filter(exercise == \"legpress\", \n         time %in% c(\"pre\", \"post\")) %>%\n  pivot_wider(names_from = \"time\", values_from = \"load\") %>%\n  mutate(change = post - pre) %>%\n  lm(post ~ pre + group, data = .) \n\n\ntenthirty %>%\n  filter(exercise == \"legpress\", \n         time %in% c(\"pre\", \"post\")) %>%\n    pivot_wider(names_from = \"time\", values_from = \"load\") %>%\n  ggplot(aes(pre, post, color = group)) + geom_point() + \n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  geom_abline(intercept = coef(m1)[1], slope = coef(m1)[2], color = \"red\")  + \n  geom_abline(intercept = coef(m1)[1] + coef(m1)[3], slope = coef(m1)[2], color = \"blue\") + \n  labs(title = \"Estimates from an ANCOVA model\", \n       subtitle = \"post ~ pre + group\", \n       color = \"Group\") + theme_classic()\n\n\n\n\n\n\nBy correcting for baseline values we can expect different estimates for each group, notice also that the standard errors are changing\n\n\n\nCode\nlibrary(emmeans)\n\nm0 <- tenthirty %>%\n  filter(exercise == \"legpress\", \n         time %in% c(\"pre\", \"post\")) %>%\n  pivot_wider(names_from = \"time\", values_from = \"load\") %>%\n  mutate(change = post - pre) %>%\n  lm(post ~ group, data = .) \n\nm2 <- tenthirty %>%\n  filter(exercise == \"legpress\", \n         time %in% c(\"pre\", \"post\")) %>%\n  pivot_wider(names_from = \"time\", values_from = \"load\") %>%\n  mutate(change = post - pre) %>%\n  lm(change ~ group, data = .) \n\n\n\n\n\nbind_rows(data.frame(emmeans(m0, specs = ~ group)) %>%\n        mutate(Model = \"Post only\"), \n        data.frame(emmeans(m1, specs = ~ group)) %>%\n        mutate(Model = \"ANCOVA\")) %>%\n        ggplot(aes(group, emmean, fill = Model)) + \n        geom_errorbar(aes(ymin = emmean, ymax = emmean + SE), \n                      position = position_dodge(), width = 0.5) +\n        geom_bar(stat = \"identity\", position = position_dodge(), \n                 width = 0.5)  \n\n\n\n\n\nGroup estimates from two different models, error bars are standard errors (SE)\n\n\n\n\n\nCalculating the differences between groups we can also see differences, here we also incorporate the change score model.\n\n\n\nCode\nbind_rows(\ndata.frame(confint(contrast(emmeans(m0, specs = ~ group)), adjust = \"none\")) %>%\n        mutate(Model = \"Post only\"),\ndata.frame(confint(contrast(emmeans(m1, specs = ~ group)), adjust = \"none\")) %>%\n        mutate(Model = \"ANCOVA\"),\ndata.frame(confint(contrast(emmeans(m2, specs = ~ group)), adjust = \"none\")) %>%\n        mutate(Model = \"Change score\")) %>%\n        filter(contrast == \"RM10 effect\") %>%\n        ggplot(aes(Model, estimate, fill = Model)) + \n        geom_errorbar(aes(ymin = lower.CL, ymax = upper.CL), width = 0.2) +\n        geom_point(shape = 21, size = 3) + \n        labs(y = \"Estimated mean difference between groups\") + \n        geom_hline(yintercept = 0, lty = 2)\n\n\n\n\n\nGroup estimates from two different models, error bars are 95% confidence intervals.\n\n\n\n\n\nThe change-score model will have lower power compared to the ANCOVA model when the correlation between pre- and post-scores decrease (Vickers 2001)"
  },
  {
    "objectID": "ws16-analyzing-trials.html",
    "href": "ws16-analyzing-trials.html",
    "title": "Analyzing trials",
    "section": "",
    "text": "Group work: Hypertrophy and graded protein dose\n\n\n\n\nRead Haun et al. (Haun et al. 2018) and find the main purpose/aim/hypothesis of the study.\nHow do they motivate their purpose/aim/hypothesis?\nWhat is the statistical test used test the main hypotheses?\n\n\n\nDesign an analysis of the data from Haun et al. (2018):\n\nInclude only two time-points (T1 and T3)\nSelect one measure of muscle hypertrophy\nSelect a test for a null-hypothesis significance test\nDo your results correspond to what is found in Haun et al. (2018)?"
  },
  {
    "objectID": "ws16-analyzing-trials.html#a-possible-solution",
    "href": "ws16-analyzing-trials.html#a-possible-solution",
    "title": "Analyzing trials",
    "section": "A possible solution",
    "text": "A possible solution\n\nWe will start by exploring the data, we will use the VL (vastus lateralis thickness) data to answer the question if protein supplementation affects training outcomes. Data exploration could mean that we plot the data and include information on groups etc.\nThe plot below shows the relationship between pre- and post-intervention values with groups indicated by different colors. Points above the line indicates that a participant has greater values after the intervention. Points below the identity line indicates that participants have a negative development pre- to post-intervention.\n\n\n\nCode\nlibrary(exscidata); library(tidyverse)\ndata(\"hypertrophy\")\n\n# For plotting we will set a color scale \ncol_scale <- c(\"#1b9e77\", \"#d95f02\", \"#7570b3\")\n\nhypertrophy %>%\n  select(PARTICIPANT, GROUP, VL_T1, VL_T3) %>%\n  ggplot(aes(VL_T1, VL_T3, color = GROUP)) + geom_point(size = 2.5) +\n  geom_abline(intercept = 0, slope = 1) + \n  \n  scale_color_manual(values = col_scale) +\n  \n  labs(title = \"Relationship between pre- and post-training VL thickness\", \n       subtitle = \"The line indicates y = x\",\n       x = \"Pre-training VL thickness (cm)\", \n       y = \"Post-training VL thickness (cm)\")\n\n\n\n\n\n\nA simple model would be an extension of the above plot. Accounting for the relationship between pre- and post-intervention values, what is the difference between groups at post-intervention. This question can be answered with an ANCOVA. We will start by fitting the model:\n\n\n\nCode\n#| echo: true\n#| message: false\n#| warning: false\n\n# Save the data set\ndat <- hypertrophy %>%\n  select(PARTICIPANT, GROUP, VL_T1, VL_T3)\n\n# Fit the model\nm1 <- lm(VL_T3 ~ VL_T1 + GROUP, data = dat)\n\n\n# Plot all diagniostic plots in a single plot\npar(mfrow = c(2, 2)) \nplot(m1)\n\n\n\n\n\nCode\npar(mfrow = c(1,1)) # resets the plot window\n\n\n\nThe diagnostic plots shows no obvious patterns in the residual vs. fitted plots (left column), the residuals might be slightly deviating from a normal distribution (top right) and no data point falls outside the Cook’s distance indicating that no single data point influences the fit to large degree (bottom right).\nWe may now extract the model coefficients. We will start by plotting them in a graph.\n\n\n\nCode\nhypertrophy %>%\n  select(PARTICIPANT, GROUP, VL_T1, VL_T3) %>%\n  ggplot(aes(VL_T1, VL_T3, color = GROUP)) + geom_point(size = 2.5) +\n  \n    scale_color_manual(values = col_scale) +\n  \n  geom_abline(intercept = coef(m1)[1], slope = coef(m1)[2], color = col_scale[1], size = 1.5) + \n   geom_abline(intercept = coef(m1)[1] + coef(m1)[3] , slope = coef(m1)[2], color = col_scale[2], size = 1.5) + \n   geom_abline(intercept = coef(m1)[1] + coef(m1)[4], slope = coef(m1)[2], color = col_scale[3], size = 1.5 ) + \n  labs(title = \"Relationship between pre- and post-training VL thickness\", \n       subtitle = \"Lines indicates group averages from the ANCOVA model\",\n       x = \"Pre-training VL thickness (cm)\", \n       y = \"Post-training VL thickness (cm)\")\n\n\n\n\n\n\nEach line in the plot above indicate group averages in post scores given pre-intervention scores. The vertical distance between lines indicate the differences between groups.\nThe model:\n\n\\[y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_3\\]\nhas an intercept \\(\\beta_0\\) that corresponds to the post-intervention values when all coefficients are set to 0 (all \\(X = 0\\)). The slope \\(\\beta_1X_1\\) is the pre-intervention values and \\(\\beta_2X_2\\) and \\(\\beta_3X_3\\) are group indicators (\\(X_2 = 1\\) indicates group = MALTO, \\(X_3 = 1\\) indicates group = WP) giving the differences between MALTO and WP to the reference level GWP.\n\nInspecting the summary gives supports what the figure already shows, that there are no great differences between groups in post-intervention values:\n\n\n\nCode\nsummary(m1)\n\n\n\nCall:\nlm(formula = VL_T3 ~ VL_T1 + GROUP, data = dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.63728 -0.20006  0.03602  0.16346  0.40851 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.84974    0.27620   3.077  0.00488 ** \nVL_T1        0.72940    0.08687   8.396 7.06e-09 ***\nGROUPMALTO   0.02309    0.10676   0.216  0.83046    \nGROUPWP      0.09790    0.10969   0.892  0.38033    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2421 on 26 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.7379,    Adjusted R-squared:  0.7076 \nF-statistic:  24.4 on 3 and 26 DF,  p-value: 1.004e-07\n\n\n\nHaun et al. (2018) used delta-scores (or change-scores) to answer their question. Instead of modelling the post-values we could model the change. We will start by exploring the relationship between the baseline and change. We will then fit a model on change scores.\n\n\n\nCode\n#| echo: true\n#| message: false\n#| warning: false\n\nhypertrophy %>%\n  select(PARTICIPANT, GROUP, VL_T1, VL_T3) %>%\n  mutate(change = VL_T3 - VL_T1) %>%\n  ggplot(aes(VL_T1, change, color = GROUP)) + geom_point(size = 2.5) +\n      scale_color_manual(values = col_scale) +\n  geom_smooth(aes(color = NULL), method = \"lm\", se = FALSE, color = \"black\") +\n\n  labs(title = \"Relationship between pre- and change scores of VL thickness\", \n       subtitle = \"The line indicates the linear relationship between baseline and change\",\n       x = \"Pre-training VL thickness (cm)\", \n       y = \"Pre- to Post-training VL thickness (cm change)\")\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\nWarning: Removed 1 rows containing non-finite values (stat_smooth).\n\n\nWarning: Removed 1 rows containing missing values (geom_point).\n\n\n\n\n\n\nThe regression line indicates that there is a negative relationship between baseline and change score values. This is something we should consider when comparing groups. Again, an ANCOVA model will make a correction for this relationship. Any imbalance between groups at baseline will be accounted for by modelling the relationship between baseline and change.\nWe will fit the model:\n\n\n\nCode\n#| echo: true\n#| message: false\n#| warning: false\n\ndat2 <- hypertrophy %>%\n  select(PARTICIPANT, GROUP, VL_T1, VL_T3) %>%\n  mutate(change = VL_T3 - VL_T1)\n\nm2 <- lm(change ~ VL_T1 + GROUP, data = dat2)\n\n\npar(mfrow = c(2, 2))\nplot(m2)\n\n\n\n\n\nCode\npar(mfrow = c(1, 1))\n\n\n\nInspecting the diagnostics plots gives us no reason for big concerns regarding the model fit (except maybe for the Q-Q plot). We may will proceed with the model summary. Like before we will plot the model estimates over the raw data.\n\n\n\nCode\n#| echo: true\n#| message: false\n#| warning: false\n\n\nhypertrophy %>%\n  select(PARTICIPANT, GROUP, VL_T1, VL_T3) %>%\n  mutate(change = VL_T3 - VL_T1) %>%\n  ggplot(aes(VL_T1, change, color = GROUP)) + geom_point(size = 2.5) +\n  \n    scale_color_manual(values = col_scale) +\n\n  geom_abline(intercept = coef(m2)[1], slope = coef(m2)[2], color = col_scale[1], size = 1.5) + \n   geom_abline(intercept = coef(m2)[1] + coef(m2)[3] , slope = coef(m2)[2], color = col_scale[2], size = 1.5) + \n   geom_abline(intercept = coef(m2)[1] + coef(m2)[4], slope = coef(m2)[2], color = col_scale[3], size = 1.5) + \n\n  labs(title = \"Relationship between pre- and change scores of VL thickness\", \n       subtitle = \"The line indicates the linear relationship between baseline and change\",\n       x = \"Pre-training VL thickness (cm)\", \n       y = \"Pre- to Post-training VL thickness (cm change)\")\n\n\nWarning: Removed 1 rows containing missing values (geom_point).\n\n\n\n\n\n\nSimilarly to the model of raw post-intervention values, the vertical difference between groups are similar. Making tables of the two model indicates that the estimated differences between groups are the same irrespective if we modeled change scores or raw data.\n\n\n\nCode\n#| echo: true\n#| message: false\n#| warning: false\n\nlibrary(knitr); library(kableExtra)\n\n\n\nAttaching package: 'kableExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\n\nCode\nbind_rows(broom::tidy(m1) %>%\n  mutate(model = \"Raw scores\"), \n  broom::tidy(m2) %>%\n  mutate(model = \"Change-scores\")) %>%\n  select(model, term:p.value) %>%\n  kable(col.names = c(\"Model\", \"Coefficient\", \"Estimate\", \"SE\", \"t-value\", \"p-value\"), \n        digits = 3) %>%\n        kableExtra::collapse_rows() %>%\n        kable_styling()\n\n\n\n\n \n  \n    Model \n    Coefficient \n    Estimate \n    SE \n    t-value \n    p-value \n  \n \n\n  \n    Raw scores \n    (Intercept) \n    0.850 \n    0.276 \n    3.077 \n    0.005 \n  \n  \n    Raw scores \n    VL_T1 \n    0.729 \n    0.087 \n    8.396 \n    0.000 \n  \n  \n    Raw scores \n    GROUPMALTO \n    0.023 \n    0.107 \n    0.216 \n    0.830 \n  \n  \n    Raw scores \n    GROUPWP \n    0.098 \n    0.110 \n    0.892 \n    0.380 \n  \n  \n    Change-scores \n    (Intercept) \n    0.850 \n    0.276 \n    3.077 \n    0.005 \n  \n  \n    Change-scores \n    VL_T1 \n    -0.271 \n    0.087 \n    -3.115 \n    0.004 \n  \n  \n    Change-scores \n    GROUPMALTO \n    0.023 \n    0.107 \n    0.216 \n    0.830 \n  \n  \n    Change-scores \n    GROUPWP \n    0.098 \n    0.110 \n    0.892 \n    0.380 \n  \n\n\n\n\n\nWhat model do you think is easiest to explain?"
  },
  {
    "objectID": "ws16-analyzing-trials.html#reproducing-the-results-from-haun2018",
    "href": "ws16-analyzing-trials.html#reproducing-the-results-from-haun2018",
    "title": "Analyzing trials",
    "section": "Reproducing the results from Haun et al. (2018)",
    "text": "Reproducing the results from Haun et al. (2018)\n\nHaun et al. (2018) used three time-points (T1, T2 and T3), for the three groups. In Figure 5 results from an ANCOVA on change scores is presented.\nLet’s see if we can replicate the results. We will start by recreating the lower panel of figure 5B. It turns out that the summary values from each of the group/time-point combinations correspond when complete cases are used.\n\n\n\nCode\nlibrary(ggtext) # used to get the plotting more accurate\n\nhypertrophy %>%\n  select(PARTICIPANT, GROUP, VL_T1, VL_T2, VL_T3) %>%\n  filter(complete.cases(.)) %>%\n  pivot_longer(names_to = \"time\", values_to = \"thickness\", \n               cols = VL_T1:VL_T3) %>%\n  group_by(time, GROUP) %>%\n  summarise(m = mean(thickness, na.rm = TRUE), \n            s = sd(thickness, na.rm = TRUE), \n            n = n()) %>%\n  mutate(sum_stat = paste0(round(m,2), \"<br>(\", round(s, 2), \")<br>n = \", n), \n         GROUP = factor(GROUP, levels = c(\"MALTO\", \"WP\", \"GWP\"))) %>%\n  ggplot(aes(time, m)) + \n  geom_errorbar(aes(ymin = m, ymax = m + s)) +\n  geom_bar(stat = \"identity\") +\n  \n  geom_richtext(aes(label = sum_stat), nudge_y = 1.2) +\n  \n  facet_wrap(~ GROUP, ncol = 3) + \n  scale_y_continuous(limits = c(0, 5))\n\n\n\n\n\n\nThe results of the ANCOVA model in Figure 5B indicates that both mid (T2) and (T3) values are used in the same model. The raw change score data is plotted in the upper panel of Figure 5B. We recreate the plot below.\n\n\n\nCode\ndat3 <- hypertrophy %>%\n  select(PARTICIPANT, GROUP, VL_T1, VL_T2, VL_T3) %>%\n  filter(complete.cases(.)) %>%\n  mutate( VL_T2 = VL_T2, \n         VL_T3 = VL_T3,\n      \n         T2 = VL_T2 - VL_T1, \n         T3 = VL_T3 - VL_T1, \n         VL_T1 = VL_T1 - mean(VL_T1, na.rm = TRUE),\n          GROUP = factor(GROUP, levels = c(\"MALTO\", \"WP\", \"GWP\"))) %>%\n  select(PARTICIPANT:VL_T1,  T2, T3) %>%\n  pivot_longer(names_to = \"time\", values_to = \"change\", cols = T2:T3) \n\n\ndat3 %>%\n  group_by(time, GROUP) %>%\n  summarise(m = mean(change)) %>%\n  ggplot(aes(time, m)) + geom_bar(stat = \"identity\") + \n  facet_wrap(~ GROUP, ncol = 3) + scale_y_continuous(limits = c(-0.5, 0.5)) + \n  geom_richtext(aes(label = round(m, 2)))\n\n\n\n\n\n\nThe model may be fitted using the lmer function from lme4. We do this since the data are correlated (as long as we use more than a single data point from each participant.)\n\n\n\nCode\nlibrary(lmerTest); library(emmeans) \n\n# fit the model\nm3 <- lmer(change ~  VL_T1 + time + time:GROUP + (1|PARTICIPANT), data = dat3)\n\nanova(m3)\n\n\nType III Analysis of Variance Table with Satterthwaite's method\n            Sum Sq  Mean Sq NumDF DenDF F value   Pr(>F)   \nVL_T1      0.14110 0.141101     1 26.00  9.0795 0.005699 **\ntime       0.06962 0.069620     1 27.00  4.4799 0.043657 * \ntime:GROUP 0.03246 0.008115     4 26.49  0.5222 0.720223   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\nsummary(m3)\n\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: change ~ VL_T1 + time + time:GROUP + (1 | PARTICIPANT)\n   Data: dat3\n\nREML criterion at convergence: -8.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.7778 -0.4112  0.0134  0.4648  1.4522 \n\nRandom effects:\n Groups      Name        Variance Std.Dev.\n PARTICIPANT (Intercept) 0.03684  0.1919  \n Residual                0.01554  0.1247  \nNumber of obs: 60, groups:  PARTICIPANT, 30\n\nFixed effects:\n                 Estimate Std. Error        df t value Pr(>|t|)   \n(Intercept)     -0.083687   0.073373 34.564048  -1.141   0.2619   \nVL_T1           -0.228386   0.075794 26.000000  -3.013   0.0057 **\ntimeT3           0.118000   0.055750 27.000000   2.117   0.0437 * \ntimeT2:GROUPWP   0.004359   0.108030 34.315981   0.040   0.9680   \ntimeT3:GROUPWP   0.088581   0.108030 34.315981   0.820   0.4179   \ntimeT2:GROUPGWP -0.002603   0.100797 34.674312  -0.026   0.9795   \ntimeT3:GROUPGWP -0.016057   0.100797 34.674312  -0.159   0.8744   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) VL_T1  timeT3 tT2:GROUPW tT3:GROUPW tT2:GROUPG\nVL_T1       -0.164                                               \ntimeT3      -0.380  0.000                                        \ntT2:GROUPWP -0.698  0.229  0.258                                 \ntT3:GROUPWP -0.502  0.229 -0.258  0.719                          \ntT2:GROUPGW -0.729  0.125  0.277  0.510      0.367               \ntT3:GROUPGW -0.519  0.125 -0.277  0.367      0.510      0.708    \n\n\nCode\n# calculate estimated marginal means (corrected means)\nemmeans(m3, specs = ~ GROUP|time) %>%\n  data.frame() %>%\n  ggplot(aes(time, emmean)) + geom_bar(stat = \"identity\") + \n  facet_wrap(~ GROUP, ncol = 3) + scale_y_continuous(limits = c(-0.5, 0.5)) + \n  geom_richtext(aes(label = round(emmean, 2)))\n\n\n\n\n\n\nThe ANOVA table do not correspond to the published values:\n\n\n\nCode\nlibrary(knitr)\n\n# get the global p-values\ndata.frame(anova(m3)) %>%\n  mutate(term = row.names(.)) %>%\n  select(term, Pr..F.) %>%\n  kable(col.names = c(\"Term\", \"P-value\"))\n\n\n\n\n \n  \n      \n    Term \n    P-value \n  \n \n\n  \n    VL_T1 \n    VL_T1 \n    0.0056993 \n  \n  \n    time \n    time \n    0.0436569 \n  \n  \n    time:GROUP \n    time:GROUP \n    0.7202234"
  },
  {
    "objectID": "ws16-analyzing-trials.html#an-alternative-model---the-mixed-effects-model",
    "href": "ws16-analyzing-trials.html#an-alternative-model---the-mixed-effects-model",
    "title": "Analyzing trials",
    "section": "An alternative model - the mixed effects model",
    "text": "An alternative model - the mixed effects model\n\n\nCode\ndat4 <- hypertrophy %>%\n  select(PARTICIPANT, GROUP, VL_T1, VL_T2, VL_T3) %>%\n\n  pivot_longer(names_to = \"time\", values_to = \"thickness\", cols = VL_T1:VL_T3) %>%\n  print()\n\n\n# A tibble: 93 x 4\n   PARTICIPANT GROUP time  thickness\n   <chr>       <chr> <chr>     <dbl>\n 1 MRV001      WP    VL_T1     NA   \n 2 MRV001      WP    VL_T2      2.99\n 3 MRV001      WP    VL_T3      3.45\n 4 MRV002      WP    VL_T1      2.93\n 5 MRV002      WP    VL_T2      2.7 \n 6 MRV002      WP    VL_T3      2.9 \n 7 MRV003      GWP   VL_T1      3.34\n 8 MRV003      GWP   VL_T2      3.34\n 9 MRV003      GWP   VL_T3      3.55\n10 MRV004      GWP   VL_T1      2.63\n# ... with 83 more rows\n\n\nCode\nm4 <- lmer(thickness ~ time * GROUP + (1|PARTICIPANT), data = dat4)\n\npairs(emmeans(m4, specs = ~ time), reverse = TRUE)\n\n\n contrast      estimate     SE   df t.ratio p.value\n VL_T2 - VL_T1  -0.0857 0.0425 55.1  -2.017  0.1176\n VL_T3 - VL_T1   0.0645 0.0425 55.1   1.519  0.2900\n VL_T3 - VL_T2   0.1502 0.0419 55.0   3.586  0.0020\n\nResults are averaged over the levels of: GROUP \nDegrees-of-freedom method: kenward-roger \nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nCode\nplot(m4)\n\n\n\n\n\nCode\nsummary(m4)\n\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: thickness ~ time * GROUP + (1 | PARTICIPANT)\n   Data: dat4\n\nREML criterion at convergence: 46\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.08725 -0.49343  0.05641  0.52872  2.69642 \n\nRandom effects:\n Groups      Name        Variance Std.Dev.\n PARTICIPANT (Intercept) 0.21083  0.4592  \n Residual                0.02714  0.1647  \nNumber of obs: 92, groups:  PARTICIPANT, 31\n\nFixed effects:\n                     Estimate Std. Error       df t value Pr(>|t|)    \n(Intercept)           3.06636    0.14708 32.67109  20.848   <2e-16 ***\ntimeVL_T2            -0.08455    0.07024 55.02973  -1.204    0.234    \ntimeVL_T3             0.02000    0.07024 55.02973   0.285    0.777    \nGROUPMALTO            0.16664    0.21314 32.67109   0.782    0.440    \nGROUPWP              -0.13590    0.21418 33.26683  -0.634    0.530    \ntimeVL_T2:GROUPMALTO -0.03545    0.10179 55.02973  -0.348    0.729    \ntimeVL_T3:GROUPMALTO -0.02200    0.10179 55.02973  -0.216    0.830    \ntimeVL_T2:GROUPWP     0.03208    0.10394 55.12072   0.309    0.759    \ntimeVL_T3:GROUPWP     0.15553    0.10394 55.12072   1.496    0.140    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n              (Intr) tmVL_T2 tmVL_T3 GROUPM GROUPW tVL_T2:GROUPM tVL_T3:GROUPM\ntimeVL_T2     -0.239                                                          \ntimeVL_T3     -0.239  0.500                                                   \nGROUPMALTO    -0.690  0.165   0.165                                           \nGROUPWP       -0.687  0.164   0.164   0.474                                   \ntVL_T2:GROUPM  0.165 -0.690  -0.345  -0.239 -0.113                            \ntVL_T3:GROUPM  0.165 -0.345  -0.690  -0.239 -0.113  0.500                     \ntVL_T2:GROUPW  0.161 -0.676  -0.338  -0.111 -0.253  0.466         0.233       \ntVL_T3:GROUPW  0.161 -0.338  -0.676  -0.111 -0.253  0.233         0.466       \n              tVL_T2:GROUPW\ntimeVL_T2                  \ntimeVL_T3                  \nGROUPMALTO                 \nGROUPWP                    \ntVL_T2:GROUPM              \ntVL_T3:GROUPM              \ntVL_T2:GROUPW              \ntVL_T3:GROUPW  0.521"
  },
  {
    "objectID": "ws17-mixed-effects-repeated-measures.html",
    "href": "ws17-mixed-effects-repeated-measures.html",
    "title": "Analyzing trials using mixed effects models",
    "section": "",
    "text": "When we have more observations from each experimental unit (participant, family, classroom etc.) we have correlated data. This needs to be accounted for in the statistica model. In the dxadata and strengthvolume data sets multiple measurements are collected from each participants.\nPrepare the data:\n\n\nCode\nlibrary(tidyverse); library(exscidata)\n\n\nWarning: package 'tidyverse' was built under R version 4.1.3\n\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.2 --\nv ggplot2 3.3.6     v purrr   0.3.4\nv tibble  3.1.7     v dplyr   1.0.9\nv tidyr   1.1.4     v stringr 1.4.0\nv readr   2.1.2     v forcats 0.5.1\n\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\n\nWarning: package 'tibble' was built under R version 4.1.3\n\n\nWarning: package 'readr' was built under R version 4.1.3\n\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\n\nCode\ndata(\"dxadata\")\n\n\ndat <- dxadata %>%\n  select(participant, time,sex, multiple, single, include,\n         lean.left_leg, lean.right_leg) %>%\n  pivot_longer(names_to = \"leg\", \n               values_to = \"lean.mass\", \n               cols = lean.left_leg:lean.right_leg) %>% \n  mutate(leg = if_else(leg == \"lean.left_leg\", \"L\", \"R\"), \n         sets = if_else(multiple == leg, \"multiple\", \"single\")) %>%\n  filter(include == \"incl\") %>%\n  \n  select(participant, time, sex, sets, leg, lean.mass) %>%\n  \n  pivot_wider(names_from = time, \n              values_from = lean.mass)  %>%\n  mutate(lbm.change = post - pre, \n         pre.mc = pre - mean(pre)) %>%\n \n  print()\n\n\n# A tibble: 68 x 8\n   participant sex    sets     leg     pre  post lbm.change  pre.mc\n   <chr>       <chr>  <chr>    <chr> <dbl> <dbl>      <dbl>   <dbl>\n 1 FP28        female multiple L      7059  7273        214 -1537. \n 2 FP28        female single   R      7104  7227        123 -1492. \n 3 FP40        female single   L      7190  7192          2 -1406. \n 4 FP40        female multiple R      7506  7437        -69 -1090. \n 5 FP21        male   single   L     10281 10470        189  1685. \n 6 FP21        male   multiple R     10200 10819        619  1604. \n 7 FP34        female single   L      6014  6326        312 -2582. \n 8 FP34        female multiple R      6009  6405        396 -2587. \n 9 FP23        male   single   L      8242  8687        445  -354. \n10 FP23        male   multiple R      8685  8480       -205    88.7\n# ... with 58 more rows\n\n\nTo fit models with correlated data we need to use mixed-effects models that account for the structure of the data set. The package lme4 has such functions (lmer). The package lmerTest provides the same function but with the addition of p-values.\nUsing a ordinary linera model (lm) when we have correlation in the data will in some situations lead to false negative results.\n\n\nCode\nlibrary(lme4); library(lmerTest)\n\n\nWarning: package 'lme4' was built under R version 4.1.3\n\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nWarning: package 'lmerTest' was built under R version 4.1.3\n\n\n\nAttaching package: 'lmerTest'\n\n\nThe following object is masked from 'package:lme4':\n\n    lmer\n\n\nThe following object is masked from 'package:stats':\n\n    step\n\n\nCode\nm0 <- lm(post ~ pre +sex + sets, data = dat)\nm1 <- lmerTest::lmer(post ~ pre + sex + sets + (1|participant), data = dat)\n\n\nWarning: Some predictor variables are on very different scales: consider\nrescaling\n\n\nWarning: Some predictor variables are on very different scales: consider\nrescaling\n\n\nCode\nm2 <- lme4::lmer(post ~ pre + sex + sets + (1|participant), data = dat)\n\n\nWarning: Some predictor variables are on very different scales: consider\nrescaling\n\n\nCode\nplot(m2)\n\n\n\n\n\nCode\nsummary(m0)\n\n\n\nCall:\nlm(formula = post ~ pre + sex + sets, data = dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1317.45  -201.06     2.05   228.57   753.23 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  443.62266  290.33993   1.528    0.131    \npre            0.97022    0.03963  24.484   <2e-16 ***\nsexmale      215.96312  158.53655   1.362    0.178    \nsetssingle  -123.22677   90.93691  -1.355    0.180    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 374.9 on 64 degrees of freedom\nMultiple R-squared:  0.9688,    Adjusted R-squared:  0.9674 \nF-statistic:   663 on 3 and 64 DF,  p-value: < 2.2e-16\n\n\nCode\nsummary(m2)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: post ~ pre + sex + sets + (1 | participant)\n   Data: dat\n\nREML criterion at convergence: 951.6\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.16339 -0.55999  0.03483  0.45253  1.49108 \n\nRandom effects:\n Groups      Name        Variance Std.Dev.\n participant (Intercept) 92349    303.9   \n Residual                51744    227.5   \nNumber of obs: 68, groups:  participant, 34\n\nFixed effects:\n              Estimate Std. Error t value\n(Intercept)  604.58946  368.30907   1.642\npre            0.94743    0.05072  18.679\nsexmale      290.59159  203.81270   1.426\nsetssingle  -123.55792   55.17543  -2.239\n\nCorrelation of Fixed Effects:\n           (Intr) pre    sexmal\npre        -0.973              \nsexmale     0.705 -0.815       \nsetssingle -0.088  0.013 -0.011\nfit warnings:\nSome predictor variables are on very different scales: consider rescaling\n\n\nCode\nsummary(m1)\n\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: post ~ pre + sex + sets + (1 | participant)\n   Data: dat\n\nREML criterion at convergence: 951.6\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.16339 -0.55999  0.03483  0.45253  1.49108 \n\nRandom effects:\n Groups      Name        Variance Std.Dev.\n participant (Intercept) 92349    303.9   \n Residual                51744    227.5   \nNumber of obs: 68, groups:  participant, 34\n\nFixed effects:\n              Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)  604.58946  368.30907   32.81095   1.642   0.1102    \npre            0.94743    0.05072   32.55667  18.679   <2e-16 ***\nsexmale      290.59159  203.81270   31.82448   1.426   0.1637    \nsetssingle  -123.55792   55.17543   32.37086  -2.239   0.0321 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n           (Intr) pre    sexmal\npre        -0.973              \nsexmale     0.705 -0.815       \nsetssingle -0.088  0.013 -0.011\nfit warnings:\nSome predictor variables are on very different scales: consider rescaling\n\n\nInference from lme4 (without p-values) can be made based on confidence intervals.\n\n\nCode\nconfint(m2)\n\n\nComputing profile confidence intervals ...\n\n\n                  2.5 %      97.5 %\n.sig01       201.703720  396.941748\n.sigma       179.355748  290.662544\n(Intercept) -107.769056 1337.570567\npre            0.846308    1.045576\nsexmale     -102.219992  693.375809\nsetssingle  -233.172861  -13.806501"
  },
  {
    "objectID": "ws2-installing-r.html",
    "href": "ws2-installing-r.html",
    "title": "Quantitative Methods - Workshop, Assigments and Tutorials",
    "section": "",
    "text": "Footnotes\n\n\nA more elaborate description can be found at the CRAN FAQ↩︎"
  },
  {
    "objectID": "ws3-first-graph.html",
    "href": "ws3-first-graph.html",
    "title": "Quantitative Methods - Workshop, Assigments and Tutorials",
    "section": "",
    "text": "In this workshop, many of us will create our first real plot. For this purpose we will use the ggplot2 package. This choice of package is based on usage, many people use it and therefore you can easily find help online. ggplot2 is also integrated or highly compatible with other commonly used packages in R.\nIt is a good idea to write your code in a R script in this session. Be sure to comment your code extensively, this will help you explain to yourself what you are doing and make it easy to reuse parts of your code.\nA commented line in R code starts with #:\n\n# This is a comments\na <- c(\"roses\", \"are\", \"red\")\n# This is another comment\n\n#### Sections can be specified with several number/hash/pound signs ####\n\n# Sections in scripts and code chunks help you structure your work. \n# In R studio, sections can be located from the editor.\n\nIt is also a good idea to use the comments to write a statement about the purpose of the script or analysis your are writing. Later we will talk about keeping files in a structured way in projects.\n\n\nWe need to start by installing required packages. From the console we can type\n\ninstall.packages(\"tidyverse\")\n\nThis will install the tidyverse package, a package containing many package. On the tidyverse website you can read:\n\nThe tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures.\n\nBy using the tidyverse you will adopt a special dialect of R. A dialect that is very efficient and fairly easy to read (as a human).\nTidyverse will install ggplot2 for you. Notice that you only need to install a package once. It is therefore not a good idea to have an unconditional install.packages() in your script.\nTo get ggplot2 to start working we need to use another command:\n\nlibrary(\"ggplot2\")\n\nNotice that I’ve put ggplot2 inside citation marks \". This is optional!\nThe library function loads all function contained in the package to your R session. This means that you can access and use them.\nFor these exercises we will use another package. The exscidata package contains data sets related to exercise physiology. To install it we need a bit more code. Since exscidata is not on CRAN, but on github we can use the remotes package.\n\n# Install package from cran\nif(!(\"remotes\" %in% .packages(all.available = TRUE))) {\n        install.packages(\"remotes\")\n}\n\n\n# Using remotes, install exscidata from github\nif(!(\"exscidata\" %in% .packages(all.available = TRUE))) {\n        remotes::install_github(\"dhammarstrom/exscidata\")\n}\n\n\n\n# Load the exscidata package\nlibrary(exscidata)\n\n\n\n\n\n\n\nNote\n\n\n\nAbove is a if statement. Ordinarily, the statement can be read as: “If the condition is TRUE, then do whatever is in the brackets”. However, we also use a ! around a parentheses containing the %in% operator. The ! negates the test. If the package name is not contained in the vector of all packages created by the .packages(all.available = TRUE), then we want to install the package.\nThis is a way not having to install packages that are already installed when running your script.\n\n\nWe are now set to load data into our session.\n\n\n\nThe data set we will use in these exercises is called cyclingstudy. You can have a look at the variables by using the help command ?cyclingstudy.\nTo load data from a package we can use data(\"cyclingstudy\")\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nExplain the difference between install.packages() and library()\nWhat is the tidyverse, what will we use ggplot2 for?\nWhat does it mean when a package is not on CRAN?\nIdentify at least one numeric variable in the help pages for cyclingstudy, identify at least one categorical variable.\nWhat happens in your environment when you type data(\"cyclingstudy\")?\n\n\n\n\nA data set can be accessed in multiple ways. We may want to see the data. We can do this by typing View(cyclingstudy) in the console (notice the capital V). Or we can show a couple of rows and columns in the console by typing cyclingstudy.\n\n\n\nThe ggplot function (from the ggplot2 package) takes quite a lot of arguments. However, very few are needed to create a graph.\nThe ggplot2 system uses:\n\ndata → The dataset containing variables to plot\naesthetics → Scales where the data are mapped\ngeometries → Geometric representations of the data\nfacet → A part of the dataset\nstatistical transformations → Summaries of data\ncoordinates → The coordinate space\nthemes → Plot components not linked to data\n\nWe build a graph by mapping variables to different locations and visual characteristics of what is called geoms. This system makes it easy to build different types of graphs using similar syntax.\nWe will start by mapping to continuous variables to the coordinate system. For this exercise, use the variables weight.T1 and sj.max.\nggplot needs to know were the variables can be found, we therefore have to specify the data argument first. Next we map the variables to the x and y coordinates of the graph. You can copy the code below to your R script.\n\nggplot(data = cyclingstudy, aes(x = cmj.max, y = sj.max))\n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nExplain to your friend what we have done so far.\nWhat is mapping in this context?\nDefine “continuous variables”\n\n\n\n\n\n\n\nWe have not yet added graphical representations of the data mapped to coordinates (x and y). These can be added to the plot using the + operator, as we will do below.\nThink about a ggplot as a layered construction. Layers can be added (+) to build the graph you want. Layers are added to the graph sequentially, this means it matters in which order you add them.\nBut what to add?\nThere are many geoms, for an overview go to Help > Cheat Sheets > Data Visualization with ggplot2\n\n\n\n\n\n\nPractice\n\n\n\nTask 1: Identify a geom suitable for two continuous variables, that will show individual data points on x- and y- coordinates.\nTask 2: Call up the help page of the selected geom and find out what you need to add as arguments to the geom\n\n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nExplain what the argument inherit.aes = TRUE means.\nExplain the sentence (from the help page): “If NULL, the default, the data is inherited from the plot data as specified in the call to ggplot().\n\n\n\n\nBy adding, for example, points to the plot, we will be able to see the data. How would you add the geom that creates points to your plot?\n\n\nShow the code for a possible solution\nggplot(data = cyclingstudy, aes(x = cmj.max, y = sj.max)) + geom_point()\n\n\n\n\n\n\n\n\nGroup work\n\n\n\n\nTask 1: Using the cheat sheet, beside x and y. What other aesthetics (aes) may be added to the plot that will affect the appearance of the points?\nTask 2: Using pen and paper, draw a figure of the cyclingstudy data using one categorical variable and one continuous variable and hand the figure to the next group.\nTask 3: Code the figure! Create the figure that the other group has drafted for you.\n\n\n\n\n\n\nCharacteristics such as shapes or colors can also be added to geoms outside the aes(). This means we will override any mapping already given in aes(). As we already have seen, mappings that are inherited from the ggplot function to geoms.\n\n\n\n\n\n\n\n\nPractice\n\n\n\n\nTask 1: Change characteristics of your plot outside data mapping using color and fill, linetype, size and shape. What geoms are responsive to each change?\n\n\n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nExplain the what the code will produce, without running the code below (google “r shapes” to see what number each shape has.)\n\n\n# Example 1\nggplot(data = cyclingstudy, aes(x = cmj.max, y = sj.max, color = group)) + geom_point(color = \"blue\")\n\n# Example 2\nggplot(data = cyclingstudy, aes(x = cmj.max, y = sj.max, color = group)) + geom_point(shape = 8)\n\n# Example 3\nggplot(data = cyclingstudy, aes(x = cmj.max, y = sj.max, color = weight.T1)) + geom_point(shape = 8)\n\n# Example 4\nggplot(data = cyclingstudy, aes(x = cmj.max, y = sj.max, shape = timepoint)) + geom_point(color = \"red\")\n\n\n\n\n\n\n\n\nIn ggplot2 there are pre-set palettes for colors, orders of shapes and line types etc. Often you would want to control such settings.\nThere are many sources for informed selection of colors, one is colorbrewer2. We\nTo change color scales we use scale_*_* functions that will help you set, e.g., colors manually. In the example below we create a gradient from two colors\n\nggplot(data = cyclingstudy, \n       aes(x = cmj.max, y = sj.max, color = weight.T1)) + \n        geom_point() +\n        scale_colour_gradient(\n                low = \"#e41a1c\",  \n                high = \"#4daf4a\")\n\nDiscrete variables can also be set with colors using scale_color_manual\n\nggplot(data = cyclingstudy, \n       aes(x = cmj.max, y = sj.max, color = timepoint)) + \n        geom_point() +\n        scale_color_manual(values = c(\"#66c2a5\",\"#fc8d62\",\"#8da0cb\",\"#e78ac3\"))\n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nExplain to a friend what scales do\n\n\n\n\n\n\n\nSome aspects of a plot requires that data points are connected together. This essentially means that some a variable needs to group data points. In our example data set, subject gives the identity of each participant. We may use this information to group data points, or connect them with e.g. geom_line(). By adding group = subject to the aes() call in ggplot we will group all geoms that allow grouping.\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nBefore running the code below. Explain what you expect it will show.\n\n\n\n\n\n# Example 1\nggplot(data = cyclingstudy, aes(x = cmj.max, \n                                y = sj.max, \n                                color = group,\n                                shape = timepoint,\n                                group = subject)) + \n        geom_point() +\n        geom_line()\n\n\n\n\nA plot can be quite cluttered, and in this case, give a false impression of a lot of data. The design of this study results in a data set where each participant is tested at multiple time-points. We can therefore create facets based on some aspect of the data, such as time-point.\nThe facet_wrap() and facet_grid() creates facets.\n\n# Example 1\nggplot(data = cyclingstudy, aes(x = cmj.max, \n                                y = sj.max, \n                                color = group)) + \n        geom_point() +\n        facet_wrap(~ timepoint)\n\nAs can be seen in the code above, facet_wrap takes a one-handed formula, the ~ (tilde), indicates a formula. We can read this as “wrap by time-point”.\nIn facet_grid() we will use a two-handed formula, meaning that both sides of the tilde needs information. If we want to group only by rows, we will use a . to indicate that nothing will be used to group by column.\n\nggplot(data = cyclingstudy, aes(x = cmj.max, \n                                y = sj.max, \n                                color = group)) + \n        geom_point() +\n        facet_grid( timepoint ~ .)\n\nIn facet_grid above, we could replace the . with another variable. How would you write the code to facet the graph by group in rows and time-point in columns?\n\n\nShow the code\nggplot(data = cyclingstudy, aes(x = cmj.max, \n                                y = sj.max, \n                                color = group)) + \n        geom_point() +\n        facet_grid( group ~ timepoint)\n\n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nExplain to a friend, what do the plot produced by the code above show? Include everything that is important to reproduce the plot in your description. Try to describe it without pointing at the plot!\n\n\n\n\n\n\n\nAnnotations can be added to plots, these are often user specified, such as labels and plot titles.\nWe specify labels with the labs() function and add annotations with the annotate() function.\n\nggplot(data = cyclingstudy, aes(x = cmj.max, \n                                y = sj.max, \n                                color = group)) + \n        geom_point() +\n        labs(x = \"Maximal Counter movement jump height (cm)\", \n             y = \"Maximal squat jump height (cm)\", \n             title = \"This is the title\", \n             subtitle = \"This is the subtitle\", \n             caption = \"This is a caption\", \n             color = \"This is the group aesthetics\") +\n        \n        annotate(geom = \"text\", x = 25, y = 35, label = \"This is a text annotation\")\n\nNotice that labs()makes use of all aesthetic mappings and annotate requires a geom.\n\n\n\nThe theme() function is used for non-data layers in the plot.\n\n\n\nThere are two commonly used system for combining individual plots, patchwork, https://patchwork.data-imaginist.com/ and cowplot, https://cran.r-project.org/web/packages/cowplot/vignettes/introduction.html.\nThe idea here is create figures with multiple individual figures.\npatchwork has a very simple syntax.\n\nlibrary(patchwork)\n\n\na <- ggplot(data = cyclingstudy, aes(x = cmj.max, \n                                y = sj.max, \n                                color = group)) + \n        geom_point() \n\nb <- ggplot(data = cyclingstudy, aes(y = sj.max, \n                                x = group)) + \n        geom_boxplot() \n\n\nc <- ggplot(data = cyclingstudy, aes(x = timepoint, \n                                y = sj.max, \n                                group = subject)) + \n        geom_line() \n\n\n\n(a | b) / c\n\ncowplot uses plot_grid to arrange plots\n\nlibrary(cowplot)\n\nplot_grid(a, b, c, nrow = 2)\n\nplot_grid can also use a “nested” structure.\n\nplot_grid(plot_grid(a, b, nrow = 1), \n          c, nrow = 2)\n\nIn both frameworks, annotations can be added to plots to indicate panels/sub-plots.\n\n\n\nOutput from ggplot2 can be saved from RStudio using the export buttom. However, a more reproducible manner is to save the output using ggsave\n\n\n\n\n\n\n\n\nPractice\n\n\n\n\nTask 1: Create three separate plots from the cycling data set and save them as objects in your environment.\nTask 2: Use cowplot and patchwork to group the plots together.\nTask 3: Save the plot using ggsave, explore the help pages to find what arguments are needed!\n\n\n\n\n\n\n\n\nReproduce figure 1.3 from Spiegelhalter (2019)\n\n\n# download data\nchild_heart <- read_csv(\"https://raw.githubusercontent.com/dspiegel29/ArtofStatistics/master/01-1-2-3-child-heart-survival-times/01-1-child-heart-survival-x.csv\")\n\n\nReproduce figure 2.2 and 2.3\n\n\nbeans <- read.csv(\"https://raw.githubusercontent.com/dspiegel29/ArtofStatistics/master/02-2-3-jelly-bean-counts/02-1-bean-data-full-x.csv\", header = FALSE)"
  },
  {
    "objectID": "ws4-data-wrangling-tables.html",
    "href": "ws4-data-wrangling-tables.html",
    "title": "Quantitative Methods - Workshop, Assigments and Tutorials",
    "section": "",
    "text": "The tidyverse contains two packages with functions used to wrangle data, dplyr and tidyr. On wikipedia we can read that:\n\nData analysts typically spend the majority of their time in the process of data wrangling compared to the actual analysis of the data.\n\nIn the age of data we would be ignorant to teach data analysis without data wrangling\n\n\ndplyr contains verbs used for data manipulation, such as filtering rows, selecting variables and changing or creating variables. Verbs can be used in a pipe and read as sequential operations:\n\n> Take the data **then do** \n> filter based on group **then do**\n> create a new variable **then do**\n> show the data in the console.\n\nThe pipe operations are made possible by another package, magrittr. This package contains the forward pipe operator %>%. The forward pipe operator (%>%) can be read as “then do”. The operator takes the object on the left-hand side and puts it as the first argument in the following function.\nTranslating the “pipe” above from human language to R language using the “tidyverse dialect” looks like this:\n\ndata %>%\n        filter(group == \"xx\") %>%\n        mutate(new.var = old.var + old.var2) %>%\n        print()\n\n\n\n\nWe will use the cyclingstudy data from exscidata in our exercises. Load the required components to your session.\n\n\nShow the code\nlibrary(tidyverse) # loads dplyr etc.\nlibrary(exscidata) # loads the data-package\n\ndata(\"cyclingstudy\")\n\n\n\n\n\nMutate can help you create new variables (or overwrite existing once). In the cycling data set there is a variable called VO2.max, this variable is expressed in absolute units (ml min-1), however, we might want to express it as relative units (ml kg-1 min-1).\n\ncyclingstudy %>%\n        mutate(rel.vo2max = VO2.max / weight.T1) %>%\n        print()\n\nThe mutate function creates new variables (or overwrite existing) in a flexible way. Here we simply use division. Other mathematical operators can similarly be used (+, -, *, etc.).\nNotice the print() function in the end of the pipe. This is used to display the results of any manipulations done in the pipe. Notice also that our new variable is not listed. We might need to select a sub-set of variables to get a better overview. We will do this using the select function.\n\ncyclingstudy %>%\n        mutate(rel.vo2max = VO2.max / weight.T1) %>%\n        select(subject, group, timepoint, age, height.T1, weight.T1, VO2.max, rel.vo2max) %>%\n        print()\n\nThe select function takes variable names as “unquoted” names. We can also select a range of columns using the syntax <from>:<to> where <from> is the first column you would like to select and <to> would be the last. Subsequently the above pipe can be re-written as\n\ncyclingstudy %>%\n        mutate(rel.vo2max = VO2.max / weight.T1) %>%\n        select(subject:weight.T1, VO2.max, rel.vo2max) %>%\n        print()\n\nselect can also be used to re-name variables. Simpler variable names for weight, height and relative V̇O2max could be weight, height, vo2max.kg.\n\ncyclingstudy %>%\n        mutate(rel.vo2max = VO2.max / weight.T1) %>%\n        select(subject:age, height = height.T1, weight = weight.T1, VO2.max,vo2max.kg = rel.vo2max) %>%\n        print()\n\n\n\n\nFiltering is used to select specific observations (rows) of a data set. We filter based on specific conditions such as:\n\nAll values bigger than X\nAll values less than Y\nAll observations than contain A, B or C in variable V.\n\nThe above examples must be translated to formal expressions.\n\n\nAn expression that make comparisons can be\n\nx < y → x less than y\nx > y → x greater than y\nx <= y → x less or equal to y\nx >= y → x greater or equal to y\nx == y → x exactly equal to y\nx != y → x not exactly equal to y\n\nIn the filter function these expressions give either TRUE or FALSE. If TRUE the rows are included in the filtered data frame.\nWe can see the mechanism behind filtering by creating a vector of TRUE and FALSE based on an expression. Let’s say that we want to see which rows has weight.T1 greater than 75.\n\ncyclingstudy$weight.T1 > 75\n\n [1]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n[13]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE\n[25]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE  TRUE\n[37]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE    NA  TRUE  TRUE FALSE\n[49]    NA  TRUE FALSE    NA  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[61]  TRUE FALSE  TRUE  TRUE    NA  TRUE  TRUE FALSE  TRUE FALSE FALSE FALSE\n[73]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n\n\nWe can see that the first row returns a TRUE while the second row returns FALSE.\nUsing the filter function, we just add the expression as an argument in the function and all the rows that comes up TRUE will remain.\n\ncyclingstudy %>%\n        filter(weight.T1 > 75) %>%\n        print()\n\n# A tibble: 60 x 101\n   subject group timepoint   age height.T1 weight.T1 sj.max cmj.max lac.125\n     <dbl> <chr> <chr>     <dbl>     <dbl>     <dbl>  <dbl>   <dbl>   <dbl>\n 1       1 INCR  pre          33       183      80.3   31.0    35.0    1.5 \n 2       3 INCR  pre          39       193      98.1   26.8    28.8    1.17\n 3       4 DECR  pre          37       175      79.2   29.2    30.8    0.88\n 4       5 DECR  pre          31       176      88     31.2    25.8    1.06\n 5       6 INCR  pre          33       168      79.6   34.2    35.3    1.27\n 6       7 MIX   pre          42       180      77.6   30.1    33.0    0.85\n 7       8 MIX   pre          26       179      75.5   32.8    33.2    0.93\n 8       9 MIX   pre          41       185      82.4   22.7    22.4    1.48\n 9      10 INCR  pre          35       187      75.6   29.7    31.1    0.93\n10      13 DECR  pre          41       183      76     34.3    36      1.67\n# ... with 50 more rows, and 92 more variables: lac.175 <dbl>, lac.225 <dbl>,\n#   lac.250 <dbl>, lac.275 <dbl>, lac.300 <dbl>, lac.325 <dbl>, lac.350 <dbl>,\n#   lac.375 <dbl>, VO2.125 <dbl>, VO2.175 <dbl>, VO2.225 <dbl>, VO2.250 <dbl>,\n#   VO2.275 <dbl>, VO2.300 <dbl>, VO2.325 <dbl>, VO2.350 <dbl>, VO2.375 <dbl>,\n#   VCO2.125 <dbl>, VCO2.175 <dbl>, VCO2.225 <dbl>, VCO2.250 <dbl>,\n#   VCO2.275 <dbl>, VCO2.300 <dbl>, VCO2.325 <dbl>, VCO2.350 <dbl>,\n#   VCO2.375 <dbl>, VE.125 <dbl>, VE.175 <dbl>, VE.225 <dbl>, VE.250 <dbl>, ...\n\n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\nHow many rows in the cyclingstudy data set has\n\nVO2.max values greater than 6000\nVO2.max values less than 6000\nVO2.max values less or equal to than 5360\nVO2.max values greater or equal to than 5360\nthe value pre in timepoint\nthe a value in timepoint other than pre\n\n\n\n\n\n\n\nLogical operators similarly creates TRUE or FALSE as the basis of filtering operations. These can be used in combination with comparisons.\n\n! x → NOT x\nx & y → x and y\nx | y → x or y\nis.na(x) → returns TRUE if x is NA\n\nWe might want to keep all rows with weight.T1 greater than 80 that are also from the group INCR. This can be solved with an AND operator (&).\n\n#| echo: true\n\ncyclingstudy %>%\n        filter(weight.T1 > 80 & group == \"INCR\") %>%\n        print()\n\n# A tibble: 9 x 101\n  subject group timepoint   age height.T1 weight.T1 sj.max cmj.max lac.125\n    <dbl> <chr> <chr>     <dbl>     <dbl>     <dbl>  <dbl>   <dbl>   <dbl>\n1       1 INCR  pre          33       183      80.3   31.0    35.0    1.5 \n2       3 INCR  pre          39       193      98.1   26.8    28.8    1.17\n3      20 INCR  pre          42       180      82.3   32.5    32.2    2.26\n4       3 INCR  meso1        39        NA     101.    26.0    26.5    0.72\n5      20 INCR  meso1        42        NA      82.5   30.2    30.6    0.96\n6       3 INCR  meso2        39        NA      99.2   25.4    26.8    0.5 \n7      20 INCR  meso2        42        NA      81.1   29.6    29.8    1.53\n8       3 INCR  meso3        39        NA      99.2   27.1    27.0    0.77\n9      20 INCR  meso3        43        NA      81.5   30.0    30.9    1.77\n# ... with 92 more variables: lac.175 <dbl>, lac.225 <dbl>, lac.250 <dbl>,\n#   lac.275 <dbl>, lac.300 <dbl>, lac.325 <dbl>, lac.350 <dbl>, lac.375 <dbl>,\n#   VO2.125 <dbl>, VO2.175 <dbl>, VO2.225 <dbl>, VO2.250 <dbl>, VO2.275 <dbl>,\n#   VO2.300 <dbl>, VO2.325 <dbl>, VO2.350 <dbl>, VO2.375 <dbl>, VCO2.125 <dbl>,\n#   VCO2.175 <dbl>, VCO2.225 <dbl>, VCO2.250 <dbl>, VCO2.275 <dbl>,\n#   VCO2.300 <dbl>, VCO2.325 <dbl>, VCO2.350 <dbl>, VCO2.375 <dbl>,\n#   VE.125 <dbl>, VE.175 <dbl>, VE.225 <dbl>, VE.250 <dbl>, VE.275 <dbl>, ...\n\n\nWe can similarly use OR (|) to select either weight greater than 80 or group INCR.\n\n#| echo: true\n\ncyclingstudy %>%\n        filter(weight.T1 > 80 | group == \"INCR\") %>%\n        print()\n\n# A tibble: 46 x 101\n   subject group timepoint   age height.T1 weight.T1 sj.max cmj.max lac.125\n     <dbl> <chr> <chr>     <dbl>     <dbl>     <dbl>  <dbl>   <dbl>   <dbl>\n 1       1 INCR  pre          33       183      80.3   31.0    35.0    1.5 \n 2       3 INCR  pre          39       193      98.1   26.8    28.8    1.17\n 3       5 DECR  pre          31       176      88     31.2    25.8    1.06\n 4       6 INCR  pre          33       168      79.6   34.2    35.3    1.27\n 5       9 MIX   pre          41       185      82.4   22.7    22.4    1.48\n 6      10 INCR  pre          35       187      75.6   29.7    31.1    0.93\n 7      14 MIX   pre          35       183      81.3   27.6    30.0    1.13\n 8      15 INCR  pre          34       178      75.1   33.5    32.4    0.8 \n 9      16 INCR  pre          27       178      77.8   32.9    33.7    0.94\n10      18 DECR  pre          41       186     105.    33.5    33.7    1.75\n# ... with 36 more rows, and 92 more variables: lac.175 <dbl>, lac.225 <dbl>,\n#   lac.250 <dbl>, lac.275 <dbl>, lac.300 <dbl>, lac.325 <dbl>, lac.350 <dbl>,\n#   lac.375 <dbl>, VO2.125 <dbl>, VO2.175 <dbl>, VO2.225 <dbl>, VO2.250 <dbl>,\n#   VO2.275 <dbl>, VO2.300 <dbl>, VO2.325 <dbl>, VO2.350 <dbl>, VO2.375 <dbl>,\n#   VCO2.125 <dbl>, VCO2.175 <dbl>, VCO2.225 <dbl>, VCO2.250 <dbl>,\n#   VCO2.275 <dbl>, VCO2.300 <dbl>, VCO2.325 <dbl>, VCO2.350 <dbl>,\n#   VCO2.375 <dbl>, VE.125 <dbl>, VE.175 <dbl>, VE.225 <dbl>, VE.250 <dbl>, ...\n\n\nNotice that there are rows containing weights less than 80 from the INCR group.\nAny logical statement can also be negated with ! indication NOT. This means we will get a vector of TRUE for any expression previously being FALSE. Notice the extra parentheses below.\n\n#| echo: true\n\ncyclingstudy %>%\n        filter(!(weight.T1 > 80 & group == \"INCR\")) %>%\n        print()\n\n# A tibble: 71 x 101\n   subject group timepoint   age height.T1 weight.T1 sj.max cmj.max lac.125\n     <dbl> <chr> <chr>     <dbl>     <dbl>     <dbl>  <dbl>   <dbl>   <dbl>\n 1       2 DECR  pre          32       174      71.4   31.6    33.8    1.19\n 2       4 DECR  pre          37       175      79.2   29.2    30.8    0.88\n 3       5 DECR  pre          31       176      88     31.2    25.8    1.06\n 4       6 INCR  pre          33       168      79.6   34.2    35.3    1.27\n 5       7 MIX   pre          42       180      77.6   30.1    33.0    0.85\n 6       8 MIX   pre          26       179      75.5   32.8    33.2    0.93\n 7       9 MIX   pre          41       185      82.4   22.7    22.4    1.48\n 8      10 INCR  pre          35       187      75.6   29.7    31.1    0.93\n 9      11 MIX   pre          34       168      55.8   33.2    30.8    0.62\n10      13 DECR  pre          41       183      76     34.3    36      1.67\n# ... with 61 more rows, and 92 more variables: lac.175 <dbl>, lac.225 <dbl>,\n#   lac.250 <dbl>, lac.275 <dbl>, lac.300 <dbl>, lac.325 <dbl>, lac.350 <dbl>,\n#   lac.375 <dbl>, VO2.125 <dbl>, VO2.175 <dbl>, VO2.225 <dbl>, VO2.250 <dbl>,\n#   VO2.275 <dbl>, VO2.300 <dbl>, VO2.325 <dbl>, VO2.350 <dbl>, VO2.375 <dbl>,\n#   VCO2.125 <dbl>, VCO2.175 <dbl>, VCO2.225 <dbl>, VCO2.250 <dbl>,\n#   VCO2.275 <dbl>, VCO2.300 <dbl>, VCO2.325 <dbl>, VCO2.350 <dbl>,\n#   VCO2.375 <dbl>, VE.125 <dbl>, VE.175 <dbl>, VE.225 <dbl>, VE.250 <dbl>, ...\n\n\nThe dplyr function filter also accepts multiple arguments separated with a comma. This is equal to adding conditions with the AND operator. Example:\n\n#| echo: true\n\ncyclingstudy %>%\n        filter(weight.T1 > 80, \n               group == \"INCR\", \n               timepoint == \"pre\", \n               age > 35) %>%\n        print()\n\n# A tibble: 2 x 101\n  subject group timepoint   age height.T1 weight.T1 sj.max cmj.max lac.125\n    <dbl> <chr> <chr>     <dbl>     <dbl>     <dbl>  <dbl>   <dbl>   <dbl>\n1       3 INCR  pre          39       193      98.1   26.8    28.8    1.17\n2      20 INCR  pre          42       180      82.3   32.5    32.2    2.26\n# ... with 92 more variables: lac.175 <dbl>, lac.225 <dbl>, lac.250 <dbl>,\n#   lac.275 <dbl>, lac.300 <dbl>, lac.325 <dbl>, lac.350 <dbl>, lac.375 <dbl>,\n#   VO2.125 <dbl>, VO2.175 <dbl>, VO2.225 <dbl>, VO2.250 <dbl>, VO2.275 <dbl>,\n#   VO2.300 <dbl>, VO2.325 <dbl>, VO2.350 <dbl>, VO2.375 <dbl>, VCO2.125 <dbl>,\n#   VCO2.175 <dbl>, VCO2.225 <dbl>, VCO2.250 <dbl>, VCO2.275 <dbl>,\n#   VCO2.300 <dbl>, VCO2.325 <dbl>, VCO2.350 <dbl>, VCO2.375 <dbl>,\n#   VE.125 <dbl>, VE.175 <dbl>, VE.225 <dbl>, VE.250 <dbl>, VE.275 <dbl>, ...\n\n\nFinally, dplyr comes with two convenient functions to find values between and near\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nKeep rows in your data frame from the pre time-point, age greater than 31 but height less than 180.\nUse between to find rows with VO2.max values between 4800 and 5200 (see ?between)\nUse near to find weight.T1 values close to 80.26 with a tolerance of 0.75 (see ?near)\nRemove all rows that are NA in the height.T1 variable.\n\n\n\n\n\n\n\n\nA super power of dplyr is its ability to group and summarize data. The group_by function creates a grouped data frame suitable for summaries per group. In the cyclingstudy data set we have three groups that we might want to describe using some summary function.\nExamples of summary functions in R are:\n\nmean() → computes arithmetic mean\nmedian() → computes the median\nsd() → computes the standard deviation from the mean\nIQR() → returns the inter-quartile range\nmin() and max() → gives the minimum and maximum values from a vector\nquantile() → sample quantiles from the smallest (probs = 0) to largest (probs = 1) values.\nAll the above functions comes with the optional argument of na.rm = TRUE. This can be read as remove missing values (NA). If there are missing values (NA) and na.rm = FALSE (the default), the calculations will return NA. This is inconvenient but can often work as a sanity check of your code.\n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nWhat do we mean by sanity check?\nWhat do you expect from the R code sd(c(4, 5, 7, NA, 5))\nWhat would you add to the code above to improve it?\n\n\n\n\nIn addition to the summaries above that are generic for base R, dplyr provides you with a number of great functions to…\n\nn() → count the number of observations in each group\nn_distinct() → return the number of unique values from a vector for each group\n\nIn practice a grouped summary may look like this:\n\ncyclingstudy %>%\n        group_by(group) %>%\n        summarise(mean.vo2max = mean(VO2.max, na.rm = TRUE))\n\nResults from the above code includes multiple data points from each participant. The variable describing time-points can be added to the grouping.\n\ncyclingstudy %>%\n        group_by(group, timepoint) %>%\n        summarise(mean.vo2max = mean(VO2.max, na.rm = TRUE))\n\nMultiple summary functions can be added to the summarise() function where each adds a new variable to the result data frame.\n\ncyclingstudy %>%\n        group_by(group, timepoint) %>%\n        summarise(mean.vo2max = mean(VO2.max, na.rm = TRUE), \n                  sd.vo2max = sd(VO2.max, na.rm = TRUE))\n\n\n\n\nData not always in a form that makes tables, graphs or statistical methods directly available. Data can be described as being in long form and wide form. The long form data is tidy in the sense that all columns are distinct variables.\n\nThere are examples of wide data sets as part of the cycling data set. Using only the timepoint == pre values and columns corresponding to lactate values from the graded exercise test we have an example of wide data as the columns lac.125, lac.175, lac.225, etc., contains lactate values from different exercise intensities. This means that a separate variable (watt or exercise intensity) is combined in each column of lactate values.\n\ncyclingstudy %>%\n        filter(timepoint == \"pre\") %>%\n        dplyr::select(subject, group, lac.125:lac.375) %>%\n        print()\n\n# A tibble: 20 x 11\n   subject group lac.125 lac.175 lac.225 lac.250 lac.275 lac.300 lac.325 lac.350\n     <dbl> <chr>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1       1 INCR     1.5     1.86    2.38    3.54    6.21   NA      NA      NA   \n 2       2 DECR     1.19    1.49    2.34    3.21    5.33   NA      NA      NA   \n 3       3 INCR     1.17    1.52    1.22    1.54    2.04    3.32    4.72   NA   \n 4       4 DECR     0.88    0.99    2.13    3.25   NA       6.15   NA      NA   \n 5       5 DECR     1.06    1.41    1.9     2.04    3.04    3.59    4.73   NA   \n 6       6 INCR     1.27    1.73    3.21    4.83   NA      NA      NA      NA   \n 7       7 MIX      0.85    0.84    1.16    1.71    3.33    6.25   NA      NA   \n 8       8 MIX      0.93    1.34    1.94   NA       3.71    7.29   NA      NA   \n 9       9 MIX      1.48    1.17    1.95   NA       3.24    6.21   NA      NA   \n10      10 INCR     0.93    0.87    0.86    0.92    1.2     1.69    2.6     4.69\n11      11 MIX      0.62    1.22    2.85    5.86    9.87   NA      NA      NA   \n12      13 DECR     1.67    1.81    2.78    4.25    6.87   NA      NA      NA   \n13      14 MIX      1.13    1.33    2.74    3.97   NA      NA      NA      NA   \n14      15 INCR     0.8     1.12    1.43   NA       2.4     3.77    6.3    NA   \n15      16 INCR     0.94    1.18    1.89    2.83    5.33   NA      NA      NA   \n16      17 MIX      1.54    1.62    2.7     4.1    NA      NA      NA      NA   \n17      18 DECR     1.75    2.08    2.99    4.22   NA      NA      NA      NA   \n18      19 DECR     1.23    2.51    4.65   NA      NA      NA      NA      NA   \n19      20 INCR     2.26    2.05    3.19    5.17   NA      NA      NA      NA   \n20      21 DECR     0.68    0.89    1.98    3.18    5.57   NA      NA      NA   \n# ... with 1 more variable: lac.375 <dbl>\n\n\nUsing pivot_longer we can change this data into a long format. Pivot wider needs information on the new variable names for values and names. Names are the column names thta will form a variable and values are the values contained in the cells of the old variables. We also need to specify what columns to make longer, notice that I select variables <from>:<to>.\n\ncyclingstudy %>%\n        filter(timepoint == \"pre\") %>%\n        dplyr::select(subject, group, lac.125:lac.375) %>%\n        pivot_longer(names_to = \"watt\", \n                     values_to = \"lactate\", \n                     cols = lac.125:lac.375) %>%\n        print()\n\n# A tibble: 180 x 4\n   subject group watt    lactate\n     <dbl> <chr> <chr>     <dbl>\n 1       1 INCR  lac.125    1.5 \n 2       1 INCR  lac.175    1.86\n 3       1 INCR  lac.225    2.38\n 4       1 INCR  lac.250    3.54\n 5       1 INCR  lac.275    6.21\n 6       1 INCR  lac.300   NA   \n 7       1 INCR  lac.325   NA   \n 8       1 INCR  lac.350   NA   \n 9       1 INCR  lac.375   NA   \n10       2 DECR  lac.125    1.19\n# ... with 170 more rows\n\n\npivot_wider makes it easy to remove prefix and fix the data type of the names variable. Below we specify to remove lac. from all names and convert the new variable to numeric data.\n\ncyclingstudy %>%\n        filter(timepoint == \"pre\") %>%\n        dplyr::select(subject, group, lac.125:lac.375) %>%\n        pivot_longer(names_to = \"watt\", \n                     values_to = \"lactate\", \n                     cols = lac.125:lac.375, \n                     names_prefix = \"lac.\", \n                     names_transform = list(watt = as.numeric)) %>%\n        print()\n\n# A tibble: 180 x 4\n   subject group  watt lactate\n     <dbl> <chr> <dbl>   <dbl>\n 1       1 INCR    125    1.5 \n 2       1 INCR    175    1.86\n 3       1 INCR    225    2.38\n 4       1 INCR    250    3.54\n 5       1 INCR    275    6.21\n 6       1 INCR    300   NA   \n 7       1 INCR    325   NA   \n 8       1 INCR    350   NA   \n 9       1 INCR    375   NA   \n10       2 DECR    125    1.19\n# ... with 170 more rows\n\n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\nPerform the opposite operation of the below data set:\n\ndata.frame(id = c(\"id1\", \"id1\", \"id1\", \"id2\", \"id2\", \"id2\"), \n           NAME = c(\"a\", \"b\", \"c\", \"a\", \"b\", \"c\"), \n           NUMBER = c(4, 6, 7, 2, 3, 5)) %>%\n\n        print()\n\n\n\n\n\n\n\nTable 1 in experimental or observational studies often contains descriptive data on the sample. These tables may help readers to understand to what group/population a study may be generalized to and how key characteristics are distributed among experimental groups.\nWe will prepare data from cyclingstudy to create a Table 1 with descriptive data:\n\nSelect a set of key variables that you want to describe (center, spread and/or range) from baseline measurements\nGroup the data set on group and perform summary calculations\n\n\n\nTo display numbers with the correct number of decimals R provides many options. A simple function (round()) provides rounding. The problem is that you will lose trailing zero, e.g., 2.0 will be displayed as 2. To keep the trailing zero we must use the sprintf() function. Examples:\n\n# Rounding\nround(2.10, 2) \n\n[1] 2.1\n\n# Formatting to keep the trailing zero\nsprintf(\"%.2f\", 2.10)\n\n[1] \"2.10\"\n\n\nCombining vectors may be a good idea to make the table more attractive. The mean and standard deviation is commonly presented as mean (SD). Data from a column of means and a column of SD’s can be combined to create a nice display using the paste0() function. Example:\n\ndata.frame(m = c(46.7, 47.89, 43.5),  # A vector of means\n           s = c(4.21, 4.666, 3.1)) %>% # A vector of SD's\n        mutate(stat = paste0(round(m, 1), \n                             \" (\",\n                             round(s, 1), \n                             \")\")) %>%\n        print()\n\n      m     s       stat\n1 46.70 4.210 46.7 (4.2)\n2 47.89 4.666 47.9 (4.7)\n3 43.50 3.100 43.5 (3.1)\n\n\nA character vector of group names can be arranged and re-named using the factor function. In the cyclingstudy data set the groups (INCR, DECR and MIX) may be given more descriptive names, example:\n\ncyclingstudy %>%\n        mutate(group = factor(group, levels = c(\"INCR\", \"DECR\", \"MIX\"), \n                              labels = c(\"Increased intensity\", \n                                         \"Decreased intensity\", \n                                         \"Mixed intensity\"))) %>%\n        distinct(group)\n\n# A tibble: 3 x 1\n  group              \n  <fct>              \n1 Increased intensity\n2 Decreased intensity\n3 Mixed intensity    \n\n\n\n\n\n\nWe will use a table generator to create the table. See next part of this workshop here"
  },
  {
    "objectID": "ws4b-tables.html",
    "href": "ws4b-tables.html",
    "title": "Quantitative Methods - Workshop, Assigments and Tutorials",
    "section": "",
    "text": "Footnotes\n\n\nQuarto was released on the 28:th of July 2022↩︎\nSee the list at the bottom of the gt package webpage↩︎"
  },
  {
    "objectID": "ws5-writing-reports.html",
    "href": "ws5-writing-reports.html",
    "title": "Writing reports and working with R projects",
    "section": "",
    "text": "These workshop notes contains links to relevant documentation-"
  },
  {
    "objectID": "ws5-writing-reports.html#quarto-and-rmarkdown",
    "href": "ws5-writing-reports.html#quarto-and-rmarkdown",
    "title": "Writing reports and working with R projects",
    "section": "Quarto and Rmarkdown",
    "text": "Quarto and Rmarkdown\n\nQuarto and R Markdown a special kind of scripts where text and computer code can be combined to generate reports.\nUnder the hood, a report generator is converting code and plain text to an output format such as html, pdf or docx (more formats are available).\nQuarto is a new, well documented format that gives extra flexibility, but also requires installation of extra software.\nR Markdown is even more well documented, see e.g. R Markdown, R Markdown: The Definitev Guide"
  },
  {
    "objectID": "ws5-writing-reports.html#code-execution-and-the-environment",
    "href": "ws5-writing-reports.html#code-execution-and-the-environment",
    "title": "Writing reports and working with R projects",
    "section": "Code execution and the environment",
    "text": "Code execution and the environment\n\nWhen a quarto or Rmarkdown file is “knitted”, the source file looks for e.g., data files in the same directory as the source file is saved.\nWorking in a RStudio projects makes it easy to work with the report interactively as you can use relative paths.\nRStudio has an excellent guide to its project feature"
  },
  {
    "objectID": "ws5-writing-reports.html#rstudio-projects",
    "href": "ws5-writing-reports.html#rstudio-projects",
    "title": "Writing reports and working with R projects",
    "section": "RStudio projects",
    "text": "RStudio projects\n\nA projects is basically a collection of settings together with a root directory.\nSettings can be accessed in Tools > Project Options.\nThis means that you will be able to work with relative paths. If reading a csv file using relative paths, your code will look like this from a project.\n\n\ndat <- read_csv(\"./data/my-data.csv\")\n\n\nIf your are using absolute paths, reaching the same goal could look like this\n\n\ndat <- read_csv(\"C:/Users/Daniel/Dropbox/Some-folder/a-project/data/my-data.csv\")\n\n\nRStudio projects helps you create good habits for reproducible analysis as all analyses are conducted within a stand-alone folder structure. Your data and scripts can be shared.\nUse a basic structure for all projects:\n\nMy-project\n        |\n        |-.Rproj        (The project settings)\n        |--/data        (Contains all data needed for your analysis)\n        |--/R           (Contains all scripts/R-files)\n        |--/output      (Collection of all output files)\n\n\n\nStart a new project from the Project menu."
  },
  {
    "objectID": "ws5-writing-reports.html#writing-in-quartor-markdown",
    "href": "ws5-writing-reports.html#writing-in-quartor-markdown",
    "title": "Writing reports and working with R projects",
    "section": "Writing in Quarto/R Markdown",
    "text": "Writing in Quarto/R Markdown\n\nThe basic syntax in quarto/R Markdown files is markdown. Markdown makes it easy to format text without point-and-click as all formatting can be added with syntax, example:\n\nThis text is an example of the markdown syntax which includes **bold**, *italic*,\n^super^ and ~subscript~ and ~~striketrough~~\nResulting in:\n\nThis text is an example of the markdown syntax which includes bold, italic, super and subscript and striketrough\n\n\nIn addition to text formatting, the markdown syntax offers solutions for adding images, tables, equations, lists, diagrams, different highligt blocks. See the the quarto documentation for more information."
  },
  {
    "objectID": "ws5-writing-reports.html#code-chunks",
    "href": "ws5-writing-reports.html#code-chunks",
    "title": "Writing reports and working with R projects",
    "section": "Code chunks",
    "text": "Code chunks\n\nCode chunks are sections of your source file containing code. We primarily write R-code, but e.g., python is also possible.\nThe code chunk comes with several options specified in the top of the chunk using #|, such as:\n\n```{r}\n#| eval: true\n#| echo: true\n#| warning: false\n#| error: true\n#| include: false\n```\n\nNote that the #| is a “new” intervention, some documentation will still suggest that you use code chunk settings in the code chunk header."
  },
  {
    "objectID": "ws5-writing-reports.html#inline-code",
    "href": "ws5-writing-reports.html#inline-code",
    "title": "Writing reports and working with R projects",
    "section": "Inline code",
    "text": "Inline code\n\nCode may be included inline to include code generated outputs\n\n\na_variable <- 3.14\n\n\nTo include the variable in the text:\n\n\nThe variable will be displayed here `r a_variable`"
  },
  {
    "objectID": "ws5-writing-reports.html#bibliographies",
    "href": "ws5-writing-reports.html#bibliographies",
    "title": "Writing reports and working with R projects",
    "section": "Bibliographies",
    "text": "Bibliographies\n\nBibliographies/Citations may be added to reports using a external bibliography file. A simple format is bibtex\nPubmed entries can be searched using TexMed\nIn the visual editor, bibliographies can be easily created"
  },
  {
    "objectID": "ws5-writing-reports.html#some-notes-on-different-formats",
    "href": "ws5-writing-reports.html#some-notes-on-different-formats",
    "title": "Writing reports and working with R projects",
    "section": "Some notes on different formats",
    "text": "Some notes on different formats\n\nHTML is the basic output from R Markdown and quarto. This is suitable for first drafts.\nTo be able to render PDF files you must have an installation of rendering software. TinyTeX is generally recomended, see the documentation and installation instructions here\nWord documents creates an editable document with associated pros and cons.\nAll formats has different advantages and offer flexibility that gives opportunities to create any type of document\nOther formats such as presentations, webpages, apps etc. makes quarto / R Markdown very versatile."
  },
  {
    "objectID": "ws6-git.html",
    "href": "ws6-git.html",
    "title": "Collaborative coding with github",
    "section": "",
    "text": "Jennifer Bryan has written an excellent paper (Bryan 2017) on the use of git and GitHub when working with R.\nJennifer is also co-author on the web-book “Happy Git and GitHub for the useR” which is an excellent resource whenever you are stuck.\n\n\nGit is a version control system. It keeps track of changes made to files contained in a repository. Additionally, using GitHub users can collaborate on a repository by keeping an online version of the repository.\nThe benefits of using git comes from enabling collaboration with other and your future you (Bryan 2017)."
  },
  {
    "objectID": "ws6-git.html#different-alternatives-for-version-control",
    "href": "ws6-git.html#different-alternatives-for-version-control",
    "title": "Collaborative coding with github",
    "section": "Different alternatives for version control",
    "text": "Different alternatives for version control\n\n\n\n(Figure from Bryan 2017)"
  },
  {
    "objectID": "ws6-git.html#basic-git-and-github",
    "href": "ws6-git.html#basic-git-and-github",
    "title": "Collaborative coding with github",
    "section": "Basic git and GitHub",
    "text": "Basic git and GitHub\n\nCreate new repositories online on github, this will be an empty repository\nClone the repository in your R project (New Project > Version Control > Git)\nMake changes, add files etc.\ngit add file for a specific file or git add -A for all changed files\ngit commit -m \"message\" commit changes to the version control system with a message describing what you have done.\ngit push to the remote repository (GitHub)\nIf collaborating, a collaborator may git pull all changes to their local repository.\nCheck if you have any changes that needs to be commited or pulled by git status\n\n\nUsing forks and pull requests\n\nA fork is a copy of a repository that may evolve independently to the original repository under your own username on github.\nCreate a fork from the GitHub web interface (www.github.com).\nAfter you have made changes you may file a pull request to the original repository. You will have to describe the changes and why you think the maintainer should accept pulling your changes into the original repository.\nThis is a great way to suggest changes to complex projects.\n\n\n\nUsing branches\n\nSimilarly to forks, a branch can also be used to update a repository with changes which are then merged to the main branch after testing or review.\nSee the GitHub documentation for details."
  },
  {
    "objectID": "ws7-norwegian.html",
    "href": "ws7-norwegian.html",
    "title": "Workshop 7: Hvordan i huleste løser jeg arbeidskrav 1? En oppsummering av emnet så langt",
    "section": "",
    "text": "Hensikten med oppgaven er (1) å beskrive reliabilitet i en testmetode fra fysiologilabben, og (2) bruke verktøy for reproduserbar dataanalyse. Testet skal beskrives i detalj, inkludert gjennomføring av testen og hvordan data prosesseres fra rådata til ferdig rapport. Kravet om verktøy for reproduserbar dataanalyse innebærer at dere forventes lage en rapport som kan reproduseres på en annen PC, rapporten skal derfor være koblet til data og kod som kan gjenskape den."
  },
  {
    "objectID": "ws7-norwegian.html#steg-1-forberedelse-av-data",
    "href": "ws7-norwegian.html#steg-1-forberedelse-av-data",
    "title": "Workshop 7: Hvordan i huleste løser jeg arbeidskrav 1? En oppsummering av emnet så langt",
    "section": "Steg 1: Forberedelse av data",
    "text": "Steg 1: Forberedelse av data\nSom et første steg kan det være lurt å lage datasett som er «tidy». Dette innebærer en rad per observasjon og en kolonne per variabel. I en reliabilitetsstudie kan man f.eks. samle inn data fra et antall deltakere og to tester. Data fra de to første deltakerne kan se slik ut:\n\n\n\nparticipant\ntime\nvalue\n\n\n\n\n1\n1\n67\n\n\n2\n1\n54\n\n\n1\n2\n69\n\n\n2\n2\n53"
  },
  {
    "objectID": "ws7-norwegian.html#steg-2-sett-opp-verktøy-for-reproduserbar-dataanalyse",
    "href": "ws7-norwegian.html#steg-2-sett-opp-verktøy-for-reproduserbar-dataanalyse",
    "title": "Workshop 7: Hvordan i huleste løser jeg arbeidskrav 1? En oppsummering av emnet så langt",
    "section": "Steg 2: Sett opp verktøy for reproduserbar dataanalyse",
    "text": "Steg 2: Sett opp verktøy for reproduserbar dataanalyse\nHer kreves ekstra programvare på din PC og en brukere på www.github.com. Last ned git fra https://git-scm.com/downloads og legg til sti (file path) til git executable (git.exe) i RStudio gjennom menyene Tools > Global Options > Git/SVN > Git executable. Du finner git.exe i f.eks. C:/Program Files/Git/bin/git.exe.\nNeste steg er å skape en brukere på www.github.com.\nDen enkleste måten å starte en mappe (repository) med versjonskontroll og kobling til github er å logge på github.com, velg «New» eller gå til https://github.com/new for å lage en ny repository. Velg et godt navn (beskrivende for hva hensikten med analysen er, men kort). Trykk på «create repository», kopier adressen i det neste steget (f.eks. https://github.com/dhammarstrom/vo2max-reliability.git).\nStarte opp RStudio. Velg «New project» i Project menyen. Velg «version control» og «Git», lim inn adressen fra github.com. Velg en passende plass på din PC for å lagre prosjektmappen, trykk på «Create project». Du har nå en mappe med versjonskontroll som er koblet mot en «remote repository» på github.com."
  },
  {
    "objectID": "ws7-norwegian.html#steg-3-legg-inn-data-og-starte-opp-et-quarto-dokument",
    "href": "ws7-norwegian.html#steg-3-legg-inn-data-og-starte-opp-et-quarto-dokument",
    "title": "Workshop 7: Hvordan i huleste løser jeg arbeidskrav 1? En oppsummering av emnet så langt",
    "section": "Steg 3: Legg inn data og starte opp et quarto-dokument",
    "text": "Steg 3: Legg inn data og starte opp et quarto-dokument\nI den prosjektmappe du har på PC kan du nå legge til data. Skape en ny mappe med navnet «data» og legg in dataene du forbrett i Steg 1. Dette kan være en xlsx- eller csv-fil.\nFor å bruke quarto trenger du å installere quarto, gå til https://quarto.org/, trykk på «Get Started» og følg instruksjonene.\nEtte å ha startet RStudio på nytt bør du ha mulighet til å starte et nytt quarto-dokument fra File > New File > Quarto document.\nSpare dokumentet i din prosjektmappe. Vi er nå klare for å logge den første versjonen av prosjektet."
  },
  {
    "objectID": "ws7-norwegian.html#steg-4-git-add-commit-and-push",
    "href": "ws7-norwegian.html#steg-4-git-add-commit-and-push",
    "title": "Workshop 7: Hvordan i huleste løser jeg arbeidskrav 1? En oppsummering av emnet så langt",
    "section": "Steg 4: «Git add, commit and push»",
    "text": "Steg 4: «Git add, commit and push»\nFiler kan legges til i versjonshistorikken ved å det grafiske grensesnittet i RStudio. Trykk på Ctrl+Alt+M og du får opp et vindu med endringer. Her bør du nå se din data-fil og quarto-dokumentet som du har spart. Marker de filer du ønsker å legge til i historikken (under Staged i menyen til venstre). Skriv et «Commit message» (til høyre) og trykk på «commit». Et vindu åpnes og sier noe om statusen for din commit. For å laste opp filene till github.com så trenger du å trykke på «Push» (pilen som peker oppover lengst oppe til høyre).\nHvis du eller din kamerat har gjort forandringer i mappen på github kan du trykke på «pull» (pil som peker nedover). Du laster da ned forandringer til din lokale mappe.\nHvor ofte bør jeg legge til forandringer? Det er opp til deg, men det kan være lurt å legge til flere små forandringer for å ha mulighet til å se når noe går galt."
  },
  {
    "objectID": "ws7-norwegian.html#steg-5-beregne-reliabilitet",
    "href": "ws7-norwegian.html#steg-5-beregne-reliabilitet",
    "title": "Workshop 7: Hvordan i huleste løser jeg arbeidskrav 1? En oppsummering av emnet så langt",
    "section": "Steg 5: Beregne reliabilitet",
    "text": "Steg 5: Beregne reliabilitet\nDette steget krever noe data wrangling, noe innsikt i hva som er reliabilitet og noe innsikt i hvordan du kan presentere resultater. For bakgrunn og diskusjon se kursnotatene kapittel 9.\nHele analysen er ikke større enn at den får plass i et quarto-dokument. I mer komplekse prosjekter kreves det iblant flere filer/skript for å håndtere data og funksjoner.\nI quarto-dokumentet blir første steg å laste inn data. Du har plassert dataene i en mappen som heter «data», denne kan du nå gjennom en relativ adresse. For å laste inn en xlsx-fil trenger du pakken «readxl», for å laste inn en csv-fil vil «readr» eller funksjoner fra base R fungere. I eksemplet under laster vi inn en excelfil og lagrer den i et objekt som vi navngir «dat».\n\nlibrary(tidyverse); library(readxl)\ndat <- read_excel(\"data/vo2max-g2.xlsx\") %>%\n        select(fp, time, vo2max = `VO2_max_ml/min`) %>%\n        print()\n\n# A tibble: 22 x 3\n      fp time  vo2max\n   <dbl> <chr>  <dbl>\n 1     1 pre    4515 \n 2     1 post   4372 \n 3     2 pre    5042.\n 4     2 post   5050.\n 5     3 pre    3088 \n 6     3 post   3130.\n 7     4 pre    2828.\n 8     4 post   2897 \n 9     5 pre    4922 \n10     5 post   4696.\n# ... with 12 more rows\n\n\nVi skal beregne reliabilitet basert på differenser mellom to målinger. Hvis en test har høy reliabilitet så er differensen mellom to målinger liten. Vi ønsker bestemme den typiske differensen (eller typical error) mellom to målinger, dette definerer vi som standardavviket for differensene delt på kvadratroten av 2 (!, se Hopkins 2000).\nDen enkleste måten å regne ut differensen mellom parvise målinger er å lage datasettet til «wide» format. Dette gjør vi med pivot_wider, vi lager seden en ny variabel for å skape differensen. Standardavvik av differensen kan beregnes i summarise. Typiske feilet/differensen/error kan uttrykkes som en % av gjennomsnitt og ved hjelp av «limits of agreement» (Hopkins 2000) (Se også kursnotatene).\n\ndat %>%\n        pivot_wider(names_from = time, values_from = vo2max) %>%\n        mutate(diff = post - pre) %>%\n        summarise(s = sd(diff),  # SD av differense\n                  m = mean(c(pre, post)),  # Gjennomsnitt av alle målingene\n                  te = s / sqrt(2)) %>% # \"Typical error\" \n        print()\n\n# A tibble: 1 x 3\n      s     m    te\n  <dbl> <dbl> <dbl>\n1  230. 4023.  163."
  },
  {
    "objectID": "ws7-norwegian.html#steg-6-skriv-rapporten",
    "href": "ws7-norwegian.html#steg-6-skriv-rapporten",
    "title": "Workshop 7: Hvordan i huleste løser jeg arbeidskrav 1? En oppsummering av emnet så langt",
    "section": "Steg 6: Skriv rapporten",
    "text": "Steg 6: Skriv rapporten\nNå er vi klare for å skrive rapporten. For å vise at du kan håndtere alle delene i en kvantitativ rapport bør den inneholde:\n\nEn figur. Bruk ggplot2 for å gi en grafisk representasjon av dataene, kanskje rådata fra test 1 og 2 plotet på x og y aksel. Alternativt noe mer avansert, som en Bland-Altman plot\nEn tabell, bruk gt for å lage en tabell for å beskrive deltakerne eller resultater\nBibliography/Referanser. Bruk visual editor og sett in referanser (Insert > Citation)\nTeksten bør beskrive testen (protokoll, databearbeiding osv) og tolkning av resultatene. Her bør dere bruke relevante referanser for å sette deres resultat i kontekst.\n\nSe til at rapporten er mulig å lage til eks. html (knit/render virker). Legg alt på github og lim inn adressen i innleveringen på canvas.\nLykke till!"
  },
  {
    "objectID": "ws7-reliability.html",
    "href": "ws7-reliability.html",
    "title": "Interpreting Reliability",
    "section": "",
    "text": "The reliability of a test can provide information on what variation to expect between two repeated measurements. This information can be use to put test results into perspective.\nBelow we will use simulated data to get an idea of what the measurements mean."
  },
  {
    "objectID": "ws7-reliability.html#data-simulation",
    "href": "ws7-reliability.html#data-simulation",
    "title": "Interpreting Reliability",
    "section": "Data simulation",
    "text": "Data simulation\nA great thing about R is that you can simulate data. Simulated data take pre specified characteristics, thus adhering to assumptions.\nThe faux package can be used to simulate variables with a known correlation. We will use this functionality to simulate a reliability study on VO2max. One-hundred participants were tested three times. The average VO2max, between-participant standard deviation and correlation between measurements were approximated from the cyclingstudy dataset.\nFirst we will load required packages.\n\n\nCode\nlibrary(faux) # to simulate data\nlibrary(tidyverse) # data wrangling/plotting \n## library(Hmisc) # a function to calculate a correlation matrix\nlibrary(cowplot) # for plotting \nlibrary(ggtext) # for plotting (text formatting)\nlibrary(lme4) # to calculate the intra-class correlation\n\nset.seed(2) # Always set seed when simulating data!\n\n\nThe mean and (SD) VO2max was 4842 (532) mL min-1. The correlations between repeated tests ranged from r = 0.797 to r = 0.966. We will use a correlation of r = 0.85 for our simulations. The data is plotted below.\n\n\nCode\ndat <- rnorm_multi(n = 100, \n                   varnames = c(\"t1\", \"t2\", \"t3\", \"t4\"), \n                   mu = c(avg$m, avg$m, avg$m, avg$m), # averages from cyclingstudy dataset\n                   sd = c(avg$s, avg$s, avg$s, avg$s), # sd from cyclingstudy dataset\n                   r = 0.85) %>%\n        mutate(id = paste0(\"ID\", seq(1:100))) \n\n\ndat %>%\n        pivot_longer(names_to = \"time\", \n                     values_to = \"vo2max\", \n                     cols = t1:t4) %>%\n        ggplot(aes(time, vo2max, group = id)) + geom_line() +\n        labs(x = \"Test occasion\", \n             y = \"VO<sub>2max</sub> (mL min<sup>-1</sup>)\") + \n        theme(axis.title.y = element_markdown())\n\n\n\n\n\nSimulated VO2max data, n = 100\n\n\n\n\nWe will use test occasions 1 and 2 to calculate the reliability. Occasion 3 and 4 will be used to test if another test score difference ends up where it is supposed to!\nThe reliability can be calculated as a typical error (TE):\n\\[ TE = \\frac{SD(\\text{Difference score})}{ \\sqrt{2}}\\]\nThe TE can be expressed as a percentage of the average (coefficient of variation; CV):\n\\[\\%CV = 100 \\times \\frac{TE}{M}\\]\nIn R, these are easily calculated in a dplyr pipe.\n\n\nCode\nsum_stat <- dat %>%\n\n        mutate(diff = t2 - t1) %>%\n        summarise(s = sd(diff),               # SD of difference scores\n                  mdiff = mean(diff),         # Mean difference\n                  m = mean(c(t1, t2)),        # Mean of all measurements\n                  te = s / sqrt(2),           # Typical error\n                  cv = 100 * (te / m),        # Percentage CV\n                  L = qt(0.975, 99) * s,      # Limits of agreement 95%\n                  iisd = mean(c(sd(t1), sd(t2)))) %>%   # Mean SD between participants\n        print()\n\n\n         s     mdiff        m       te       cv        L     iisd\n1 266.9049 -2.699002 4939.503 188.7303 3.820836 529.5973 558.9533\n\n\nWe can plot differences (y-axis) against the overall average per participant (x-axis) to produce a Bland-Altman plot. We also plot the average difference between measurements, and the 95% limits of agreement.\n\n\nCode\n# A plot showing the estimated distribution of the errors (differences)\n\ndistribution <- dat %>%\n        rowwise() %>%\n        mutate(diff = t2 - t1, \n               avg = mean(c(t1, t2))) %>%\n        ggplot(aes(x = diff)) +\n        scale_x_continuous(limits = c(-1000, 1000)) +\n        stat_function(fun = dnorm, \n                      geom = \"area\",\n                      args = list(mean = sum_stat$mdiff, \n                                               sd = sum_stat$s), \n                      fill = \"steelblue\") +\n        \n        \n        theme(axis.text = element_blank(), \n              axis.title = element_blank(), \n              axis.ticks = element_blank(), \n              panel.grid = element_blank()) + \n        labs(title = \"A t-distribution\") +\n        coord_flip()\n\n\nbland_altman <- dat %>%\n        rowwise() %>%\n        mutate(diff = t2 - t1, \n               avg = mean(c(t1, t2))) %>%\n        ggplot(aes(avg, diff)) + \n        \n        geom_point() +\n        scale_y_continuous(limits = c(-1000, 1000)) +\n        \n        geom_hline(yintercept = sum_stat$mdiff) + \n        \n        geom_hline(yintercept = sum_stat$mdiff + sum_stat$L) + \n        geom_hline(yintercept = sum_stat$mdiff - sum_stat$L)  +\n        labs(x = \"Average of both measures\", \n             y = \"Difference between measurements\", \n             title = \"Bland-Altman plot\")\n        \n\nplot_grid(bland_altman, distribution, align = \"h\", nrow = 1, rel_widths = c(1, 0.3))\n\n\n\n\n\nThe distribution is included to give an idea of the assumed distribution of errors (differences) from which the limits of agreements are calculated. These limits can be used to specify a range within 95% of repeated measures will fall from another test. This assumes:\n\nSimilar variation\nSimilar correlation\nNo bias\n\nLet’s see if we can make this happen with data from test occasion 3 and 4. First we calculate how many test-retests differences are found within 95% limits of agreement. We can then display the “new” data as part of the first Bland-Altman plot.\n\n\nCode\nnew_dat <- dat %>%\n        rowwise() %>%\n        mutate(diff = t2 - t1, \n               diff2 = t4 - t3, # calculates the new test data \n               avg = mean(c(t1, t2)), \n\n               within = if_else(between(diff2, sum_stat$mdiff - sum_stat$L,\n                                        sum_stat$mdiff + sum_stat$L), \"in\", \"out\"))\n        \n\nnew_dat %>%\n        group_by(within) %>%\n        summarise(n = n())\n\n\n# A tibble: 2 x 2\n  within     n\n  <chr>  <int>\n1 in        95\n2 out        5\n\n\nCode\nnew_dat %>%     \n        \n        ggplot(aes(avg, diff)) + \n        \n        geom_point(alpha = 0.4) +\n        \n        geom_point(size = 2.5,\n                   aes(avg, diff2, \n                       shape = within, \n                       fill = within)) +\n        scale_shape_manual(values = c(21, 25)) +\n        scale_fill_manual(values = c(\"cornflowerblue\", \"magenta2\")) +\n        \n        scale_y_continuous(limits = c(-1500, 1500)) +\n        \n        geom_hline(yintercept = sum_stat$mdiff) + \n        \n        geom_hline(yintercept = sum_stat$mdiff + sum_stat$L) + \n        geom_hline(yintercept = sum_stat$mdiff - sum_stat$L)  +\n        labs(x = \"Average of both measures\", \n             y = \"Difference between measurements\")"
  },
  {
    "objectID": "ws7-reliability.html#te-as-an-intra-individual-variation",
    "href": "ws7-reliability.html#te-as-an-intra-individual-variation",
    "title": "Interpreting Reliability",
    "section": "TE as an intra-individual variation!",
    "text": "TE as an intra-individual variation!\nThe typical error (or standard error of measurement) of our simulated data is about 3.8% of the mean. We can think of this as the long-run variation in an athlete or patient. If nothing changes most repeated measures will be inside e.g., a 95% range (\\(\\pm 2 \\times SD \\approx 95\\%\\) of all values under a normal distribution).\n\n\nCode\nind_cv <- cyclingstudy %>%\n   \n        filter(subject %in% c(6, 8)) %>%\n        filter(timepoint == \"pre\") %>%\n        select(subject, timepoint, VO2.max) %>%\n        mutate(te = VO2.max * (sum_stat$cv / 100), \n               lower = VO2.max - 2 * te, \n               upper = VO2.max + 2 * te) \n\ncyclingstudy %>%\n   \n       filter(subject %in% c(6, 8)) %>%\n        select(subject, timepoint, VO2.max) %>%\n        mutate(timepoint = factor(timepoint, levels = c(\"pre\", \"meso1\", \"meso2\", \"meso3\"))) %>%\n        \n        ggplot(aes(timepoint, VO2.max, group = subject, color = as.factor(subject))) + \n        geom_point() + \n        geom_line() + \n        \n        scale_color_manual(values = c(\"coral2\", \"steelblue\")) +\n        \n        geom_hline(yintercept = ind_cv$lower, \n                   color = c(\"coral2\", \"steelblue\"), \n                   lty = c(2, 2), \n                   size = 1.5) +\n        geom_hline(yintercept = ind_cv$upper, \n                   color = c(\"coral2\", \"steelblue\"), \n                   lty = c(2, 2), \n                   size = 1.5) +\n        labs(x = \"Time-point\", \n             y = \"VO<sub>2max</sub>\", \n             color = \"Participant\") +\n        theme(axis.title.y = element_markdown())"
  },
  {
    "objectID": "ws8-linear-model.html",
    "href": "ws8-linear-model.html",
    "title": "The Linear Model",
    "section": "",
    "text": "A straight line can be described using the simple equation \\(y = a + bx\\). Here \\(a\\) is the intercept (\\(y\\) when \\(x=0\\)) and \\(b\\) is the slope (difference in \\(y\\) for every unit difference in \\(x\\)).\nThe line is heading upwards if \\(b > 0\\) and downwards if \\(b < 0\\).\n\n\n\n\n\nStraight lines following the equation \\(y = a + bx\\)"
  },
  {
    "objectID": "ws8-linear-model.html#creating-models-of-data-using-straight-lines",
    "href": "ws8-linear-model.html#creating-models-of-data-using-straight-lines",
    "title": "The Linear Model",
    "section": "Creating models of data using straight lines",
    "text": "Creating models of data using straight lines\n\nNotes on models1\n\nWe can use the straight line to create a model that describes data.\nA statistical model is an abstract representation of the underlying data that we hope captures some characteristics of the real world(!).\nThe straight line effectively avoids complexity of the real world\nStatistical models are constructions made for some purpose (e.g., prediction or explanation)\n\nWe will start our journey in statistical modelling using straight lines.\n\n\n\n\n\n\nGroup work\n\n\n\n\nTask: What line would best describe the data shown in the plots below? Copy the code in the code chunk below the figure and add straight lines that best describes the data.\n\nStraight lines can be added to a ggplot using the geom_abline() function. It takes the arguments slope and intercept.\n\n\n\n\n\n\n\n\n\nCode\na <- ggplot(data.frame(x = c(1, 2, 3), \n                  y = c(1, 2, 3)), \n       aes(x, y)) +\n        geom_point(size = 3) +\n        scale_y_continuous(limits = c(0, 5)) +\n        scale_x_continuous(limits = c(0, 5)) \n\nb <- ggplot(data.frame(x = c(1, 1, 2), \n                  y = c(1, 2, 2)), \n       aes(x, y)) +\n        geom_point(size = 3) +\n        scale_y_continuous(limits = c(0, 5)) +\n        scale_x_continuous(limits = c(0, 5)) \n        \nc <- ggplot(data.frame(x = c(1, 1, 2, 2), \n                  y = c(1, 2, 1, 2)), \n       aes(x, y)) +\n        geom_point(size = 3) +\n        scale_y_continuous(limits = c(0, 5)) +\n        scale_x_continuous(limits = c(0, 5))\n\n\nd <- ggplot(data.frame(x = c(1, 2, 3, 3, 4, 5), \n                  y = c(2, 3, 1, 3, 2, 3)), \n       aes(x, y)) +\n        geom_point(size = 3) +\n        scale_y_continuous(limits = c(0, 5)) +\n        scale_x_continuous(limits = c(0, 5))"
  },
  {
    "objectID": "ws8-linear-model.html#fitting-a-straight-line-to-data",
    "href": "ws8-linear-model.html#fitting-a-straight-line-to-data",
    "title": "The Linear Model",
    "section": "Fitting a straight line to data",
    "text": "Fitting a straight line to data\nTo achieve the goal of describing data with a model, a straight line can be fitted to data by minimizing the error in \\(y\\).\nFor every observation (\\(y_i\\)) a line produces a prediction \\(\\hat{y}_i\\). The best fitting line is the line that minimizes the sum of squared errors:\n\\[\\sum(y_i - \\hat{y}_i)^2\\]\n\n\n\n\n\nBy adding the residual error (\\(y_i - \\hat{y}_i\\)) to the equation mentioned above we can formalize the model using the following notation:\n\\[y_i = \\beta_0 + \\beta_1x_{i} + \\epsilon_i\\]\nEach observation \\(y_i\\) can be described with coefficients describing the straight line \\(\\beta_0 + \\beta_1x_i\\) and some error \\(\\epsilon_i\\).\nWhich can be translated to (Spiegelhalter 2019):\n\\[\\text{observation = deterministic model + residual error}\\]\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\nDefine these concepts:\n\nModel\nCoefficients\nIntercept\nSlope\nResiduals"
  },
  {
    "objectID": "ws8-linear-model.html#fitting-a-regression-model-in-r",
    "href": "ws8-linear-model.html#fitting-a-regression-model-in-r",
    "title": "The Linear Model",
    "section": "Fitting a regression model in R",
    "text": "Fitting a regression model in R\nWork in pairs and use the code below to fit and analyze a regression model\nWe will use the cyclingstudy data set to fit regression models. In R, a linear model can be fitted with the lm function. This function needs a formula and a data set (data frame).\nA formula is written as y ~ x, this formula can be read as “y explained by x”.\nLet’s use the pre time-point data to predict/explain VO2.max with height.T1.\n\n\nCode\nlibrary(exscidata)\n\ndata(\"cyclingstudy\")\n\n# Reduce the data set \ndat <- cyclingstudy %>%\n        filter(timepoint == \"pre\") %>%\n        select(subject, height.T1, VO2.max) \n\nmod <- lm(VO2.max ~ height.T1, data = dat)\n\n\nThe resulting model object is a list that contains a lot of information. We will put some of these components in a figure by first creating a data frame. Copy the code and run it in your own environment!\nFirst, mod$model is a data.frame of the data used to fit the model. mod$fitted.values contains the predicted values from the regression model and mod$residuals contains each residual (\\(y_i - \\hat{y}_i\\)). We can store these together in a new data frame:\n\n\nCode\nmod_dat <- data.frame(mod$model, \n           fitted = mod$fitted.values, \n           resid = mod$residuals)\n\n\nWe will start by adding the fitted values as a function of the predictor values (height.T1). Let’s make the points a bit larger and filled circles.\n\n\nCode\nmod_dat %>%\n        ggplot(aes(height.T1, fitted)) + geom_point(size = 3, shape = 21, fill = \"steelblue\")\n\n\nNext we will add the residuals as segments starting from the fitted values. geom_segment takes the aesthetics (aes) x, xend, y and yend. x and xend will be the predictor values (height.T1), y will be the fitted values and yend will be the fitted values + residuals.\n\n\nCode\nmod_dat %>%\n        ggplot(aes(height.T1, fitted)) + geom_point(size = 4, shape = 21, fill = \"steelblue\") + \n        # Add residuals\n        geom_segment(aes(x = height.T1, xend = height.T1, y = fitted, yend = fitted + resid))\n\n\nNotice that there are some overlap between individuals in height.\nNext, let’s add the observed values in a new geom_point. We make the points a bit bigger and make the filled circles.\n\n\nCode\nmod_dat %>%\n        ggplot(aes(height.T1, fitted)) + geom_point(size = 4, shape = 21, fill = \"steelblue\") + \n        # Add residuals\n        geom_segment(aes(x = height.T1, xend = height.T1, y = fitted, yend = fitted + resid)) +\n        # Add observed values\n        geom_point(aes(height.T1, VO2.max), shape = 21, fill = \"hotpink\", size = 4)\n\n\nAt last, let’s add the model prediction as an annotation. Using annotate we can specify a geom and if we chose \"segment\" it let’s us specify a start and an end on the x and y axis. We can use the function coef() to get the coefficients from the model where coef(mod)[1] is the intercept and coef(mod)[2] is the slope.\n\n\nCode\nmod_dat %>%\n        ggplot(aes(height.T1, fitted)) + geom_point(size = 4, shape = 21, fill = \"steelblue\") + \n        # Add residuals\n        geom_segment(aes(x = height.T1, xend = height.T1, y = fitted, yend = fitted + resid)) +\n        # Add observed values\n        geom_point(aes(height.T1, VO2.max), shape = 21, fill = \"hotpink\", size = 4) +\n        # Add the model\n        annotate(geom = \"segment\", \n                 x = min(mod_dat$height.T1), \n                 xend = max(mod_dat$height.T1), \n                 y = coef(mod)[1] + coef(mod)[2] * min(mod_dat$height.T1), \n                 yend = coef(mod)[1] + coef(mod)[2] * max(mod_dat$height.T1), \n                 \n                 color = \"mediumorchid1\", size = 1)\n\n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\nUse the figure and explain to a friend:\n\nWhat do the object mod$fitted.values contain?\nWhat information can we get from mod$residuals?\nExplain how the line draw between these points gives a graphical representation of the model?:\n\n\n\nCode\nx = min(mod_dat$height.T1) \n\nxend = max(mod_dat$height.T1) \n\ny = coef(mod)[1] + coef(mod)[2] * min(mod_dat$height.T1) \n\nyend = coef(mod)[1] + coef(mod)[2] * max(mod_dat$height.T1)"
  },
  {
    "objectID": "ws8-linear-model.html#predicting-from-a-regression-model",
    "href": "ws8-linear-model.html#predicting-from-a-regression-model",
    "title": "The Linear Model",
    "section": "Predicting from a regression model",
    "text": "Predicting from a regression model\nSimple predictions can be made from our model using the model coefficients. When the intercept and slope is known, we can simply plug in \\(x\\) values to get predictions.\n\\[\\hat{y} = \\beta_0 + \\beta_1x\\]\n\\[\\hat{y} = 10 + 2.2x\\]\nif \\(x=2\\), then\n\\[\\hat{y} = 10 + 2.2 \\times 2\\]\n\\[ = 14.4\\]\n\n\n\n\n\n\nGroup work\n\n\n\nUse R and solve the following problems\n\nCalculate the expected difference in VO2max between a person that has a stature of 175 and 185 cm\nWhat would be the expected VO2max of a person of height 201 cm?"
  },
  {
    "objectID": "ws9-linear-model-categorical.html",
    "href": "ws9-linear-model-categorical.html",
    "title": "The linear model: Categorical predictors and diagnostics",
    "section": "",
    "text": "References\n\nHaun, C. T., C G. Vann, C. Brooks Mobley, Shelby C. Osburn, Petey W. Mumford, Paul A. Roberson, Matthew A. Romero, et al. 2019. “Pre-Training Skeletal Muscle Fiber Size and Predominant Fiber Type Best Predict Hypertrophic Responses to 6 Weeks of Resistance Training in Previously Trained Young Men.” Journal Article. Frontiers in Physiology 10 (297). https://doi.org/10.3389/fphys.2019.00297.\n\nFootnotes\n\n\nSee Common statistical tests are linear models https://lindeloev.github.io/tests-as-linear/↩︎"
  },
  {
    "objectID": "feedback-assignments.html#innlevering-av-eksamen",
    "href": "feedback-assignments.html#innlevering-av-eksamen",
    "title": "Sjekkliste for arbeidskrav og eksamen",
    "section": "Innlevering av eksamen",
    "text": "Innlevering av eksamen\n\nEksamen skal leveres som en pdf på inspera.\nI pdf:en skal det stå beskrevet hvor man kan finne data og kod (beskriv dette under preface i mallen).\nJeg har skapt en mal for innlevering av eksamen, for å bruke denne: o Lag en fork av https://github.com/dhammarstrom/innlevering-idr4000-qmd til din egen github bruker. o Last ned din fork til R Studio og oppdatere qmd-filene med dine tekster og repository med dine data. o For å lage pdf:en, tryck på render. Det kan kreves flere render for å løse problemer som oppstår. Se til å levere den endlige versjonen! o Den pdf som skapes finner du i mappen _book o Legg til endringer (gi add -A) og commit (git commit -m ‘a message’) og push (git push). Din versjon av innleveringsmappen er nå oppdatert på din github profil\nFor å bruke denne løsningen kreves at du har quarto installert (https://quarto.org/docs/get-started/).\nDet kreves også en installasjon av TeX, quarto sier at TinyTeX er et godt alternativ (se https://quarto.org/docs/output-formats/pdf-engine.html)\nDu kan være interessert av å forandre automatisk generert tekst i rapporten, eks. figurtekster, overskrifter. Se i .yml-filen for å endre disse innstillingene."
  }
]