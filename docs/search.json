[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "assignment-1.html",
    "href": "assignment-1.html",
    "title": "Quantitative Methods - Workshop, Assigments and Tutorials",
    "section": "",
    "text": "References\n\nHalperin, I., D. B. Pyne, and D. T. Martin. 2015. “Threats to Internal Validity in Exercise Science: A Review of Overlooked Confounding Variables.” Journal Article. Int J Sports Physiol Perform 10 (7): 823–29. https://doi.org/10.1123/ijspp.2014-0566.\n\n\nHopkins, W. G. 2000. “Measures of Reliability in Sports Medicine and Science.” Journal Article. Sports Med 30 (1): 1–15. http://www.ncbi.nlm.nih.gov/pubmed/10907753.\n\n\nTanner, R. K., and C. J. Gore. 2012. Physiological Tests for Elite Athletes 2nd Edition. Book. Human Kinetics. https://books.google.no/books?id=0OPIiMks58MC."
  },
  {
    "objectID": "assignment-2.html",
    "href": "assignment-2.html",
    "title": "Quantitative Methods - Workshop, Assigments and Tutorials",
    "section": "",
    "text": "References\n\nCampbell, Michael J., Stephen John Walters, and David Machin. 2020. Medical Statistics: A Textbook for the Health Sciences. Fifth edition. Hoboken, NJ: Wiley-Blackwell.\n\n\nFrigessi, Arnoldo, and Odd O Aalen. 2018. Statistiske Metoder i Medisin Og Helsefag. Oslo: Gyldendal akademisk.\n\n\nNewell, J., D. Higgins, N. Madden, J. Cruickshank, J. Einbeck, K. McMillan, and R. McDonald. 2007. “Software for Calculating Blood Lactate Endurance Markers.” Journal Article. Journal of Sports Sciences 25 (12): 1403–9. https://doi.org/10.1080/02640410601128922.\n\n\nSpiegelhalter, D. J. 2019. The Art of Statistics : How to Learn from Data. Book. First US edition. New York: Basic Books.\n\n\nTanner, R. K., and C. J. Gore. 2012. Physiological Tests for Elite Athletes 2nd Edition. Book. Human Kinetics. https://books.google.no/books?id=0OPIiMks58MC."
  },
  {
    "objectID": "assignment-3.html",
    "href": "assignment-3.html",
    "title": "Quantitative Methods - Workshop, Assigments and Tutorials",
    "section": "",
    "text": "This assignment is set up as a statistical laboratory, we will perform simulations and your assignment is to interpret and explain the results. Create a report based on the code used in the lab and make sure you answer the specified questions (1-8). You can be as creative as you want and explore the results further.\nThe report should be handed in on canvas as a link to github repository containing a reproducible .Rmd (or qmd) file.\n\n\nIn this assignment we will simulate a population of possible values, from this population we will draw random samples, calculate statistics and interpret them. The population of values can be regarded as the possible differences between two treatments in a cross-over study where participants have performed both treatments. The values in the population are calculate as \\(Treatment - Control\\).\nWe will simulate a population of one million numbers with a mean of 1.5 and a standard deviation of 3. We will make two different set of studies, one set with a sample size of 8 and one set with a sample size of 40. In order to be sure you replicate your results, include and run set.seed() before simulations in your final script.\nWe will use the lm function to estimate the average value of the population. We do this in an “intercept-only” model. This model can be written as\n\\[Y_i = \\beta_0 + \\epsilon_i\\]\nwhere \\(\\beta_0\\) is the intercept and can be interpreted as the average value of \\(Y\\), our dependent variable. \\(\\epsilon\\) is the error term, each observation (\\(i\\)) deviates from the intercept to some degree. If the intercept term is positive or negative we can interpret it as a difference between the two treatments (described above). This model is equivalent to a one-sample t-test. Let’s get started!\nIn the code chunk below, we will simulate the population of differences between treatments. We will then draw two random samples corresponding sample sizes of 8 and 40 and save these data in data frames with the dependent variable named y. We fit the very simple model y ~ 1 as a linear model and save the model object as m1 and m2.\n\nlibrary(tidyverse)\n\nset.seed(1)\npopulation <- rnorm(1000000, mean = 1.5, sd = 3)\n\n\nsamp1 <- data.frame(y = sample(population, 8, replace = FALSE))\n\nsamp2 <- data.frame(y = sample(population, 40, replace = FALSE))\n\n\nm1 <- lm(y ~ 1, data = samp1)\nm2 <- lm(y ~ 1, data = samp2)\n\nsummary(m1)\n\n\nCall:\nlm(formula = y ~ 1, data = samp1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.5322 -1.2523 -0.0883  1.3540  4.8692 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)    1.840      1.251    1.47    0.185\n\nResidual standard error: 3.539 on 7 degrees of freedom\n\n\nThe results from a simple model can be calculated by hand. The Estimate corresponds to the average of all values in the sample, from the smaller sample, samp1 we can do mean(samp1$y). This average should correspond to coef(m1) which should be 1.84. The variation of the data is most often described with the standard deviation (SD). The SD of y in the smaller sample is sd(samp1$y) (corresponding to 3.539). However, the regression table (summary(m1)) show you the standard error (SE). This statistic is an attempt to estimate the variation in a hypothetical distribution of means. The standard error is (in this simple case) \\(SE_y = \\frac{SD_y}{\\sqrt{n}}\\). Calculating by hand using the data in samp1 we would do sd(samp1$y)/sqrt(8). Amazingly this corresponds to 1.251!\nBy using the estimate 1.84 and the corresponding SE (1.251) we can calculate the t-value as the ratio \\(\\frac{Estimate}{SE}\\). The t-value may in turn be used to determine the are under the curve of a t-distribution. The t-value from the above calculation is 1.4702611. Using our single \\(n=8\\) study, we estimate that values of t, as extreme or even more extreme as our observed value both above and below 0, would occur in 18.5% of studies if the null-hypothesis was true. This corresponds to a p-value of 0.185. The figure below shows a graphical representation of a t-value distribution under the assumption that the null-hypothesis is true.\n\n\n\n\n\nA t-distribution estimated from model m1 with the shaded area corresponding to the observed p-value.\n\n\n\n\n\nIn light of what you know now about the process of conducting a study with a random sample, use your own words and…\n\nExplain the estimate, SE, t-value, and p-value from the regression models that we created previously (m1 and m2).\nDiscuss what contributes to the different results in the two studies (m1 and m2).\nWhy do we use the shaded area in the lower and upper tail of the t-distribution (See Figure @ref(fig:t-dist-fig)).\n\n\n\n\n\nBelow we will perform 1000 studies and save the results from each study. This will make it possible for us to get an actual sampling distribution. Copy the code to your own document to run the experiment.\n\n# Create data frames to store the model estimates\nresults_8 <- data.frame(estimate = rep(NA, 1000), \n                      se = rep(NA, 1000), \n                      pval = rep(NA, 1000), \n                      n = 8)  \n\nresults_40 <- data.frame(estimate = rep(NA, 1000), \n                      se = rep(NA, 1000), \n                      pval = rep(NA, 1000), \n                      n = 40)\n\n# A for loop used to sample 1000 studies, each iteration (i) will draw a new sample\n# from the population. \n\nfor(i in 1:1000) {\n  \n  # Draw a sample \n  samp1 <- data.frame(y = sample(population, 8, replace = FALSE))\n  samp2 <- data.frame(y = sample(population, 40, replace = FALSE))\n\n  # Model the data\n  m1 <- lm(y ~ 1, data = samp1)\n  m2 <- lm(y ~ 1, data = samp2)\n  \n  # Extract values from the models\n  results_8[i, 1] <- coef(summary(m1))[1, 1]\n  results_8[i, 2] <- coef(summary(m1))[1, 2]\n  results_8[i, 3] <- coef(summary(m1))[1, 4]\n\n  results_40[i, 1] <- coef(summary(m2))[1, 1]\n  results_40[i, 2] <- coef(summary(m2))[1, 2]\n  results_40[i, 3] <- coef(summary(m2))[1, 4]\n  \n  \n}\n\n\n# Save the results in a combined data frame\n\nresults <- bind_rows(results_8, results_40)\n\n\nUsing the results data frame…\n\nCalculate the standard deviation of the estimate variable, and the average of the se variable for each of the study sample sizes (8 and 40). Explain why these numbers are very similar. How can you define the Standard Error (SE) in light of these calculations?\nCreate a histogram (see example code below) of the p-values from each study sample-size. How do you interpret these histograms, what do they tell you about the effect of sample size on statistical power?\nCalculate the number of studies from each sample size that declare a statistical significant effect (specify a threshold for \\(\\alpha\\), your significance level).\nUsing the pwr package, calculate the power of a one-sample t-test, with a effect size of 1.5/3, your specified significance level and sample sizes 8 and 40. Explain the results in the light of your simulations.\n\n\n\n# Example code for copy and paste\n\n# A two facets histogram can be created with ggplot2\nresults %>%\n  ggplot(aes(pval)) + \n  geom_histogram() +\n  facet_wrap(~ n)\n\n\n# Count the proportion of tests below a certain p-value for each \nresults %>%\n  filter(pval < 0.05) %>%\n  group_by(n) %>%\n  summarise(sig_results = n()/1000)\n\n# Using the pwr package\nlibrary(pwr)\n\npwr.t.test(n = 40, sig.level = 0.05, d = 1.5/3, type = \"one.sample\")\n\n\n\n\nWe will now simulate a population without differences between treatment and control. The code below is very similar to the one we use above, except that we use an average effect of 0 in the population.\n\npopulation <- rnorm(1000000, mean = 0, sd = 3)\n\n\n# Create data frames to store the model estimates\nresults_8 <- data.frame(estimate = rep(NA, 1000), \n                      se = rep(NA, 1000), \n                      pval = rep(NA, 1000), \n                      n = 8)  \n\nresults_40 <- data.frame(estimate = rep(NA, 1000), \n                      se = rep(NA, 1000), \n                      pval = rep(NA, 1000), \n                      n = 40)\n\n# A for loop used to sample 1000 studies, each iteration (i) will draw a new sample\n# from the population. \n\nfor(i in 1:1000) {\n  \n  # Draw a sample \n  samp1 <- data.frame(y = sample(population, 8, replace = FALSE))\n  samp2 <- data.frame(y = sample(population, 40, replace = FALSE))\n\n  # Model the data\n  m1 <- lm(y ~ 1, data = samp1)\n  m2 <- lm(y ~ 1, data = samp2)\n  \n  # Extract values from the models\n  results_8[i, 1] <- coef(summary(m1))[1, 1]\n  results_8[i, 2] <- coef(summary(m1))[1, 2]\n  results_8[i, 3] <- coef(summary(m1))[1, 4]\n\n  results_40[i, 1] <- coef(summary(m2))[1, 1]\n  results_40[i, 2] <- coef(summary(m2))[1, 2]\n  results_40[i, 3] <- coef(summary(m2))[1, 4]\n  \n  \n}\n\n\n# Save the results in a combined data frame\n\nresults_null <- bind_rows(results_8, results_40)\n\n\nUsing the new data frame with results from studies of a population with an average effect of zero, create new histograms.\n\nWith a significance level of 5%, how many studies would give you a “false positive” result if you did many repeated studies?"
  },
  {
    "objectID": "assignment-4.html",
    "href": "assignment-4.html",
    "title": "Quantitative Methods - Workshop, Assigments and Tutorials",
    "section": "",
    "text": "Footnotes\n\n\nAvoid using review articles or meta-analyses↩︎\nSee Teaching undergraduate students to read empirical articles: An evaluation and revision of the QALMRI method, this advice was also heavily influenced by this website↩︎\nHulley, S. B. (2013). Designing clinical research. Philadelphia, Wolters Kluwer/Lippincott Williams & Wilkins.↩︎"
  },
  {
    "objectID": "assignment-5.html",
    "href": "assignment-5.html",
    "title": "Quantitative Methods - Workshop, Assigments and Tutorials",
    "section": "",
    "text": "References\n\nHammarström, Daniel, Sjur Øfsteng, Lise Koll, Marita Hanestadhaugen, Ivana Hollan, William Apró, Jon Elling Whist, Eva Blomstrand, Bent R. Rønnestad, and Stian Ellefsen. 2020. “Benefits of Higher Resistance-Training Volume Are Related to Ribosome Biogenesis.” Journal Article. The Journal of Physiology 598 (3): 543–65. https://doi.org/10.1113/JP278455."
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "a Select one laboratory assignments for your portfolio exam. All groups presents one selected method on 2022-11-23. b This assignment is presented in connection with lectures."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quantitative methods and statistics (In Sport and Exercise Science)",
    "section": "",
    "text": "Welcome to the course Quantitative methods and Statistics (IDR4000). The course aims to give students an overview of methodological aspects within the field of sport and exercise-physiology. Specifically, planning, conducting and analysing research projects with human participants will be covered. The lecture notes for the course can be found here. This website will contain tutorials that we will work on in class, assignments and additional materials related to the course content."
  },
  {
    "objectID": "index.html#practical-information",
    "href": "index.html#practical-information",
    "title": "Quantitative methods and statistics (In Sport and Exercise Science)",
    "section": "Practical information",
    "text": "Practical information\nThese notes were updated on 2022-10-26 and cover the course held during 2022 autumn semester. Contact Daniel Hammarström if you have any questions regarding this content.\n\nLearning objectives\nLearning objectives can be read in Norwegian here.\n\n\nLearning strategies\nThe course will include lectures, laboratory exercises, computer exercises and workshops, seminars and student presentations. Lectures will be held in-person and as pre-recorded published on these websites.\nComputer exercises will eventually require that you have special computer software installed on your computer. The software is free (see specific chapters in lecture notes).\nAssignments will be presented on this website with information on how to hand them in. The whole course is evaluated based on a portfolio exam (see below).\n\n\nCourse evaluation\nAs a student you can contribute to the quality of the course by engaging in course evaluation throughout the course. You will be asked to answer a pre-course questionnaire about your expectations and a post-course questionnaire about your experiences. You are also welcomed to take part in systematic discussions during the course about the quality of teaching and course material. With these notes I want to underline the importance of student participation in the continuous development of the course (and program) teaching/learning quality.\n\n\nLecturers and course administration\nIn order of appearance\n\nDaniel Hammarström (daniel.hammarstrom@inn.no), is responsible for course administration and will be teaching statistics.\nTomas Urianstad will organize laboratory work in the physiology lab.\nKristian Lian will be involved in molecular methods.\nProf. Carsten Lundby will cover aspects CO2 re-breathing techniques (physiology).\nProf. Finnur Dellsén will cover philosophy of science.\nProf. Stian Ellefsen will teach molecular methods.\n\n\n\nUpdates, notifications and general communication\nThese course notes will be updated during the course. General information and last minute changes will be posted on Canvas, make sure to check it as part of your daily study routine.\n\n\nLiterature\nA full list of recommended literature can be found here. Literature will be referenced in specific sections in the lecture notes.\n\n\nGrades\nThe course is graded pass/fail.\n\n\nLanguage\nMy (Daniel) first language is Swedish, I’m sure most of you will understand what I’m talking about. However, due to the fact that we accept international students to the program, most written communication and some lectures will be in English. You are not expected to write in English, it is however possible!"
  },
  {
    "objectID": "index.html#assignments-and-portfolio-exam",
    "href": "index.html#assignments-and-portfolio-exam",
    "title": "Quantitative methods and statistics (In Sport and Exercise Science)",
    "section": "Assignments and Portfolio exam",
    "text": "Assignments and Portfolio exam\nThe course is based on several assignments. Some of these assignments are to be handed in as part of a portfolio exam upon which your grade is based.\nAssignments that are due during the course (arbeidskrav) are expected to be further improved after feedback from fellow students and teachers before inclusion in your portfolio.\nThe table below shows all assignments that are part of the course. Some are not to be included in the portfolio and some assignments are group assignments (see Table). In addition to these assignments, non-mandatory homework will be presented during the course.\n\n\n\nAssignment\nDue date\nIncluded in portfolio\nGroup assignment\n\n\n\n\nDescriptive statistics, reliability and validity, tools for reproducible data science\n2022-09-26\nYes\nYes\n\n\nRegression models and prediction from data\n2022-10-03\nNo\nYes\n\n\nExtraction and analysis of DNA\n2022-11-25\nOptionala\nYes\n\n\nExtraction of RNA and analysis of qPCR experiments\n2022-11-25\nOptionala\nYes\n\n\nExtraction and analysis of Protein\n2022-11-25\nOptionala\nYes\n\n\nPhilosophy of scienceb (See canvas)\n2022-10-28\nYes\nNo\n\n\nDrawing inference from statistical models and statistical power\n2022-11-07\nNo\nYes\n\n\nStudy designs\n2022-11-17\nYes\nNo\n\n\nAnalyzing repeated measures experiments\n2022-11-21\nYes\nNo\n\n\n\na Select one laboratory assignments for your portfolio exam. All groups presents one selected method on 2021-11-23. b This assignment is presented in connection with lectures. \nIn addition to arbeidskrav/assignments, you are required to contribute to the course wiki. The wiki page is hosted at github.com/dhammarstrom/IDR4000-2022/. In order to contribute you need to set up your own github account. The language of the wiki should be Norwegian.\nSmaller assignments and quizzes are presented as part of the course, but you are not required to do them to pass the course."
  },
  {
    "objectID": "index.html#other-information",
    "href": "index.html#other-information",
    "title": "Quantitative methods and statistics (In Sport and Exercise Science)",
    "section": "Other information",
    "text": "Other information"
  },
  {
    "objectID": "notes1-linear-models.html",
    "href": "notes1-linear-models.html",
    "title": "Notes: Univariate regression models by hand",
    "section": "",
    "text": "The univariate regression model has one dependent and one independent variable. The goal is to determine the relationship between the two variables.\nHere we will explore the mathematics of this model and replicate results from R “by hand”.\nFirst, let’s simulate some data."
  },
  {
    "objectID": "notes1-linear-models.html#a-categorical-predictor-variable",
    "href": "notes1-linear-models.html#a-categorical-predictor-variable",
    "title": "Notes: Univariate regression models by hand",
    "section": "A categorical predictor variable",
    "text": "A categorical predictor variable\n\nxcat <- ifelse(x < 7, 0, 1)\n\n\n## Calculate the correlation \nzx <- (xcat - mean(xcat)) / sd(xcat)\nzy <- (y - mean(y)) / sd(y)\n\n\nrxy <- sum(zx * zy) / (length(xcat) - 1)\n\n\n## Calculate the slope \n\nb1 <- rxy * (sd(y)/sd(xcat))\n\nb0 <- mean(y) - b1 * mean(xcat)\n\n\nb1\n\n[1] 3.891868\n\nb0\n\n[1] 22.09147\n\n## Calculate the errors\n\ne <- y - (b0 + b1*xcat)\n\n## \n\nvarx <- (sum((xcat-mean(xcat))^2))/length(xcat)\n\nseb0 <- se/sqrt(length(xcat)) * sqrt(1 + ((mean(xcat)^2) / varx))\n\nsx <- sqrt(sum((xcat - mean(xcat))^2) / length(xcat))\n\nseb1 <- se/sqrt(length(y)) * 1 /  sx\n\n\n## Calculate"
  },
  {
    "objectID": "workshops.html",
    "href": "workshops.html",
    "title": "Workshops",
    "section": "",
    "text": "Workshop\nAdditional material\n\n\n\n\nIntroduction to data science (Norwegian)\n\n\n\nInstalling and starting R\n\n\n\nCreating your first graph\n\n\n\nData wrangling and tables\n\n\n\nData wrangling and tables, part 2\n\n\n\nWriting reports\n\n\n\nCollaborative coding\n\n\n\nReliability and writing up a report (in Norwegian)\n\n\n\nDiscussions on reliability\n\n\n\nThe linear model (1): Straight lines and predicting from models\n\n\n\nThe linear model (2): Categorical predictors and diagnostics\n\n\n\nThe linear model (3): Curve-linear regression\n\n\n\nCreating functions in R"
  },
  {
    "objectID": "ws1-data-science-intro.html",
    "href": "ws1-data-science-intro.html",
    "title": "Quantitative Methods - Workshop, Assigments and Tutorials",
    "section": "",
    "text": "References\n\nBroman, Karl W., and Kara H. Woo. 2018. “Data Organization in Spreadsheets.” Journal Article. The American Statistician 72 (1): 2–10. https://doi.org/10.1080/00031305.2017.1375989."
  },
  {
    "objectID": "ws12-writing-functions.html",
    "href": "ws12-writing-functions.html",
    "title": "Writing R functions and calculate a lactate threshold",
    "section": "",
    "text": "For a comprehensive overview of writing functions in R, Chapter 19 in (Wickham and Grolemund 2017) contains an excellent introduction."
  },
  {
    "objectID": "ws12-writing-functions.html#why-write-functions",
    "href": "ws12-writing-functions.html#why-write-functions",
    "title": "Writing R functions and calculate a lactate threshold",
    "section": "Why write functions",
    "text": "Why write functions\n\nYou should invest in writing functions, if you plan to re-use code by copy paste more than once (Wickham and Grolemund 2017).\nFunctions can make your code more readable, reusable and make sure you minimize errors.\nThe DRY principle applies → Do not repeat yourself (Wickham and Grolemund 2017)."
  },
  {
    "objectID": "ws12-writing-functions.html#what-is-a-function",
    "href": "ws12-writing-functions.html#what-is-a-function",
    "title": "Writing R functions and calculate a lactate threshold",
    "section": "What is a function?",
    "text": "What is a function?\n\nR is easily “extended” through the creation of function.\nFunctions can live inside package that you or someone else has written.\nFunctions can also live in your environment after being defined in your script.\nmean(), sd() mutate()and ggplot() are examples of functions that comes with the basic installation of R or packages.\nFor a function to live in your environment, it must have a name\nThe function can come with a set of arguments\nThe actual mechanics of the function is defined in the body\n\n\n# An example function\nmy_fun  <- function(arg1, arg2) { # Name and arguments of the function\n        \n        # This is the body\n        sum <- arg1 + arg2\n        \n        return(sum)\n        \n}\n\nmy_fun(2, 4)\n\n\nNaming the function\n\nThe same rules apply to naming functions as other R objects.\n(Wickham and Grolemund 2017) recommend longer and informative/descriptive names written in “snake_case” (as opposed to “camelCase”).\nBe consistent!\nDo not use names of other function!\n\n\n\nArguments\n\nA function may be defined with named arguments, this is the “input” of the function.\nAn argument may be a data frame, a single value or vector that the function will use.\nThe function can be used with the arguments used in their place or named out of place.\n\n\n# An example function\nmy_fun  <- function(value1, value2) { # Name and arguments of the function\n        \n        # This is the body\n        diff <- value1 - value2\n        \n        return(diff)\n        \n}\n\n# these are different\nmy_fun(value2 = 1, value1 = 4)\n\nmy_fun(1, 4)\n\n\n\nBody\n\nThe function body defines the output of the function\nIt usually makes use of the data/values/variables defined in the arguments and returns a given output\nOutput can be any R object.\nThe return() function is used to explicitly make some part of the body to the output of the function\n\n\n# An example function\nmy_fun  <- function(value1, value2) { # Name and arguments of the function\n        \n        # This is the body\n        diff <- value1 - value2\n        \n        sum <- value1 + value2\n        \n        results <- list()\n        \n        results$diff <- diff\n        results$sum <- sum\n        \n        return(results)\n        \n}\n\n# these are different\nmy_fun(value2 = 1, value1 = 4)\n\nmy_fun(1, 4)\n\n\nEnvironment\n\nIf a variable is not defined in the function, R will try to look for it in the environment.\nIt is considered good practice to not rely on variables not defined in the function\n\n\n# An example function\n\n# A variable defined outside the function\nvalue1 <- 3\n\n\nmy_fun  <- function(value2) { # Name and arguments of the function\n        \n        # This is the body\n        diff <- value1 - value2\n        \n        sum <- value1 + value2\n        \n        results <- list()\n        \n        results$diff <- diff\n        results$sum <- sum\n        \n        return(results)\n        \n}\n\nmy_fun(value2 = 1)\n\nmy_fun(5)\n\n\n\n\nConditions, errors and messages\n\nFunctions can include conditional sections, using if makes it possible to make the function flexible depending on input variables\n\n\nmy_fun <- function(value1, value2, calculate.sum = FALSE) {\n        \n        if (calculate.sum == TRUE) {\n                \n                sum <- value1 + value2\n                \n                return(sum)\n                \n        } else {\n                \n                print(\"No sum calculated\")\n                \n        }\n        \n}\n\n# Calculates the sum \nmy_fun(2, 4, TRUE)\n\n# Does not calculate the sum\nmy_fun(2, 4, FALSE)\n\n\nA function can stop if the input variable is of the wrong type,\n\n\nmy_fun <- function(value1, value2) {\n        \n        if (!any(is.numeric(c(value1, value2)))) {\n                \n                stop(\"One or more values are not numeric\")\n                \n        } else {\n                \n                sum <- value1 + value2\n                \n                return(sum)\n                \n        }\n        \n}\n\n# Calculates the sum \nmy_fun(value1 = \"error?\", value2 = 4)\n\n# Does not calculate the sum\nmy_fun(value1 = 2, value2 = 4)\n\n\nThere are several additional ways to create conditional operations (see (Wickham and Grolemund 2017)).\n\n\n\n\n\n\n\nGroup work\n\n\n\n\nWrite a function that is reusable using this code:\n\n\nz <- df$var1 - mean(df$var1, na.rm = TRUE) / sd(df$var1, na.rm = TRUE)\n\n\nWrite a function that stops if the input is not a character vector\nWrite a function that calculates the standard deviation\n\n\\[s=\\sqrt{\\frac{\\sum(x_i-\\bar{x})^2}{n-1}}\\]\n\n# An example vector\nx <- c(2, 5, 7, 8)\n\n# Sum of squares\nss <- sum((x - mean(x))^2)\n\n## Calculates standard deviation\ns <- sqrt(ss / (length(x)-1))"
  },
  {
    "objectID": "ws12-writing-functions.html#a-function-to-calculate-lactate-thresholds",
    "href": "ws12-writing-functions.html#a-function-to-calculate-lactate-thresholds",
    "title": "Writing R functions and calculate a lactate threshold",
    "section": "A function to calculate lactate thresholds",
    "text": "A function to calculate lactate thresholds\n\nLactate threshold (LT) tests are commonly performed in the laboratory\nCalculations of the LT differs between labs and there are many methods\nA possible method is to calculate the power at a fixed lactate value (e.g. 4 mmoL L-1)\n\n\nlibrary(tidyverse)\nlibrary(exscidata)\n\ndata(\"cyclingstudy\")\n\n\ncyclingstudy %>%\n  # Select columns needed for analysis\n  select(subject, group, timepoint, lac.225:lac.375) %>%\n  # Only one participant and time-point\n  filter(timepoint == \"pre\", subject == 10) %>%\n  # Pivot to long format data using the lactate columns\n  pivot_longer(names_to = \"watt\", \n               values_to = \"lactate\", \n               names_prefix = \"lac.\",\n               names_transform = list(watt = as.numeric),\n               cols = lac.225:lac.375) %>%\n  # Plot the data, group = subject needed to connect the points\n  ggplot(aes(watt, lactate, group = subject))  + \n  geom_line(lty = 2) +\n  geom_point(shape = 21, fill = \"lightblue\", size = 2.5) \n\n\n\n\nA workload and lactate relationship\n\n\n\n\n\nManually, we could estimate the watt at a specific lactate using ocular inspection\n\n\ncyclingstudy %>%\n  # Select columns needed for analysis\n  select(subject, group, timepoint, lac.225:lac.375) %>%\n  # Only one participant and time-point\n  filter(timepoint == \"pre\", subject == 10) %>%\n  # Pivot to long format data using the lactate columns\n  pivot_longer(names_to = \"watt\", \n               values_to = \"lactate\", \n               names_prefix = \"lac.\",\n               names_transform = list(watt = as.numeric),\n               cols = lac.225:lac.375) %>%\n  # Plot the data, group = subject needed to connect the points\n  ggplot(aes(watt, lactate, group = subject))  + \n  geom_line(lty = 2) +\n  geom_point(shape = 21, fill = \"lightblue\", size = 2.5) +\n  # Adding straight lines at specific values\n  geom_hline(yintercept = 4, color = \"red\") +\n  geom_vline(xintercept = 341.5, color = \"blue\")\n\n\n\n\nA workload and lactate relationship, manual identification of the lactate threshold\n\n\n\n\n\nA better approximation can be derived from the curve linear model\n\n\ncyclingstudy %>%\n  # Select columns needed for analysis\n  select(subject, group, timepoint, lac.225:lac.375) %>%\n  # Only one participant and time-point\n  filter(timepoint == \"pre\", subject == 10) %>%\n  # Pivot to long format data using the lactate columns\n  pivot_longer(names_to = \"watt\", \n               values_to = \"lactate\", \n               names_prefix = \"lac.\",\n               names_transform = list(watt = as.numeric),\n               cols = lac.225:lac.375) %>%\n  # Plot the data, group = subject needed to connect the points\n  ggplot(aes(watt, lactate, group = subject))  + \n  geom_line(lty = 2) +\n  geom_point(shape = 21, fill = \"lightblue\", size = 2.5) +\n  geom_hline(yintercept = 4, color = \"red\") +\n  geom_vline(xintercept = 341.5, color = \"blue\") +\n  # Adding a straight line from a linear model\n  geom_smooth(method = \"lm\", se = FALSE, formula = y ~ x, color = \"#e41a1c\") +\n  \n  # Adding a polynomial linear model to the plot\n  \n  # poly(x, 2) add a second degree polynomial model.\n  geom_smooth(method = \"lm\", se = FALSE, formula = y ~ poly(x, 2), color = \"#377eb8\") +\n  # poly(x, 3) add a third degree polynomial model.\n  geom_smooth(method = \"lm\", se = FALSE, formula = y ~ poly(x, 3), color = \"#4daf4a\") +\n  # poly(x, 4) add a forth degree polynomial model.\n  geom_smooth(method = \"lm\", se = FALSE, formula = y ~ poly(x, 4), color = \"#ff7f00\") \n\n\n\n\nA workload and lactate relationship, adding curve linear models\n\n\n\n\n\nThese models are all “wrong but some are useful”1\n\n\nlactate <- cyclingstudy %>%\n  # Select columns needed for analysis\n  select(subject, group, timepoint, lac.225:lac.375) %>%\n  # Only one participant and time-point\n  filter(timepoint == \"pre\", subject == 10) %>%\n  # Pivot to long format data using the lactate columns\n  pivot_longer(names_to = \"watt\", \n               values_to = \"lactate\", \n               names_prefix = \"lac.\",\n               names_transform = list(watt = as.numeric),\n               cols = lac.225:lac.375) %>%\n  # Remove NA (missing) values to avoid warning/error messages.\n  filter(!is.na(lactate))\n\n# fit \"straight line\" model\nm1 <- lm(lactate ~ watt, data = lactate)\n\n# fit second degree polynomial\nm2 <- lm(lactate ~ poly(watt, 2, raw = TRUE), data = lactate)\n\n# fit third degree polynomial\nm3 <- lm(lactate ~ poly(watt, 3, raw = TRUE), data = lactate)\n\n# fit forth degree polynomial\nm4 <- lm(lactate ~ poly(watt, 4, raw = TRUE), data = lactate)\n\n# Store all residuals as new variables\nlactate$resid.m1 <- resid(m1)\nlactate$resid.m2 <- resid(m2)\nlactate$resid.m3 <- resid(m3)\nlactate$resid.m4 <- resid(m4)\n\nlactate %>%\n  # gather all the data from the models\n  pivot_longer(names_to = \"model\", \n               values_to = \"residual\", \n               names_prefix = \"resid.\", \n               names_transform = list(residual = as.numeric), \n               cols = resid.m1:resid.m4) %>%\n  # Plot values with the observed watt on x axis and residual values at the y\n  ggplot(aes(watt, residual, fill = model)) + geom_point(shape = 21, size = 3) +\n  \n  # To set the same colors/fills as above we use scale fill manual\n  scale_fill_manual(values = c(\"#e41a1c\", \"#377eb8\", \"#4daf4a\", \"#ff7f00\"))\n\n\n\n\nAssessing the fit of different linear models on a exercise intensity to lactate accumulation relationship\n\n\n\n\n\nUsing the predict() function we can predict lactate values at a specific power output.\nWe are modelling the effect of watt on lactate, so we are unable to input a specific lactate value, instead we could approximate with “inverse prediction\n\n\nndf <- data.frame(watt = seq(from = 225, to = 350, by = 0.1)) # high resolution, we can find the nearest10:th a watt\n\nndf$predictions <- predict(m3, newdata = ndf)\n\n# Which value of the predictions comes closest to our value of 4 mmol L-1?\n# abs finds the absolute value, makes all values positive, \n# predictions - 4 givs an exact prediction of 4 mmol the value zero\n# filter the row which has the prediction - 4 equal to the minimal absolut difference between prediction and 4 mmol\nlactate_threshold <- ndf %>%\n  filter(abs(predictions - 4) == min(abs(predictions - 4)))\n\n\n\n\n\n\n\nGroup work\n\n\n\n\nWrite a function that calculates the lactate threshold using a fixed lactate value.\nThe function should have arguments that defines the data and lactate value\nThe output should be a value of the approximate watt at the fixed lactate"
  },
  {
    "objectID": "ws2-installing-r.html",
    "href": "ws2-installing-r.html",
    "title": "Quantitative Methods - Workshop, Assigments and Tutorials",
    "section": "",
    "text": "Footnotes\n\n\nA more elaborate description can be found at the CRAN FAQ↩︎"
  },
  {
    "objectID": "ws3-first-graph.html",
    "href": "ws3-first-graph.html",
    "title": "Quantitative Methods - Workshop, Assigments and Tutorials",
    "section": "",
    "text": "In this workshop, many of us will create our first real plot. For this purpose we will use the ggplot2 package. This choice of package is based on usage, many people use it and therefore you can easily find help online. ggplot2 is also integrated or highly compatible with other commonly used packages in R.\nIt is a good idea to write your code in a R script in this session. Be sure to comment your code extensively, this will help you explain to yourself what you are doing and make it easy to reuse parts of your code.\nA commented line in R code starts with #:\n\n# This is a comments\na <- c(\"roses\", \"are\", \"red\")\n# This is another comment\n\n#### Sections can be specified with several number/hash/pound signs ####\n\n# Sections in scripts and code chunks help you structure your work. \n# In R studio, sections can be located from the editor.\n\nIt is also a good idea to use the comments to write a statement about the purpose of the script or analysis your are writing. Later we will talk about keeping files in a structured way in projects.\n\n\nWe need to start by installing required packages. From the console we can type\n\ninstall.packages(\"tidyverse\")\n\nThis will install the tidyverse package, a package containing many package. On the tidyverse website you can read:\n\nThe tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures.\n\nBy using the tidyverse you will adopt a special dialect of R. A dialect that is very efficient and fairly easy to read (as a human).\nTidyverse will install ggplot2 for you. Notice that you only need to install a package once. It is therefore not a good idea to have an unconditional install.packages() in your script.\nTo get ggplot2 to start working we need to use another command:\n\nlibrary(\"ggplot2\")\n\nNotice that I’ve put ggplot2 inside citation marks \". This is optional!\nThe library function loads all function contained in the package to your R session. This means that you can access and use them.\nFor these exercises we will use another package. The exscidata package contains data sets related to exercise physiology. To install it we need a bit more code. Since exscidata is not on CRAN, but on github we can use the remotes package.\n\n# Install package from cran\nif(!(\"remotes\" %in% .packages(all.available = TRUE))) {\n        install.packages(\"remotes\")\n}\n\n\n# Using remotes, install exscidata from github\nif(!(\"exscidata\" %in% .packages(all.available = TRUE))) {\n        remotes::install_github(\"dhammarstrom/exscidata\")\n}\n\n\n\n# Load the exscidata package\nlibrary(exscidata)\n\n\n\n\n\n\n\nNote\n\n\n\nAbove is a if statement. Ordinarily, the statement can be read as: “If the condition is TRUE, then do whatever is in the brackets”. However, we also use a ! around a parentheses containing the %in% operator. The ! negates the test. If the package name is not contained in the vector of all packages created by the .packages(all.available = TRUE), then we want to install the package.\nThis is a way not having to install packages that are already installed when running your script.\n\n\nWe are now set to load data into our session.\n\n\n\nThe data set we will use in these exercises is called cyclingstudy. You can have a look at the variables by using the help command ?cyclingstudy.\nTo load data from a package we can use data(\"cyclingstudy\")\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nExplain the difference between install.packages() and library()\nWhat is the tidyverse, what will we use ggplot2 for?\nWhat does it mean when a package is not on CRAN?\nIdentify at least one numeric variable in the help pages for cyclingstudy, identify at least one categorical variable.\nWhat happens in your environment when you type data(\"cyclingstudy\")?\n\n\n\n\nA data set can be accessed in multiple ways. We may want to see the data. We can do this by typing View(cyclingstudy) in the console (notice the capital V). Or we can show a couple of rows and columns in the console by typing cyclingstudy.\n\n\n\nThe ggplot function (from the ggplot2 package) takes quite a lot of arguments. However, very few are needed to create a graph.\nThe ggplot2 system uses:\n\ndata → The dataset containing variables to plot\naesthetics → Scales where the data are mapped\ngeometries → Geometric representations of the data\nfacet → A part of the dataset\nstatistical transformations → Summaries of data\ncoordinates → The coordinate space\nthemes → Plot components not linked to data\n\nWe build a graph by mapping variables to different locations and visual characteristics of what is called geoms. This system makes it easy to build different types of graphs using similar syntax.\nWe will start by mapping to continuous variables to the coordinate system. For this exercise, use the variables weight.T1 and sj.max.\nggplot needs to know were the variables can be found, we therefore have to specify the data argument first. Next we map the variables to the x and y coordinates of the graph. You can copy the code below to your R script.\n\nggplot(data = cyclingstudy, aes(x = cmj.max, y = sj.max))\n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nExplain to your friend what we have done so far.\nWhat is mapping in this context?\nDefine “continuous variables”\n\n\n\n\n\n\n\nWe have not yet added graphical representations of the data mapped to coordinates (x and y). These can be added to the plot using the + operator, as we will do below.\nThink about a ggplot as a layered construction. Layers can be added (+) to build the graph you want. Layers are added to the graph sequentially, this means it matters in which order you add them.\nBut what to add?\nThere are many geoms, for an overview go to Help > Cheat Sheets > Data Visualization with ggplot2\n\n\n\n\n\n\nPractice\n\n\n\nTask 1: Identify a geom suitable for two continuous variables, that will show individual data points on x- and y- coordinates.\nTask 2: Call up the help page of the selected geom and find out what you need to add as arguments to the geom\n\n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nExplain what the argument inherit.aes = TRUE means.\nExplain the sentence (from the help page): “If NULL, the default, the data is inherited from the plot data as specified in the call to ggplot().\n\n\n\n\nBy adding, for example, points to the plot, we will be able to see the data. How would you add the geom that creates points to your plot?\n\n\nShow the code for a possible solution\nggplot(data = cyclingstudy, aes(x = cmj.max, y = sj.max)) + geom_point()\n\n\n\n\n\n\n\n\nGroup work\n\n\n\n\nTask 1: Using the cheat sheet, beside x and y. What other aesthetics (aes) may be added to the plot that will affect the appearance of the points?\nTask 2: Using pen and paper, draw a figure of the cyclingstudy data using one categorical variable and one continuous variable and hand the figure to the next group.\nTask 3: Code the figure! Create the figure that the other group has drafted for you.\n\n\n\n\n\n\nCharacteristics such as shapes or colors can also be added to geoms outside the aes(). This means we will override any mapping already given in aes(). As we already have seen, mappings that are inherited from the ggplot function to geoms.\n\n\n\n\n\n\n\n\nPractice\n\n\n\n\nTask 1: Change characteristics of your plot outside data mapping using color and fill, linetype, size and shape. What geoms are responsive to each change?\n\n\n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nExplain the what the code will produce, without running the code below (google “r shapes” to see what number each shape has.)\n\n\n# Example 1\nggplot(data = cyclingstudy, aes(x = cmj.max, y = sj.max, color = group)) + geom_point(color = \"blue\")\n\n# Example 2\nggplot(data = cyclingstudy, aes(x = cmj.max, y = sj.max, color = group)) + geom_point(shape = 8)\n\n# Example 3\nggplot(data = cyclingstudy, aes(x = cmj.max, y = sj.max, color = weight.T1)) + geom_point(shape = 8)\n\n# Example 4\nggplot(data = cyclingstudy, aes(x = cmj.max, y = sj.max, shape = timepoint)) + geom_point(color = \"red\")\n\n\n\n\n\n\n\n\nIn ggplot2 there are pre-set palettes for colors, orders of shapes and line types etc. Often you would want to control such settings.\nThere are many sources for informed selection of colors, one is colorbrewer2. We\nTo change color scales we use scale_*_* functions that will help you set, e.g., colors manually. In the example below we create a gradient from two colors\n\nggplot(data = cyclingstudy, \n       aes(x = cmj.max, y = sj.max, color = weight.T1)) + \n        geom_point() +\n        scale_colour_gradient(\n                low = \"#e41a1c\",  \n                high = \"#4daf4a\")\n\nDiscrete variables can also be set with colors using scale_color_manual\n\nggplot(data = cyclingstudy, \n       aes(x = cmj.max, y = sj.max, color = timepoint)) + \n        geom_point() +\n        scale_color_manual(values = c(\"#66c2a5\",\"#fc8d62\",\"#8da0cb\",\"#e78ac3\"))\n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nExplain to a friend what scales do\n\n\n\n\n\n\n\nSome aspects of a plot requires that data points are connected together. This essentially means that some a variable needs to group data points. In our example data set, subject gives the identity of each participant. We may use this information to group data points, or connect them with e.g. geom_line(). By adding group = subject to the aes() call in ggplot we will group all geoms that allow grouping.\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nBefore running the code below. Explain what you expect it will show.\n\n\n\n\n\n# Example 1\nggplot(data = cyclingstudy, aes(x = cmj.max, \n                                y = sj.max, \n                                color = group,\n                                shape = timepoint,\n                                group = subject)) + \n        geom_point() +\n        geom_line()\n\n\n\n\nA plot can be quite cluttered, and in this case, give a false impression of a lot of data. The design of this study results in a data set where each participant is tested at multiple time-points. We can therefore create facets based on some aspect of the data, such as time-point.\nThe facet_wrap() and facet_grid() creates facets.\n\n# Example 1\nggplot(data = cyclingstudy, aes(x = cmj.max, \n                                y = sj.max, \n                                color = group)) + \n        geom_point() +\n        facet_wrap(~ timepoint)\n\nAs can be seen in the code above, facet_wrap takes a one-handed formula, the ~ (tilde), indicates a formula. We can read this as “wrap by time-point”.\nIn facet_grid() we will use a two-handed formula, meaning that both sides of the tilde needs information. If we want to group only by rows, we will use a . to indicate that nothing will be used to group by column.\n\nggplot(data = cyclingstudy, aes(x = cmj.max, \n                                y = sj.max, \n                                color = group)) + \n        geom_point() +\n        facet_grid( timepoint ~ .)\n\nIn facet_grid above, we could replace the . with another variable. How would you write the code to facet the graph by group in rows and time-point in columns?\n\n\nShow the code\nggplot(data = cyclingstudy, aes(x = cmj.max, \n                                y = sj.max, \n                                color = group)) + \n        geom_point() +\n        facet_grid( group ~ timepoint)\n\n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nExplain to a friend, what do the plot produced by the code above show? Include everything that is important to reproduce the plot in your description. Try to describe it without pointing at the plot!\n\n\n\n\n\n\n\nAnnotations can be added to plots, these are often user specified, such as labels and plot titles.\nWe specify labels with the labs() function and add annotations with the annotate() function.\n\nggplot(data = cyclingstudy, aes(x = cmj.max, \n                                y = sj.max, \n                                color = group)) + \n        geom_point() +\n        labs(x = \"Maximal Counter movement jump height (cm)\", \n             y = \"Maximal squat jump height (cm)\", \n             title = \"This is the title\", \n             subtitle = \"This is the subtitle\", \n             caption = \"This is a caption\", \n             color = \"This is the group aesthetics\") +\n        \n        annotate(geom = \"text\", x = 25, y = 35, label = \"This is a text annotation\")\n\nNotice that labs()makes use of all aesthetic mappings and annotate requires a geom.\n\n\n\nThe theme() function is used for non-data layers in the plot.\n\n\n\nThere are two commonly used system for combining individual plots, patchwork, https://patchwork.data-imaginist.com/ and cowplot, https://cran.r-project.org/web/packages/cowplot/vignettes/introduction.html.\nThe idea here is create figures with multiple individual figures.\npatchwork has a very simple syntax.\n\nlibrary(patchwork)\n\n\na <- ggplot(data = cyclingstudy, aes(x = cmj.max, \n                                y = sj.max, \n                                color = group)) + \n        geom_point() \n\nb <- ggplot(data = cyclingstudy, aes(y = sj.max, \n                                x = group)) + \n        geom_boxplot() \n\n\nc <- ggplot(data = cyclingstudy, aes(x = timepoint, \n                                y = sj.max, \n                                group = subject)) + \n        geom_line() \n\n\n\n(a | b) / c\n\ncowplot uses plot_grid to arrange plots\n\nlibrary(cowplot)\n\nplot_grid(a, b, c, nrow = 2)\n\nplot_grid can also use a “nested” structure.\n\nplot_grid(plot_grid(a, b, nrow = 1), \n          c, nrow = 2)\n\nIn both frameworks, annotations can be added to plots to indicate panels/sub-plots.\n\n\n\nOutput from ggplot2 can be saved from RStudio using the export buttom. However, a more reproducible manner is to save the output using ggsave\n\n\n\n\n\n\n\n\nPractice\n\n\n\n\nTask 1: Create three separate plots from the cycling data set and save them as objects in your environment.\nTask 2: Use cowplot and patchwork to group the plots together.\nTask 3: Save the plot using ggsave, explore the help pages to find what arguments are needed!\n\n\n\n\n\n\n\n\nReproduce figure 1.3 from Spiegelhalter (2019)\n\n\n# download data\nchild_heart <- read_csv(\"https://raw.githubusercontent.com/dspiegel29/ArtofStatistics/master/01-1-2-3-child-heart-survival-times/01-1-child-heart-survival-x.csv\")\n\n\nReproduce figure 2.2 and 2.3\n\n\nbeans <- read.csv(\"https://raw.githubusercontent.com/dspiegel29/ArtofStatistics/master/02-2-3-jelly-bean-counts/02-1-bean-data-full-x.csv\", header = FALSE)"
  },
  {
    "objectID": "ws4-data-wrangling-tables.html",
    "href": "ws4-data-wrangling-tables.html",
    "title": "Quantitative Methods - Workshop, Assigments and Tutorials",
    "section": "",
    "text": "The tidyverse contains two packages with functions used to wrangle data, dplyr and tidyr. On wikipedia we can read that:\n\nData analysts typically spend the majority of their time in the process of data wrangling compared to the actual analysis of the data.\n\nIn the age of data we would be ignorant to teach data analysis without data wrangling\n\n\ndplyr contains verbs used for data manipulation, such as filtering rows, selecting variables and changing or creating variables. Verbs can be used in a pipe and read as sequential operations:\n\n> Take the data **then do** \n> filter based on group **then do**\n> create a new variable **then do**\n> show the data in the console.\n\nThe pipe operations are made possible by another package, magrittr. This package contains the forward pipe operator %>%. The forward pipe operator (%>%) can be read as “then do”. The operator takes the object on the left-hand side and puts it as the first argument in the following function.\nTranslating the “pipe” above from human language to R language using the “tidyverse dialect” looks like this:\n\ndata %>%\n        filter(group == \"xx\") %>%\n        mutate(new.var = old.var + old.var2) %>%\n        print()\n\n\n\n\nWe will use the cyclingstudy data from exscidata in our exercises. Load the required components to your session.\n\n\nShow the code\nlibrary(tidyverse) # loads dplyr etc.\nlibrary(exscidata) # loads the data-package\n\ndata(\"cyclingstudy\")\n\n\n\n\n\nMutate can help you create new variables (or overwrite existing once). In the cycling data set there is a variable called VO2.max, this variable is expressed in absolute units (ml min-1), however, we might want to express it as relative units (ml kg-1 min-1).\n\ncyclingstudy %>%\n        mutate(rel.vo2max = VO2.max / weight.T1) %>%\n        print()\n\nThe mutate function creates new variables (or overwrite existing) in a flexible way. Here we simply use division. Other mathematical operators can similarly be used (+, -, *, etc.).\nNotice the print() function in the end of the pipe. This is used to display the results of any manipulations done in the pipe. Notice also that our new variable is not listed. We might need to select a sub-set of variables to get a better overview. We will do this using the select function.\n\ncyclingstudy %>%\n        mutate(rel.vo2max = VO2.max / weight.T1) %>%\n        select(subject, group, timepoint, age, height.T1, weight.T1, VO2.max, rel.vo2max) %>%\n        print()\n\nThe select function takes variable names as “unquoted” names. We can also select a range of columns using the syntax <from>:<to> where <from> is the first column you would like to select and <to> would be the last. Subsequently the above pipe can be re-written as\n\ncyclingstudy %>%\n        mutate(rel.vo2max = VO2.max / weight.T1) %>%\n        select(subject:weight.T1, VO2.max, rel.vo2max) %>%\n        print()\n\nselect can also be used to re-name variables. Simpler variable names for weight, height and relative V̇O2max could be weight, height, vo2max.kg.\n\ncyclingstudy %>%\n        mutate(rel.vo2max = VO2.max / weight.T1) %>%\n        select(subject:age, height = height.T1, weight = weight.T1, VO2.max,vo2max.kg = rel.vo2max) %>%\n        print()\n\n\n\n\nFiltering is used to select specific observations (rows) of a data set. We filter based on specific conditions such as:\n\nAll values bigger than X\nAll values less than Y\nAll observations than contain A, B or C in variable V.\n\nThe above examples must be translated to formal expressions.\n\n\nAn expression that make comparisons can be\n\nx < y → x less than y\nx > y → x greater than y\nx <= y → x less or equal to y\nx >= y → x greater or equal to y\nx == y → x exactly equal to y\nx != y → x not exactly equal to y\n\nIn the filter function these expressions give either TRUE or FALSE. If TRUE the rows are included in the filtered data frame.\nWe can see the mechanism behind filtering by creating a vector of TRUE and FALSE based on an expression. Let’s say that we want to see which rows has weight.T1 greater than 75.\n\ncyclingstudy$weight.T1 > 75\n\n [1]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n[13]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE\n[25]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE  TRUE\n[37]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE    NA  TRUE  TRUE FALSE\n[49]    NA  TRUE FALSE    NA  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[61]  TRUE FALSE  TRUE  TRUE    NA  TRUE  TRUE FALSE  TRUE FALSE FALSE FALSE\n[73]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n\n\nWe can see that the first row returns a TRUE while the second row returns FALSE.\nUsing the filter function, we just add the expression as an argument in the function and all the rows that comes up TRUE will remain.\n\ncyclingstudy %>%\n        filter(weight.T1 > 75) %>%\n        print()\n\n# A tibble: 60 x 101\n   subject group timepoint   age height.T1 weight.T1 sj.max cmj.max lac.125\n     <dbl> <chr> <chr>     <dbl>     <dbl>     <dbl>  <dbl>   <dbl>   <dbl>\n 1       1 INCR  pre          33       183      80.3   31.0    35.0    1.5 \n 2       3 INCR  pre          39       193      98.1   26.8    28.8    1.17\n 3       4 DECR  pre          37       175      79.2   29.2    30.8    0.88\n 4       5 DECR  pre          31       176      88     31.2    25.8    1.06\n 5       6 INCR  pre          33       168      79.6   34.2    35.3    1.27\n 6       7 MIX   pre          42       180      77.6   30.1    33.0    0.85\n 7       8 MIX   pre          26       179      75.5   32.8    33.2    0.93\n 8       9 MIX   pre          41       185      82.4   22.7    22.4    1.48\n 9      10 INCR  pre          35       187      75.6   29.7    31.1    0.93\n10      13 DECR  pre          41       183      76     34.3    36      1.67\n# ... with 50 more rows, and 92 more variables: lac.175 <dbl>, lac.225 <dbl>,\n#   lac.250 <dbl>, lac.275 <dbl>, lac.300 <dbl>, lac.325 <dbl>, lac.350 <dbl>,\n#   lac.375 <dbl>, VO2.125 <dbl>, VO2.175 <dbl>, VO2.225 <dbl>, VO2.250 <dbl>,\n#   VO2.275 <dbl>, VO2.300 <dbl>, VO2.325 <dbl>, VO2.350 <dbl>, VO2.375 <dbl>,\n#   VCO2.125 <dbl>, VCO2.175 <dbl>, VCO2.225 <dbl>, VCO2.250 <dbl>,\n#   VCO2.275 <dbl>, VCO2.300 <dbl>, VCO2.325 <dbl>, VCO2.350 <dbl>,\n#   VCO2.375 <dbl>, VE.125 <dbl>, VE.175 <dbl>, VE.225 <dbl>, VE.250 <dbl>, ...\n\n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\nHow many rows in the cyclingstudy data set has\n\nVO2.max values greater than 6000\nVO2.max values less than 6000\nVO2.max values less or equal to than 5360\nVO2.max values greater or equal to than 5360\nthe value pre in timepoint\nthe a value in timepoint other than pre\n\n\n\n\n\n\n\nLogical operators similarly creates TRUE or FALSE as the basis of filtering operations. These can be used in combination with comparisons.\n\n! x → NOT x\nx & y → x and y\nx | y → x or y\nis.na(x) → returns TRUE if x is NA\n\nWe might want to keep all rows with weight.T1 greater than 80 that are also from the group INCR. This can be solved with an AND operator (&).\n\n#| echo: true\n\ncyclingstudy %>%\n        filter(weight.T1 > 80 & group == \"INCR\") %>%\n        print()\n\n# A tibble: 9 x 101\n  subject group timepoint   age height.T1 weight.T1 sj.max cmj.max lac.125\n    <dbl> <chr> <chr>     <dbl>     <dbl>     <dbl>  <dbl>   <dbl>   <dbl>\n1       1 INCR  pre          33       183      80.3   31.0    35.0    1.5 \n2       3 INCR  pre          39       193      98.1   26.8    28.8    1.17\n3      20 INCR  pre          42       180      82.3   32.5    32.2    2.26\n4       3 INCR  meso1        39        NA     101.    26.0    26.5    0.72\n5      20 INCR  meso1        42        NA      82.5   30.2    30.6    0.96\n6       3 INCR  meso2        39        NA      99.2   25.4    26.8    0.5 \n7      20 INCR  meso2        42        NA      81.1   29.6    29.8    1.53\n8       3 INCR  meso3        39        NA      99.2   27.1    27.0    0.77\n9      20 INCR  meso3        43        NA      81.5   30.0    30.9    1.77\n# ... with 92 more variables: lac.175 <dbl>, lac.225 <dbl>, lac.250 <dbl>,\n#   lac.275 <dbl>, lac.300 <dbl>, lac.325 <dbl>, lac.350 <dbl>, lac.375 <dbl>,\n#   VO2.125 <dbl>, VO2.175 <dbl>, VO2.225 <dbl>, VO2.250 <dbl>, VO2.275 <dbl>,\n#   VO2.300 <dbl>, VO2.325 <dbl>, VO2.350 <dbl>, VO2.375 <dbl>, VCO2.125 <dbl>,\n#   VCO2.175 <dbl>, VCO2.225 <dbl>, VCO2.250 <dbl>, VCO2.275 <dbl>,\n#   VCO2.300 <dbl>, VCO2.325 <dbl>, VCO2.350 <dbl>, VCO2.375 <dbl>,\n#   VE.125 <dbl>, VE.175 <dbl>, VE.225 <dbl>, VE.250 <dbl>, VE.275 <dbl>, ...\n\n\nWe can similarly use OR (|) to select either weight greater than 80 or group INCR.\n\n#| echo: true\n\ncyclingstudy %>%\n        filter(weight.T1 > 80 | group == \"INCR\") %>%\n        print()\n\n# A tibble: 46 x 101\n   subject group timepoint   age height.T1 weight.T1 sj.max cmj.max lac.125\n     <dbl> <chr> <chr>     <dbl>     <dbl>     <dbl>  <dbl>   <dbl>   <dbl>\n 1       1 INCR  pre          33       183      80.3   31.0    35.0    1.5 \n 2       3 INCR  pre          39       193      98.1   26.8    28.8    1.17\n 3       5 DECR  pre          31       176      88     31.2    25.8    1.06\n 4       6 INCR  pre          33       168      79.6   34.2    35.3    1.27\n 5       9 MIX   pre          41       185      82.4   22.7    22.4    1.48\n 6      10 INCR  pre          35       187      75.6   29.7    31.1    0.93\n 7      14 MIX   pre          35       183      81.3   27.6    30.0    1.13\n 8      15 INCR  pre          34       178      75.1   33.5    32.4    0.8 \n 9      16 INCR  pre          27       178      77.8   32.9    33.7    0.94\n10      18 DECR  pre          41       186     105.    33.5    33.7    1.75\n# ... with 36 more rows, and 92 more variables: lac.175 <dbl>, lac.225 <dbl>,\n#   lac.250 <dbl>, lac.275 <dbl>, lac.300 <dbl>, lac.325 <dbl>, lac.350 <dbl>,\n#   lac.375 <dbl>, VO2.125 <dbl>, VO2.175 <dbl>, VO2.225 <dbl>, VO2.250 <dbl>,\n#   VO2.275 <dbl>, VO2.300 <dbl>, VO2.325 <dbl>, VO2.350 <dbl>, VO2.375 <dbl>,\n#   VCO2.125 <dbl>, VCO2.175 <dbl>, VCO2.225 <dbl>, VCO2.250 <dbl>,\n#   VCO2.275 <dbl>, VCO2.300 <dbl>, VCO2.325 <dbl>, VCO2.350 <dbl>,\n#   VCO2.375 <dbl>, VE.125 <dbl>, VE.175 <dbl>, VE.225 <dbl>, VE.250 <dbl>, ...\n\n\nNotice that there are rows containing weights less than 80 from the INCR group.\nAny logical statement can also be negated with ! indication NOT. This means we will get a vector of TRUE for any expression previously being FALSE. Notice the extra parentheses below.\n\n#| echo: true\n\ncyclingstudy %>%\n        filter(!(weight.T1 > 80 & group == \"INCR\")) %>%\n        print()\n\n# A tibble: 71 x 101\n   subject group timepoint   age height.T1 weight.T1 sj.max cmj.max lac.125\n     <dbl> <chr> <chr>     <dbl>     <dbl>     <dbl>  <dbl>   <dbl>   <dbl>\n 1       2 DECR  pre          32       174      71.4   31.6    33.8    1.19\n 2       4 DECR  pre          37       175      79.2   29.2    30.8    0.88\n 3       5 DECR  pre          31       176      88     31.2    25.8    1.06\n 4       6 INCR  pre          33       168      79.6   34.2    35.3    1.27\n 5       7 MIX   pre          42       180      77.6   30.1    33.0    0.85\n 6       8 MIX   pre          26       179      75.5   32.8    33.2    0.93\n 7       9 MIX   pre          41       185      82.4   22.7    22.4    1.48\n 8      10 INCR  pre          35       187      75.6   29.7    31.1    0.93\n 9      11 MIX   pre          34       168      55.8   33.2    30.8    0.62\n10      13 DECR  pre          41       183      76     34.3    36      1.67\n# ... with 61 more rows, and 92 more variables: lac.175 <dbl>, lac.225 <dbl>,\n#   lac.250 <dbl>, lac.275 <dbl>, lac.300 <dbl>, lac.325 <dbl>, lac.350 <dbl>,\n#   lac.375 <dbl>, VO2.125 <dbl>, VO2.175 <dbl>, VO2.225 <dbl>, VO2.250 <dbl>,\n#   VO2.275 <dbl>, VO2.300 <dbl>, VO2.325 <dbl>, VO2.350 <dbl>, VO2.375 <dbl>,\n#   VCO2.125 <dbl>, VCO2.175 <dbl>, VCO2.225 <dbl>, VCO2.250 <dbl>,\n#   VCO2.275 <dbl>, VCO2.300 <dbl>, VCO2.325 <dbl>, VCO2.350 <dbl>,\n#   VCO2.375 <dbl>, VE.125 <dbl>, VE.175 <dbl>, VE.225 <dbl>, VE.250 <dbl>, ...\n\n\nThe dplyr function filter also accepts multiple arguments separated with a comma. This is equal to adding conditions with the AND operator. Example:\n\n#| echo: true\n\ncyclingstudy %>%\n        filter(weight.T1 > 80, \n               group == \"INCR\", \n               timepoint == \"pre\", \n               age > 35) %>%\n        print()\n\n# A tibble: 2 x 101\n  subject group timepoint   age height.T1 weight.T1 sj.max cmj.max lac.125\n    <dbl> <chr> <chr>     <dbl>     <dbl>     <dbl>  <dbl>   <dbl>   <dbl>\n1       3 INCR  pre          39       193      98.1   26.8    28.8    1.17\n2      20 INCR  pre          42       180      82.3   32.5    32.2    2.26\n# ... with 92 more variables: lac.175 <dbl>, lac.225 <dbl>, lac.250 <dbl>,\n#   lac.275 <dbl>, lac.300 <dbl>, lac.325 <dbl>, lac.350 <dbl>, lac.375 <dbl>,\n#   VO2.125 <dbl>, VO2.175 <dbl>, VO2.225 <dbl>, VO2.250 <dbl>, VO2.275 <dbl>,\n#   VO2.300 <dbl>, VO2.325 <dbl>, VO2.350 <dbl>, VO2.375 <dbl>, VCO2.125 <dbl>,\n#   VCO2.175 <dbl>, VCO2.225 <dbl>, VCO2.250 <dbl>, VCO2.275 <dbl>,\n#   VCO2.300 <dbl>, VCO2.325 <dbl>, VCO2.350 <dbl>, VCO2.375 <dbl>,\n#   VE.125 <dbl>, VE.175 <dbl>, VE.225 <dbl>, VE.250 <dbl>, VE.275 <dbl>, ...\n\n\nFinally, dplyr comes with two convenient functions to find values between and near\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nKeep rows in your data frame from the pre time-point, age greater than 31 but height less than 180.\nUse between to find rows with VO2.max values between 4800 and 5200 (see ?between)\nUse near to find weight.T1 values close to 80.26 with a tolerance of 0.75 (see ?near)\nRemove all rows that are NA in the height.T1 variable.\n\n\n\n\n\n\n\n\nA super power of dplyr is its ability to group and summarize data. The group_by function creates a grouped data frame suitable for summaries per group. In the cyclingstudy data set we have three groups that we might want to describe using some summary function.\nExamples of summary functions in R are:\n\nmean() → computes arithmetic mean\nmedian() → computes the median\nsd() → computes the standard deviation from the mean\nIQR() → returns the inter-quartile range\nmin() and max() → gives the minimum and maximum values from a vector\nquantile() → sample quantiles from the smallest (probs = 0) to largest (probs = 1) values.\nAll the above functions comes with the optional argument of na.rm = TRUE. This can be read as remove missing values (NA). If there are missing values (NA) and na.rm = FALSE (the default), the calculations will return NA. This is inconvenient but can often work as a sanity check of your code.\n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\n\nWhat do we mean by sanity check?\nWhat do you expect from the R code sd(c(4, 5, 7, NA, 5))\nWhat would you add to the code above to improve it?\n\n\n\n\nIn addition to the summaries above that are generic for base R, dplyr provides you with a number of great functions to…\n\nn() → count the number of observations in each group\nn_distinct() → return the number of unique values from a vector for each group\n\nIn practice a grouped summary may look like this:\n\ncyclingstudy %>%\n        group_by(group) %>%\n        summarise(mean.vo2max = mean(VO2.max, na.rm = TRUE))\n\nResults from the above code includes multiple data points from each participant. The variable describing time-points can be added to the grouping.\n\ncyclingstudy %>%\n        group_by(group, timepoint) %>%\n        summarise(mean.vo2max = mean(VO2.max, na.rm = TRUE))\n\nMultiple summary functions can be added to the summarise() function where each adds a new variable to the result data frame.\n\ncyclingstudy %>%\n        group_by(group, timepoint) %>%\n        summarise(mean.vo2max = mean(VO2.max, na.rm = TRUE), \n                  sd.vo2max = sd(VO2.max, na.rm = TRUE))\n\n\n\n\nData not always in a form that makes tables, graphs or statistical methods directly available. Data can be described as being in long form and wide form. The long form data is tidy in the sense that all columns are distinct variables.\n\nThere are examples of wide data sets as part of the cycling data set. Using only the timepoint == pre values and columns corresponding to lactate values from the graded exercise test we have an example of wide data as the columns lac.125, lac.175, lac.225, etc., contains lactate values from different exercise intensities. This means that a separate variable (watt or exercise intensity) is combined in each column of lactate values.\n\ncyclingstudy %>%\n        filter(timepoint == \"pre\") %>%\n        dplyr::select(subject, group, lac.125:lac.375) %>%\n        print()\n\n# A tibble: 20 x 11\n   subject group lac.125 lac.175 lac.225 lac.250 lac.275 lac.300 lac.325 lac.350\n     <dbl> <chr>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1       1 INCR     1.5     1.86    2.38    3.54    6.21   NA      NA      NA   \n 2       2 DECR     1.19    1.49    2.34    3.21    5.33   NA      NA      NA   \n 3       3 INCR     1.17    1.52    1.22    1.54    2.04    3.32    4.72   NA   \n 4       4 DECR     0.88    0.99    2.13    3.25   NA       6.15   NA      NA   \n 5       5 DECR     1.06    1.41    1.9     2.04    3.04    3.59    4.73   NA   \n 6       6 INCR     1.27    1.73    3.21    4.83   NA      NA      NA      NA   \n 7       7 MIX      0.85    0.84    1.16    1.71    3.33    6.25   NA      NA   \n 8       8 MIX      0.93    1.34    1.94   NA       3.71    7.29   NA      NA   \n 9       9 MIX      1.48    1.17    1.95   NA       3.24    6.21   NA      NA   \n10      10 INCR     0.93    0.87    0.86    0.92    1.2     1.69    2.6     4.69\n11      11 MIX      0.62    1.22    2.85    5.86    9.87   NA      NA      NA   \n12      13 DECR     1.67    1.81    2.78    4.25    6.87   NA      NA      NA   \n13      14 MIX      1.13    1.33    2.74    3.97   NA      NA      NA      NA   \n14      15 INCR     0.8     1.12    1.43   NA       2.4     3.77    6.3    NA   \n15      16 INCR     0.94    1.18    1.89    2.83    5.33   NA      NA      NA   \n16      17 MIX      1.54    1.62    2.7     4.1    NA      NA      NA      NA   \n17      18 DECR     1.75    2.08    2.99    4.22   NA      NA      NA      NA   \n18      19 DECR     1.23    2.51    4.65   NA      NA      NA      NA      NA   \n19      20 INCR     2.26    2.05    3.19    5.17   NA      NA      NA      NA   \n20      21 DECR     0.68    0.89    1.98    3.18    5.57   NA      NA      NA   \n# ... with 1 more variable: lac.375 <dbl>\n\n\nUsing pivot_longer we can change this data into a long format. Pivot wider needs information on the new variable names for values and names. Names are the column names thta will form a variable and values are the values contained in the cells of the old variables. We also need to specify what columns to make longer, notice that I select variables <from>:<to>.\n\ncyclingstudy %>%\n        filter(timepoint == \"pre\") %>%\n        dplyr::select(subject, group, lac.125:lac.375) %>%\n        pivot_longer(names_to = \"watt\", \n                     values_to = \"lactate\", \n                     cols = lac.125:lac.375) %>%\n        print()\n\n# A tibble: 180 x 4\n   subject group watt    lactate\n     <dbl> <chr> <chr>     <dbl>\n 1       1 INCR  lac.125    1.5 \n 2       1 INCR  lac.175    1.86\n 3       1 INCR  lac.225    2.38\n 4       1 INCR  lac.250    3.54\n 5       1 INCR  lac.275    6.21\n 6       1 INCR  lac.300   NA   \n 7       1 INCR  lac.325   NA   \n 8       1 INCR  lac.350   NA   \n 9       1 INCR  lac.375   NA   \n10       2 DECR  lac.125    1.19\n# ... with 170 more rows\n\n\npivot_wider makes it easy to remove prefix and fix the data type of the names variable. Below we specify to remove lac. from all names and convert the new variable to numeric data.\n\ncyclingstudy %>%\n        filter(timepoint == \"pre\") %>%\n        dplyr::select(subject, group, lac.125:lac.375) %>%\n        pivot_longer(names_to = \"watt\", \n                     values_to = \"lactate\", \n                     cols = lac.125:lac.375, \n                     names_prefix = \"lac.\", \n                     names_transform = list(watt = as.numeric)) %>%\n        print()\n\n# A tibble: 180 x 4\n   subject group  watt lactate\n     <dbl> <chr> <dbl>   <dbl>\n 1       1 INCR    125    1.5 \n 2       1 INCR    175    1.86\n 3       1 INCR    225    2.38\n 4       1 INCR    250    3.54\n 5       1 INCR    275    6.21\n 6       1 INCR    300   NA   \n 7       1 INCR    325   NA   \n 8       1 INCR    350   NA   \n 9       1 INCR    375   NA   \n10       2 DECR    125    1.19\n# ... with 170 more rows\n\n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\nPerform the opposite operation of the below data set:\n\ndata.frame(id = c(\"id1\", \"id1\", \"id1\", \"id2\", \"id2\", \"id2\"), \n           NAME = c(\"a\", \"b\", \"c\", \"a\", \"b\", \"c\"), \n           NUMBER = c(4, 6, 7, 2, 3, 5)) %>%\n\n        print()\n\n\n\n\n\n\n\nTable 1 in experimental or observational studies often contains descriptive data on the sample. These tables may help readers to understand to what group/population a study may be generalized to and how key characteristics are distributed among experimental groups.\nWe will prepare data from cyclingstudy to create a Table 1 with descriptive data:\n\nSelect a set of key variables that you want to describe (center, spread and/or range) from baseline measurements\nGroup the data set on group and perform summary calculations\n\n\n\nTo display numbers with the correct number of decimals R provides many options. A simple function (round()) provides rounding. The problem is that you will lose trailing zero, e.g., 2.0 will be displayed as 2. To keep the trailing zero we must use the sprintf() function. Examples:\n\n# Rounding\nround(2.10, 2) \n\n[1] 2.1\n\n# Formatting to keep the trailing zero\nsprintf(\"%.2f\", 2.10)\n\n[1] \"2.10\"\n\n\nCombining vectors may be a good idea to make the table more attractive. The mean and standard deviation is commonly presented as mean (SD). Data from a column of means and a column of SD’s can be combined to create a nice display using the paste0() function. Example:\n\ndata.frame(m = c(46.7, 47.89, 43.5),  # A vector of means\n           s = c(4.21, 4.666, 3.1)) %>% # A vector of SD's\n        mutate(stat = paste0(round(m, 1), \n                             \" (\",\n                             round(s, 1), \n                             \")\")) %>%\n        print()\n\n      m     s       stat\n1 46.70 4.210 46.7 (4.2)\n2 47.89 4.666 47.9 (4.7)\n3 43.50 3.100 43.5 (3.1)\n\n\nA character vector of group names can be arranged and re-named using the factor function. In the cyclingstudy data set the groups (INCR, DECR and MIX) may be given more descriptive names, example:\n\ncyclingstudy %>%\n        mutate(group = factor(group, levels = c(\"INCR\", \"DECR\", \"MIX\"), \n                              labels = c(\"Increased intensity\", \n                                         \"Decreased intensity\", \n                                         \"Mixed intensity\"))) %>%\n        distinct(group)\n\n# A tibble: 3 x 1\n  group              \n  <fct>              \n1 Increased intensity\n2 Decreased intensity\n3 Mixed intensity    \n\n\n\n\n\n\nWe will use a table generator to create the table. See next part of this workshop here"
  },
  {
    "objectID": "ws4b-tables.html",
    "href": "ws4b-tables.html",
    "title": "Quantitative Methods - Workshop, Assigments and Tutorials",
    "section": "",
    "text": "Footnotes\n\n\nQuarto was released on the 28:th of July 2022↩︎\nSee the list at the bottom of the gt package webpage↩︎"
  },
  {
    "objectID": "ws5-writing-reports.html",
    "href": "ws5-writing-reports.html",
    "title": "Writing reports and working with R projects",
    "section": "",
    "text": "These workshop notes contains links to relevant documentation-"
  },
  {
    "objectID": "ws5-writing-reports.html#quarto-and-rmarkdown",
    "href": "ws5-writing-reports.html#quarto-and-rmarkdown",
    "title": "Writing reports and working with R projects",
    "section": "Quarto and Rmarkdown",
    "text": "Quarto and Rmarkdown\n\nQuarto and R Markdown a special kind of scripts where text and computer code can be combined to generate reports.\nUnder the hood, a report generator is converting code and plain text to an output format such as html, pdf or docx (more formats are available).\nQuarto is a new, well documented format that gives extra flexibility, but also requires installation of extra software.\nR Markdown is even more well documented, see e.g. R Markdown, R Markdown: The Definitev Guide"
  },
  {
    "objectID": "ws5-writing-reports.html#code-execution-and-the-environment",
    "href": "ws5-writing-reports.html#code-execution-and-the-environment",
    "title": "Writing reports and working with R projects",
    "section": "Code execution and the environment",
    "text": "Code execution and the environment\n\nWhen a quarto or Rmarkdown file is “knitted”, the source file looks for e.g., data files in the same directory as the source file is saved.\nWorking in a RStudio projects makes it easy to work with the report interactively as you can use relative paths.\nRStudio has an excellent guide to its project feature"
  },
  {
    "objectID": "ws5-writing-reports.html#rstudio-projects",
    "href": "ws5-writing-reports.html#rstudio-projects",
    "title": "Writing reports and working with R projects",
    "section": "RStudio projects",
    "text": "RStudio projects\n\nA projects is basically a collection of settings together with a root directory.\nSettings can be accessed in Tools > Project Options.\nThis means that you will be able to work with relative paths. If reading a csv file using relative paths, your code will look like this from a project.\n\n\ndat <- read_csv(\"./data/my-data.csv\")\n\n\nIf your are using absolute paths, reaching the same goal could look like this\n\n\ndat <- read_csv(\"C:/Users/Daniel/Dropbox/Some-folder/a-project/data/my-data.csv\")\n\n\nRStudio projects helps you create good habits for reproducible analysis as all analyses are conducted within a stand-alone folder structure. Your data and scripts can be shared.\nUse a basic structure for all projects:\n\nMy-project\n        |\n        |-.Rproj        (The project settings)\n        |--/data        (Contains all data needed for your analysis)\n        |--/R           (Contains all scripts/R-files)\n        |--/output      (Collection of all output files)\n\n\n\nStart a new project from the Project menu."
  },
  {
    "objectID": "ws5-writing-reports.html#writing-in-quartor-markdown",
    "href": "ws5-writing-reports.html#writing-in-quartor-markdown",
    "title": "Writing reports and working with R projects",
    "section": "Writing in Quarto/R Markdown",
    "text": "Writing in Quarto/R Markdown\n\nThe basic syntax in quarto/R Markdown files is markdown. Markdown makes it easy to format text without point-and-click as all formatting can be added with syntax, example:\n\nThis text is an example of the markdown syntax which includes **bold**, *italic*,\n^super^ and ~subscript~ and ~~striketrough~~\nResulting in:\n\nThis text is an example of the markdown syntax which includes bold, italic, super and subscript and striketrough\n\n\nIn addition to text formatting, the markdown syntax offers solutions for adding images, tables, equations, lists, diagrams, different highligt blocks. See the the quarto documentation for more information."
  },
  {
    "objectID": "ws5-writing-reports.html#code-chunks",
    "href": "ws5-writing-reports.html#code-chunks",
    "title": "Writing reports and working with R projects",
    "section": "Code chunks",
    "text": "Code chunks\n\nCode chunks are sections of your source file containing code. We primarily write R-code, but e.g., python is also possible.\nThe code chunk comes with several options specified in the top of the chunk using #|, such as:\n\n```{r}\n#| eval: true\n#| echo: true\n#| warning: false\n#| error: true\n#| include: false\n```\n\nNote that the #| is a “new” intervention, some documentation will still suggest that you use code chunk settings in the code chunk header."
  },
  {
    "objectID": "ws5-writing-reports.html#inline-code",
    "href": "ws5-writing-reports.html#inline-code",
    "title": "Writing reports and working with R projects",
    "section": "Inline code",
    "text": "Inline code\n\nCode may be included inline to include code generated outputs\n\n\na_variable <- 3.14\n\n\nTo include the variable in the text:\n\n\nThe variable will be displayed here `r a_variable`"
  },
  {
    "objectID": "ws5-writing-reports.html#bibliographies",
    "href": "ws5-writing-reports.html#bibliographies",
    "title": "Writing reports and working with R projects",
    "section": "Bibliographies",
    "text": "Bibliographies\n\nBibliographies/Citations may be added to reports using a external bibliography file. A simple format is bibtex\nPubmed entries can be searched using TexMed\nIn the visual editor, bibliographies can be easily created"
  },
  {
    "objectID": "ws5-writing-reports.html#some-notes-on-different-formats",
    "href": "ws5-writing-reports.html#some-notes-on-different-formats",
    "title": "Writing reports and working with R projects",
    "section": "Some notes on different formats",
    "text": "Some notes on different formats\n\nHTML is the basic output from R Markdown and quarto. This is suitable for first drafts.\nTo be able to render PDF files you must have an installation of rendering software. TinyTeX is generally recomended, see the documentation and installation instructions here\nWord documents creates an editable document with associated pros and cons.\nAll formats has different advantages and offer flexibility that gives opportunities to create any type of document\nOther formats such as presentations, webpages, apps etc. makes quarto / R Markdown very versatile."
  },
  {
    "objectID": "ws6-git.html",
    "href": "ws6-git.html",
    "title": "Collaborative coding with github",
    "section": "",
    "text": "Jennifer Bryan has written an excellent paper (Bryan 2017) on the use of git and GitHub when working with R.\nJennifer is also co-author on the web-book “Happy Git and GitHub for the useR” which is an excellent resource whenever you are stuck.\n\n\nGit is a version control system. It keeps track of changes made to files contained in a repository. Additionally, using GitHub users can collaborate on a repository by keeping an online version of the repository.\nThe benefits of using git comes from enabling collaboration with other and your future you (Bryan 2017)."
  },
  {
    "objectID": "ws6-git.html#different-alternatives-for-version-control",
    "href": "ws6-git.html#different-alternatives-for-version-control",
    "title": "Collaborative coding with github",
    "section": "Different alternatives for version control",
    "text": "Different alternatives for version control\n\n\n\n(Figure from Bryan 2017)"
  },
  {
    "objectID": "ws6-git.html#basic-git-and-github",
    "href": "ws6-git.html#basic-git-and-github",
    "title": "Collaborative coding with github",
    "section": "Basic git and GitHub",
    "text": "Basic git and GitHub\n\nCreate new repositories online on github, this will be an empty repository\nClone the repository in your R project (New Project > Version Control > Git)\nMake changes, add files etc.\ngit add file for a specific file or git add -A for all changed files\ngit commit -m \"message\" commit changes to the version control system with a message describing what you have done.\ngit push to the remote repository (GitHub)\nIf collaborating, a collaborator may git pull all changes to their local repository.\nCheck if you have any changes that needs to be commited or pulled by git status\n\n\nUsing forks and pull requests\n\nA fork is a copy of a repository that may evolve independently to the original repository under your own username on github.\nCreate a fork from the GitHub web interface (www.github.com).\nAfter you have made changes you may file a pull request to the original repository. You will have to describe the changes and why you think the maintainer should accept pulling your changes into the original repository.\nThis is a great way to suggest changes to complex projects.\n\n\n\nUsing branches\n\nSimilarly to forks, a branch can also be used to update a repository with changes which are then merged to the main branch after testing or review.\nSee the GitHub documentation for details."
  },
  {
    "objectID": "ws7-norwegian.html",
    "href": "ws7-norwegian.html",
    "title": "Workshop 7: Hvordan i huleste løser jeg arbeidskrav 1? En oppsummering av emnet så langt",
    "section": "",
    "text": "Hensikten med oppgaven er (1) å beskrive reliabilitet i en testmetode fra fysiologilabben, og (2) bruke verktøy for reproduserbar dataanalyse. Testet skal beskrives i detalj, inkludert gjennomføring av testen og hvordan data prosesseres fra rådata til ferdig rapport. Kravet om verktøy for reproduserbar dataanalyse innebærer at dere forventes lage en rapport som kan reproduseres på en annen PC, rapporten skal derfor være koblet til data og kod som kan gjenskape den."
  },
  {
    "objectID": "ws7-norwegian.html#steg-1-forberedelse-av-data",
    "href": "ws7-norwegian.html#steg-1-forberedelse-av-data",
    "title": "Workshop 7: Hvordan i huleste løser jeg arbeidskrav 1? En oppsummering av emnet så langt",
    "section": "Steg 1: Forberedelse av data",
    "text": "Steg 1: Forberedelse av data\nSom et første steg kan det være lurt å lage datasett som er «tidy». Dette innebærer en rad per observasjon og en kolonne per variabel. I en reliabilitetsstudie kan man f.eks. samle inn data fra et antall deltakere og to tester. Data fra de to første deltakerne kan se slik ut:\n\n\n\nparticipant\ntime\nvalue\n\n\n\n\n1\n1\n67\n\n\n2\n1\n54\n\n\n1\n2\n69\n\n\n2\n2\n53"
  },
  {
    "objectID": "ws7-norwegian.html#steg-2-sett-opp-verktøy-for-reproduserbar-dataanalyse",
    "href": "ws7-norwegian.html#steg-2-sett-opp-verktøy-for-reproduserbar-dataanalyse",
    "title": "Workshop 7: Hvordan i huleste løser jeg arbeidskrav 1? En oppsummering av emnet så langt",
    "section": "Steg 2: Sett opp verktøy for reproduserbar dataanalyse",
    "text": "Steg 2: Sett opp verktøy for reproduserbar dataanalyse\nHer kreves ekstra programvare på din PC og en brukere på www.github.com. Last ned git fra https://git-scm.com/downloads og legg til sti (file path) til git executable (git.exe) i RStudio gjennom menyene Tools > Global Options > Git/SVN > Git executable. Du finner git.exe i f.eks. C:/Program Files/Git/bin/git.exe.\nNeste steg er å skape en brukere på www.github.com.\nDen enkleste måten å starte en mappe (repository) med versjonskontroll og kobling til github er å logge på github.com, velg «New» eller gå til https://github.com/new for å lage en ny repository. Velg et godt navn (beskrivende for hva hensikten med analysen er, men kort). Trykk på «create repository», kopier adressen i det neste steget (f.eks. https://github.com/dhammarstrom/vo2max-reliability.git).\nStarte opp RStudio. Velg «New project» i Project menyen. Velg «version control» og «Git», lim inn adressen fra github.com. Velg en passende plass på din PC for å lagre prosjektmappen, trykk på «Create project». Du har nå en mappe med versjonskontroll som er koblet mot en «remote repository» på github.com."
  },
  {
    "objectID": "ws7-norwegian.html#steg-3-legg-inn-data-og-starte-opp-et-quarto-dokument",
    "href": "ws7-norwegian.html#steg-3-legg-inn-data-og-starte-opp-et-quarto-dokument",
    "title": "Workshop 7: Hvordan i huleste løser jeg arbeidskrav 1? En oppsummering av emnet så langt",
    "section": "Steg 3: Legg inn data og starte opp et quarto-dokument",
    "text": "Steg 3: Legg inn data og starte opp et quarto-dokument\nI den prosjektmappe du har på PC kan du nå legge til data. Skape en ny mappe med navnet «data» og legg in dataene du forbrett i Steg 1. Dette kan være en xlsx- eller csv-fil.\nFor å bruke quarto trenger du å installere quarto, gå til https://quarto.org/, trykk på «Get Started» og følg instruksjonene.\nEtte å ha startet RStudio på nytt bør du ha mulighet til å starte et nytt quarto-dokument fra File > New File > Quarto document.\nSpare dokumentet i din prosjektmappe. Vi er nå klare for å logge den første versjonen av prosjektet."
  },
  {
    "objectID": "ws7-norwegian.html#steg-4-git-add-commit-and-push",
    "href": "ws7-norwegian.html#steg-4-git-add-commit-and-push",
    "title": "Workshop 7: Hvordan i huleste løser jeg arbeidskrav 1? En oppsummering av emnet så langt",
    "section": "Steg 4: «Git add, commit and push»",
    "text": "Steg 4: «Git add, commit and push»\nFiler kan legges til i versjonshistorikken ved å det grafiske grensesnittet i RStudio. Trykk på Ctrl+Alt+M og du får opp et vindu med endringer. Her bør du nå se din data-fil og quarto-dokumentet som du har spart. Marker de filer du ønsker å legge til i historikken (under Staged i menyen til venstre). Skriv et «Commit message» (til høyre) og trykk på «commit». Et vindu åpnes og sier noe om statusen for din commit. For å laste opp filene till github.com så trenger du å trykke på «Push» (pilen som peker oppover lengst oppe til høyre).\nHvis du eller din kamerat har gjort forandringer i mappen på github kan du trykke på «pull» (pil som peker nedover). Du laster da ned forandringer til din lokale mappe.\nHvor ofte bør jeg legge til forandringer? Det er opp til deg, men det kan være lurt å legge til flere små forandringer for å ha mulighet til å se når noe går galt."
  },
  {
    "objectID": "ws7-norwegian.html#steg-5-beregne-reliabilitet",
    "href": "ws7-norwegian.html#steg-5-beregne-reliabilitet",
    "title": "Workshop 7: Hvordan i huleste løser jeg arbeidskrav 1? En oppsummering av emnet så langt",
    "section": "Steg 5: Beregne reliabilitet",
    "text": "Steg 5: Beregne reliabilitet\nDette steget krever noe data wrangling, noe innsikt i hva som er reliabilitet og noe innsikt i hvordan du kan presentere resultater. For bakgrunn og diskusjon se kursnotatene kapittel 9.\nHele analysen er ikke større enn at den får plass i et quarto-dokument. I mer komplekse prosjekter kreves det iblant flere filer/skript for å håndtere data og funksjoner.\nI quarto-dokumentet blir første steg å laste inn data. Du har plassert dataene i en mappen som heter «data», denne kan du nå gjennom en relativ adresse. For å laste inn en xlsx-fil trenger du pakken «readxl», for å laste inn en csv-fil vil «readr» eller funksjoner fra base R fungere. I eksemplet under laster vi inn en excelfil og lagrer den i et objekt som vi navngir «dat».\n\nlibrary(tidyverse); library(readxl)\ndat <- read_excel(\"data/vo2max-g2.xlsx\") %>%\n        select(fp, time, vo2max = `VO2_max_ml/min`) %>%\n        print()\n\n# A tibble: 22 x 3\n      fp time  vo2max\n   <dbl> <chr>  <dbl>\n 1     1 pre    4515 \n 2     1 post   4372 \n 3     2 pre    5042.\n 4     2 post   5050.\n 5     3 pre    3088 \n 6     3 post   3130.\n 7     4 pre    2828.\n 8     4 post   2897 \n 9     5 pre    4922 \n10     5 post   4696.\n# ... with 12 more rows\n\n\nVi skal beregne reliabilitet basert på differenser mellom to målinger. Hvis en test har høy reliabilitet så er differensen mellom to målinger liten. Vi ønsker bestemme den typiske differensen (eller typical error) mellom to målinger, dette definerer vi som standardavviket for differensene delt på kvadratroten av 2 (!, se Hopkins 2000).\nDen enkleste måten å regne ut differensen mellom parvise målinger er å lage datasettet til «wide» format. Dette gjør vi med pivot_wider, vi lager seden en ny variabel for å skape differensen. Standardavvik av differensen kan beregnes i summarise. Typiske feilet/differensen/error kan uttrykkes som en % av gjennomsnitt og ved hjelp av «limits of agreement» (Hopkins 2000) (Se også kursnotatene).\n\ndat %>%\n        pivot_wider(names_from = time, values_from = vo2max) %>%\n        mutate(diff = post - pre) %>%\n        summarise(s = sd(diff),  # SD av differense\n                  m = mean(c(pre, post)),  # Gjennomsnitt av alle målingene\n                  te = s / sqrt(2)) %>% # \"Typical error\" \n        print()\n\n# A tibble: 1 x 3\n      s     m    te\n  <dbl> <dbl> <dbl>\n1  230. 4023.  163."
  },
  {
    "objectID": "ws7-norwegian.html#steg-6-skriv-rapporten",
    "href": "ws7-norwegian.html#steg-6-skriv-rapporten",
    "title": "Workshop 7: Hvordan i huleste løser jeg arbeidskrav 1? En oppsummering av emnet så langt",
    "section": "Steg 6: Skriv rapporten",
    "text": "Steg 6: Skriv rapporten\nNå er vi klare for å skrive rapporten. For å vise at du kan håndtere alle delene i en kvantitativ rapport bør den inneholde:\n\nEn figur. Bruk ggplot2 for å gi en grafisk representasjon av dataene, kanskje rådata fra test 1 og 2 plotet på x og y aksel. Alternativt noe mer avansert, som en Bland-Altman plot\nEn tabell, bruk gt for å lage en tabell for å beskrive deltakerne eller resultater\nBibliography/Referanser. Bruk visual editor og sett in referanser (Insert > Citation)\nTeksten bør beskrive testen (protokoll, databearbeiding osv) og tolkning av resultatene. Her bør dere bruke relevante referanser for å sette deres resultat i kontekst.\n\nSe til at rapporten er mulig å lage til eks. html (knit/render virker). Legg alt på github og lim inn adressen i innleveringen på canvas.\nLykke till!"
  },
  {
    "objectID": "ws7-reliability.html",
    "href": "ws7-reliability.html",
    "title": "Interpreting Reliability",
    "section": "",
    "text": "The reliability of a test can provide information on what variation to expect between two repeated measurements. This information can be use to put test results into perspective.\nBelow we will use simulated data to get an idea of what the measurements mean."
  },
  {
    "objectID": "ws7-reliability.html#data-simulation",
    "href": "ws7-reliability.html#data-simulation",
    "title": "Interpreting Reliability",
    "section": "Data simulation",
    "text": "Data simulation\nA great thing about R is that you can simulate data. Simulated data take pre specified characteristics, thus adhering to assumptions.\nThe faux package can be used to simulate variables with a known correlation. We will use this functionality to simulate a reliability study on VO2max. One-hundred participants were tested three times. The average VO2max, between-participant standard deviation and correlation between measurements were approximated from the cyclingstudy dataset.\nFirst we will load required packages.\n\n\nCode\nlibrary(faux) # to simulate data\nlibrary(tidyverse) # data wrangling/plotting \n## library(Hmisc) # a function to calculate a correlation matrix\nlibrary(cowplot) # for plotting \nlibrary(ggtext) # for plotting (text formatting)\nlibrary(lme4) # to calculate the intra-class correlation\n\nset.seed(2) # Always set seed when simulating data!\n\n\nThe mean and (SD) VO2max was 4842 (532) mL min-1. The correlations between repeated tests ranged from r = 0.797 to r = 0.966. We will use a correlation of r = 0.85 for our simulations. The data is plotted below.\n\n\nCode\ndat <- rnorm_multi(n = 100, \n                   varnames = c(\"t1\", \"t2\", \"t3\", \"t4\"), \n                   mu = c(avg$m, avg$m, avg$m, avg$m), # averages from cyclingstudy dataset\n                   sd = c(avg$s, avg$s, avg$s, avg$s), # sd from cyclingstudy dataset\n                   r = 0.85) %>%\n        mutate(id = paste0(\"ID\", seq(1:100))) \n\n\ndat %>%\n        pivot_longer(names_to = \"time\", \n                     values_to = \"vo2max\", \n                     cols = t1:t4) %>%\n        ggplot(aes(time, vo2max, group = id)) + geom_line() +\n        labs(x = \"Test occasion\", \n             y = \"VO<sub>2max</sub> (mL min<sup>-1</sup>)\") + \n        theme(axis.title.y = element_markdown())\n\n\n\n\n\nSimulated VO2max data, n = 100\n\n\n\n\nWe will use test occasions 1 and 2 to calculate the reliability. Occasion 3 and 4 will be used to test if another test score difference ends up where it is supposed to!\nThe reliability can be calculated as a typical error (TE):\n\\[ TE = \\frac{SD(\\text{Difference score})}{ \\sqrt{2}}\\]\nThe TE can be expressed as a percentage of the average (coefficient of variation; CV):\n\\[\\%CV = 100 \\times \\frac{TE}{M}\\]\nIn R, these are easily calculated in a dplyr pipe.\n\n\nCode\nsum_stat <- dat %>%\n\n        mutate(diff = t2 - t1) %>%\n        summarise(s = sd(diff),               # SD of difference scores\n                  mdiff = mean(diff),         # Mean difference\n                  m = mean(c(t1, t2)),        # Mean of all measurements\n                  te = s / sqrt(2),           # Typical error\n                  cv = 100 * (te / m),        # Percentage CV\n                  L = qt(0.975, 99) * s,      # Limits of agreement 95%\n                  iisd = mean(c(sd(t1), sd(t2)))) %>%   # Mean SD between participants\n        print()\n\n\n         s     mdiff        m       te       cv        L     iisd\n1 265.3947 -4.333928 4806.635 187.6624 3.904236 526.6006 527.6439\n\n\nWe can plot differences (y-axis) against the overall average per participant (x-axis) to produce a Bland-Altman plot. We also plot the average difference between measurements, and the 95% limits of agreement.\n\n\nCode\n# A plot showing the estimated distribution of the errors (differences)\n\ndistribution <- dat %>%\n        rowwise() %>%\n        mutate(diff = t2 - t1, \n               avg = mean(c(t1, t2))) %>%\n        ggplot(aes(x = diff)) +\n        scale_x_continuous(limits = c(-1000, 1000)) +\n        stat_function(fun = dnorm, \n                      geom = \"area\",\n                      args = list(mean = sum_stat$mdiff, \n                                               sd = sum_stat$s), \n                      fill = \"steelblue\") +\n        \n        \n        theme(axis.text = element_blank(), \n              axis.title = element_blank(), \n              axis.ticks = element_blank(), \n              panel.grid = element_blank()) + \n        labs(title = \"A t-distribution\") +\n        coord_flip()\n\n\nbland_altman <- dat %>%\n        rowwise() %>%\n        mutate(diff = t2 - t1, \n               avg = mean(c(t1, t2))) %>%\n        ggplot(aes(avg, diff)) + \n        \n        geom_point() +\n        scale_y_continuous(limits = c(-1000, 1000)) +\n        \n        geom_hline(yintercept = sum_stat$mdiff) + \n        \n        geom_hline(yintercept = sum_stat$mdiff + sum_stat$L) + \n        geom_hline(yintercept = sum_stat$mdiff - sum_stat$L)  +\n        labs(x = \"Average of both measures\", \n             y = \"Difference between measurements\", \n             title = \"Bland-Altman plot\")\n        \n\nplot_grid(bland_altman, distribution, align = \"h\", nrow = 1, rel_widths = c(1, 0.3))\n\n\n\n\n\nThe distribution is included to give an idea of the assumed distribution of errors (differences) from which the limits of agreements are calculated. These limits can be used to specify a range within 95% of repeated measures will fall from another test. This assumes:\n\nSimilar variation\nSimilar correlation\nNo bias\n\nLet’s see if we can make this happen with data from test occasion 3 and 4. First we calculate how many test-retests differences are found within 95% limits of agreement. We can then display the “new” data as part of the first Bland-Altman plot.\n\n\nCode\nnew_dat <- dat %>%\n        rowwise() %>%\n        mutate(diff = t2 - t1, \n               diff2 = t4 - t3, # calculates the new test data \n               avg = mean(c(t1, t2)), \n\n               within = if_else(between(diff2, sum_stat$mdiff - sum_stat$L,\n                                        sum_stat$mdiff + sum_stat$L), \"in\", \"out\"))\n        \n\nnew_dat %>%\n        group_by(within) %>%\n        summarise(n = n())\n\n\n# A tibble: 2 x 2\n  within     n\n  <chr>  <int>\n1 in        97\n2 out        3\n\n\nCode\nnew_dat %>%     \n        \n        ggplot(aes(avg, diff)) + \n        \n        geom_point(alpha = 0.4) +\n        \n        geom_point(size = 2.5,\n                   aes(avg, diff2, \n                       shape = within, \n                       fill = within)) +\n        scale_shape_manual(values = c(21, 25)) +\n        scale_fill_manual(values = c(\"cornflowerblue\", \"magenta2\")) +\n        \n        scale_y_continuous(limits = c(-1500, 1500)) +\n        \n        geom_hline(yintercept = sum_stat$mdiff) + \n        \n        geom_hline(yintercept = sum_stat$mdiff + sum_stat$L) + \n        geom_hline(yintercept = sum_stat$mdiff - sum_stat$L)  +\n        labs(x = \"Average of both measures\", \n             y = \"Difference between measurements\")"
  },
  {
    "objectID": "ws7-reliability.html#te-as-an-intra-individual-variation",
    "href": "ws7-reliability.html#te-as-an-intra-individual-variation",
    "title": "Interpreting Reliability",
    "section": "TE as an intra-individual variation!",
    "text": "TE as an intra-individual variation!\nThe typical error (or standard error of measurement) of our simulated data is about 3.9% of the mean. We can think of this as the long-run variation in an athlete or patient. If nothing changes most repeated measures will be inside e.g., a 95% range (\\(\\pm 2 \\times SD \\approx 95\\%\\) of all values under a normal distribution).\n\n\nCode\nind_cv <- cyclingstudy %>%\n   \n        filter(subject %in% c(6, 8)) %>%\n        filter(timepoint == \"pre\") %>%\n        select(subject, timepoint, VO2.max) %>%\n        mutate(te = VO2.max * (sum_stat$cv / 100), \n               lower = VO2.max - 2 * te, \n               upper = VO2.max + 2 * te) \n\ncyclingstudy %>%\n   \n       filter(subject %in% c(6, 8)) %>%\n        select(subject, timepoint, VO2.max) %>%\n        mutate(timepoint = factor(timepoint, levels = c(\"pre\", \"meso1\", \"meso2\", \"meso3\"))) %>%\n        \n        ggplot(aes(timepoint, VO2.max, group = subject, color = as.factor(subject))) + \n        geom_point() + \n        geom_line() + \n        \n        scale_color_manual(values = c(\"coral2\", \"steelblue\")) +\n        \n        geom_hline(yintercept = ind_cv$lower, \n                   color = c(\"coral2\", \"steelblue\"), \n                   lty = c(2, 2), \n                   size = 1.5) +\n        geom_hline(yintercept = ind_cv$upper, \n                   color = c(\"coral2\", \"steelblue\"), \n                   lty = c(2, 2), \n                   size = 1.5) +\n        labs(x = \"Time-point\", \n             y = \"VO<sub>2max</sub>\", \n             color = \"Participant\") +\n        theme(axis.title.y = element_markdown())"
  },
  {
    "objectID": "ws8-linear-model.html",
    "href": "ws8-linear-model.html",
    "title": "The Linear Model",
    "section": "",
    "text": "A straight line can be described using the simple equation \\(y = a + bx\\). Here \\(a\\) is the intercept (\\(y\\) when \\(x=0\\)) and \\(b\\) is the slope (difference in \\(y\\) for every unit difference in \\(x\\)).\nThe line is heading upwards if \\(b > 0\\) and downwards if \\(b < 0\\).\n\n\n\n\n\nStraight lines following the equation \\(y = a + bx\\)"
  },
  {
    "objectID": "ws8-linear-model.html#creating-models-of-data-using-straight-lines",
    "href": "ws8-linear-model.html#creating-models-of-data-using-straight-lines",
    "title": "The Linear Model",
    "section": "Creating models of data using straight lines",
    "text": "Creating models of data using straight lines\n\nNotes on models1\n\nWe can use the straight line to create a model that describes data.\nA statistical model is an abstract representation of the underlying data that we hope captures some characteristics of the real world(!).\nThe straight line effectively avoids complexity of the real world\nStatistical models are constructions made for some purpose (e.g., prediction or explanation)\n\nWe will start our journey in statistical modelling using straight lines.\n\n\n\n\n\n\nGroup work\n\n\n\n\nTask: What line would best describe the data shown in the plots below? Copy the code in the code chunk below the figure and add straight lines that best describes the data.\n\nStraight lines can be added to a ggplot using the geom_abline() function. It takes the arguments slope and intercept.\n\n\n\n\n\n\n\n\n\nCode\na <- ggplot(data.frame(x = c(1, 2, 3), \n                  y = c(1, 2, 3)), \n       aes(x, y)) +\n        geom_point(size = 3) +\n        scale_y_continuous(limits = c(0, 5)) +\n        scale_x_continuous(limits = c(0, 5)) \n\nb <- ggplot(data.frame(x = c(1, 1, 2), \n                  y = c(1, 2, 2)), \n       aes(x, y)) +\n        geom_point(size = 3) +\n        scale_y_continuous(limits = c(0, 5)) +\n        scale_x_continuous(limits = c(0, 5)) \n        \nc <- ggplot(data.frame(x = c(1, 1, 2, 2), \n                  y = c(1, 2, 1, 2)), \n       aes(x, y)) +\n        geom_point(size = 3) +\n        scale_y_continuous(limits = c(0, 5)) +\n        scale_x_continuous(limits = c(0, 5))\n\n\nd <- ggplot(data.frame(x = c(1, 2, 3, 3, 4, 5), \n                  y = c(2, 3, 1, 3, 2, 3)), \n       aes(x, y)) +\n        geom_point(size = 3) +\n        scale_y_continuous(limits = c(0, 5)) +\n        scale_x_continuous(limits = c(0, 5))"
  },
  {
    "objectID": "ws8-linear-model.html#fitting-a-straight-line-to-data",
    "href": "ws8-linear-model.html#fitting-a-straight-line-to-data",
    "title": "The Linear Model",
    "section": "Fitting a straight line to data",
    "text": "Fitting a straight line to data\nTo achieve the goal of describing data with a model, a straight line can be fitted to data by minimizing the error in \\(y\\).\nFor every observation (\\(y_i\\)) a line produces a prediction \\(\\hat{y}_i\\). The best fitting line is the line that minimizes the sum of squared errors:\n\\[\\sum(y_i - \\hat{y}_i)^2\\]\n\n\n\n\n\nBy adding the residual error (\\(y_i - \\hat{y}_i\\)) to the equation mentioned above we can formalize the model using the following notation:\n\\[y_i = \\beta_0 + \\beta_1x_{i} + \\epsilon_i\\]\nEach observation \\(y_i\\) can be described with coefficients describing the straight line \\(\\beta_0 + \\beta_1x_i\\) and some error \\(\\epsilon_i\\).\nWhich can be translated to (Spiegelhalter 2019):\n\\[\\text{observation = deterministic model + residual error}\\]\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\nDefine these concepts:\n\nModel\nCoefficients\nIntercept\nSlope\nResiduals"
  },
  {
    "objectID": "ws8-linear-model.html#fitting-a-regression-model-in-r",
    "href": "ws8-linear-model.html#fitting-a-regression-model-in-r",
    "title": "The Linear Model",
    "section": "Fitting a regression model in R",
    "text": "Fitting a regression model in R\nWork in pairs and use the code below to fit and analyze a regression model\nWe will use the cyclingstudy data set to fit regression models. In R, a linear model can be fitted with the lm function. This function needs a formula and a data set (data frame).\nA formula is written as y ~ x, this formula can be read as “y explained by x”.\nLet’s use the pre time-point data to predict/explain VO2.max with height.T1.\n\n\nCode\nlibrary(exscidata)\n\ndata(\"cyclingstudy\")\n\n# Reduce the data set \ndat <- cyclingstudy %>%\n        filter(timepoint == \"pre\") %>%\n        select(subject, height.T1, VO2.max) \n\nmod <- lm(VO2.max ~ height.T1, data = dat)\n\n\nThe resulting model object is a list that contains a lot of information. We will put some of these components in a figure by first creating a data frame. Copy the code and run it in your own environment!\nFirst, mod$model is a data.frame of the data used to fit the model. mod$fitted.values contains the predicted values from the regression model and mod$residuals contains each residual (\\(y_i - \\hat{y}_i\\)). We can store these together in a new data frame:\n\n\nCode\nmod_dat <- data.frame(mod$model, \n           fitted = mod$fitted.values, \n           resid = mod$residuals)\n\n\nWe will start by adding the fitted values as a function of the predictor values (height.T1). Let’s make the points a bit larger and filled circles.\n\n\nCode\nmod_dat %>%\n        ggplot(aes(height.T1, fitted)) + geom_point(size = 3, shape = 21, fill = \"steelblue\")\n\n\nNext we will add the residuals as segments starting from the fitted values. geom_segment takes the aesthetics (aes) x, xend, y and yend. x and xend will be the predictor values (height.T1), y will be the fitted values and yend will be the fitted values + residuals.\n\n\nCode\nmod_dat %>%\n        ggplot(aes(height.T1, fitted)) + geom_point(size = 4, shape = 21, fill = \"steelblue\") + \n        # Add residuals\n        geom_segment(aes(x = height.T1, xend = height.T1, y = fitted, yend = fitted + resid))\n\n\nNotice that there are some overlap between individuals in height.\nNext, let’s add the observed values in a new geom_point. We make the points a bit bigger and make the filled circles.\n\n\nCode\nmod_dat %>%\n        ggplot(aes(height.T1, fitted)) + geom_point(size = 4, shape = 21, fill = \"steelblue\") + \n        # Add residuals\n        geom_segment(aes(x = height.T1, xend = height.T1, y = fitted, yend = fitted + resid)) +\n        # Add observed values\n        geom_point(aes(height.T1, VO2.max), shape = 21, fill = \"hotpink\", size = 4)\n\n\nAt last, let’s add the model prediction as an annotation. Using annotate we can specify a geom and if we chose \"segment\" it let’s us specify a start and an end on the x and y axis. We can use the function coef() to get the coefficients from the model where coef(mod)[1] is the intercept and coef(mod)[2] is the slope.\n\n\nCode\nmod_dat %>%\n        ggplot(aes(height.T1, fitted)) + geom_point(size = 4, shape = 21, fill = \"steelblue\") + \n        # Add residuals\n        geom_segment(aes(x = height.T1, xend = height.T1, y = fitted, yend = fitted + resid)) +\n        # Add observed values\n        geom_point(aes(height.T1, VO2.max), shape = 21, fill = \"hotpink\", size = 4) +\n        # Add the model\n        annotate(geom = \"segment\", \n                 x = min(mod_dat$height.T1), \n                 xend = max(mod_dat$height.T1), \n                 y = coef(mod)[1] + coef(mod)[2] * min(mod_dat$height.T1), \n                 yend = coef(mod)[1] + coef(mod)[2] * max(mod_dat$height.T1), \n                 \n                 color = \"mediumorchid1\", size = 1)\n\n\n\n\n\n\n\n\nReview your understanding\n\n\n\n\n\nUse the figure and explain to a friend:\n\nWhat do the object mod$fitted.values contain?\nWhat information can we get from mod$residuals?\nExplain how the line draw between these points gives a graphical representation of the model?:\n\n\n\nCode\nx = min(mod_dat$height.T1) \n\nxend = max(mod_dat$height.T1) \n\ny = coef(mod)[1] + coef(mod)[2] * min(mod_dat$height.T1) \n\nyend = coef(mod)[1] + coef(mod)[2] * max(mod_dat$height.T1)"
  },
  {
    "objectID": "ws8-linear-model.html#predicting-from-a-regression-model",
    "href": "ws8-linear-model.html#predicting-from-a-regression-model",
    "title": "The Linear Model",
    "section": "Predicting from a regression model",
    "text": "Predicting from a regression model\nSimple predictions can be made from our model using the model coefficients. When the intercept and slope is known, we can simply plug in \\(x\\) values to get predictions.\n\\[\\hat{y} = \\beta_0 + \\beta_1x\\]\n\\[\\hat{y} = 10 + 2.2x\\]\nif \\(x=2\\), then\n\\[\\hat{y} = 10 + 2.2 \\times 2\\]\n\\[ = 14.4\\]\n\n\n\n\n\n\nGroup work\n\n\n\nUse R and solve the following problems\n\nCalculate the expected difference in VO2max between a person that has a stature of 175 and 185 cm\nWhat would be the expected VO2max of a person of height 201 cm?"
  },
  {
    "objectID": "ws9-linear-model-categorical.html",
    "href": "ws9-linear-model-categorical.html",
    "title": "The linear model: Categorical predictors and diagnostics",
    "section": "",
    "text": "Footnotes\n\n\nSee Common statistical tests are linear models https://lindeloev.github.io/tests-as-linear/↩︎"
  }
]